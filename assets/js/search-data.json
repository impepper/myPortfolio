{
  
    
        "post0": {
            "title": "台灣加權股價指數預測 - 使用 AutoTS",
            "content": "&#32929;&#20729;&#38928;&#28204;&#26159;&#27231;&#22120;&#23416;&#32722;&#22312;&#37329;&#34701;&#38936;&#22495;&#26368;&#37325;&#35201;&#30340;&#25033;&#29992;&#20043;&#19968;&#12290;&#22312;&#36889;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#23559;&#22039;&#35430;&#36879;&#36942;&#20351;&#29992; Python &#30340;&#32218;&#24615;&#22238;&#27512;&#27169;&#22411;&#20358;&#36914;&#34892;&#32929;&#31080;&#20729;&#26684;&#30340;&#38928;&#28204;&#12290; . 預測股市的價格，一直是投資者的終極目標。 在每天數以億計的交易之中，每筆交易，都代表者投資者對於該股票的價格預期，並且期望透過交易獲利。 . 也因此，股票漲跌，便取決於投資者在交易市場中的投資行為。 如果投資者能夠準確預測市場動向，便有機會創造誘人的財富。 . 如果您具有股票市場的投資經驗以及機器學習的量化數據分析技能，對於您預測價格趨勢將會有明顯的助益。 . Python &#20989;&#24335;&#24235; AutoTS . AutoTS 是一個用於自動時間序列分析的 Python 函式庫。它是 autoML 的一部分，其目標是為初學者提供自動化的時間序列模型工具。 . 時間序列問題無論是在銷量預測，天氣預測還是在股票預測等問題中都至關重要，而如今隨著機器學習等快速發展，已經出現了非常多時間序列建模相關的函式庫，AutoTS 融合了自動化機器學習技術，會先對數據進行預處理，從數據中刪除異常值，再通過學習尋找最佳值。Auto 只需使用一行代碼，就可以訓練多個時間序列模型，包括ARIMA、SARIMAX、FB Prophet、VAR，並得出效果最佳的模型。它具有以下幾個特性： . 它可用於查找最佳時間序列預測模型，具體取決於您使用的數據類型。 | 它可以處理單變量和多變量時間序列。 | 它還可以通過刪除和填充 NaN 值來處理混亂的數據，它還可以處理異常值。 | 您可以將此庫用於時間序列預測的任何任務，例如預測未來 n 天的股票價格。 . AutoTS是一個非常不錯的時間序列函式庫，在碰到時間序列問題時，可以考慮使用AutoTS來進行訓練和預測，作為一個非常不錯的Baseline。 . &#21443;&#32771;&#36899;&#32080; . https://pypi.org/project/AutoTS/ | https://github.com/winedarksea/AutoTS | . &#21488;&#28771;&#21152;&#27402;&#32929;&#20729;&#25351;&#25976;&#38928;&#28204; . 我將透過 yfinance API 從 yfinance API 取得台灣加權指數（代碼 ^TWII）近3年的歷史價格數據，並藉由這些數據來進行分析及預測。 . 我們首先先將需要的函式庫載入： . import pandas as pd import yfinance as yf import datetime from datetime import date, timedelta import warnings warnings.filterwarnings(&quot;ignore&quot;) . 透過 yfinance API 從 yfinance API 取得台灣加權指數（代碼 ^TWII）近3年的歷史價格數據 . today = date.today() d1 = today.strftime(&quot;%Y-%m-%d&quot;) end_date = d1 d2 = date.today() - timedelta(days=1095) d2 = d2.strftime(&quot;%Y-%m-%d&quot;) start_date = d2 data = yf.download(&#39;^TWII&#39;, start=start_date, end=end_date, progress=False) data[&quot;Date&quot;] = data.index data = data[[&quot;Date&quot;, &quot;Open&quot;, &quot;High&quot;, &quot;Low&quot;, &quot;Close&quot;, &quot;Adj Close&quot;, &quot;Volume&quot;]] data.reset_index(drop=True, inplace=True) . 初步了解取得的數據內容 . print(data.shape) print(data.head()) . (730, 7) Date Open High Low Close 0 2019-06-17 10488.700195 10562.969727 10474.190430 10530.540039 1 2019-06-18 10547.150391 10573.769531 10521.700195 10566.740234 2 2019-06-19 10650.480469 10778.639648 10650.480469 10775.339844 3 2019-06-20 10749.410156 10799.139648 10745.250000 10785.009766 4 2019-06-21 10817.709961 10840.290039 10773.469727 10803.769531 Adj Close Volume 0 10530.540039 1444800 1 10566.740234 1524200 2 10775.339844 2370300 3 10785.009766 2079800 4 10803.769531 2780700 . 我們繪製一下指數的走勢（以K線圖表示）： . import plotly.graph_objects as go figure = go.Figure(data=[go.Candlestick(x=data[&quot;Date&quot;], open=data[&quot;Open&quot;], high=data[&quot;High&quot;], low=data[&quot;Low&quot;], close=data[&quot;Close&quot;])]) figure.update_layout(title = &quot;台灣加權指數走勢&quot;, xaxis_rangeslider_visible=False) figure.show() . 分析時間序列數據的最佳方法之一是創建互動式圖表，您可以在其中手動選擇輸出圖表本身的時間間隔。 一種方法是在圖表下方添加一個滑動區塊，並在圖表上方添加按鈕來控制時間的間隔。 以下是創建互動式K線圖表的方法，您可以在其中選擇輸出本身的時間間隔： . figure = go.Figure(data = [go.Candlestick(x = data.index, open = data[&quot;Open&quot;], high = data[&quot;High&quot;], low = data[&quot;Low&quot;], close = data[&quot;Close&quot;])]) figure.update_layout(title = &quot;台灣加權指數走勢 (互動式圖表)&quot;) figure.update_xaxes( rangeslider_visible = True, rangeselector = dict( buttons = list([ dict(count = 1, label = &quot;1m&quot;, step = &quot;month&quot;, stepmode = &quot;backward&quot;), dict(count = 6, label = &quot;6m&quot;, step = &quot;month&quot;, stepmode = &quot;backward&quot;), dict(count = 1, label = &quot;YTD&quot;, step = &quot;year&quot;, stepmode = &quot;todate&quot;), dict(count = 1, label = &quot;1y&quot;, step = &quot;year&quot;, stepmode = &quot;backward&quot;), dict(step = &quot;all&quot;) ]) ) ) figure.show() . AutoTS &#26178;&#38291;&#24207;&#21015;&#19979;&#30340;&#20729;&#26684;&#38928;&#28204; . 接下來，我們透過AutoTS函式庫，來預測接下來30天的台灣加權股價指數（收盤價格）： . from autots import AutoTS # 載入 AutoTS 函式庫 model = AutoTS(forecast_length=30, frequency=&#39;infer&#39;, ensemble=&#39;simple&#39;) model = model.fit(data, date_col=&#39;Date&#39;, value_col=&#39;Close&#39;, id_col=None) prediction = model.predict() forecast = prediction.forecast . Inferred frequency is: B Model Number: 1 with model AverageValueNaive in generation 0 of 10 Model Number: 2 with model AverageValueNaive in generation 0 of 10 Model Number: 3 with model AverageValueNaive in generation 0 of 10 Model Number: 4 with model DatepartRegression in generation 0 of 10 Model Number: 5 with model DatepartRegression in generation 0 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn svm _base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. . Model Number: 6 with model DatepartRegression in generation 0 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn neural_network _multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html . Model Number: 7 with model DatepartRegression in generation 0 of 10 Epoch 1/50 24/24 [==============================] - 9s 8ms/step - loss: 0.4147 Epoch 2/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4123 Epoch 3/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4116 Epoch 4/50 24/24 [==============================] - 0s 7ms/step - loss: 0.4103 Epoch 5/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4097 Epoch 6/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4109 Epoch 7/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4109 Epoch 8/50 24/24 [==============================] - 0s 7ms/step - loss: 0.4082 Epoch 9/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4064 Epoch 10/50 24/24 [==============================] - 0s 9ms/step - loss: 0.4070 Epoch 11/50 24/24 [==============================] - 0s 9ms/step - loss: 0.4073 Epoch 12/50 24/24 [==============================] - 0s 7ms/step - loss: 0.4084 Epoch 13/50 24/24 [==============================] - 0s 9ms/step - loss: 0.4095 Epoch 14/50 24/24 [==============================] - 0s 7ms/step - loss: 0.4060 Epoch 15/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4064 Epoch 16/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4070 Epoch 17/50 24/24 [==============================] - 0s 7ms/step - loss: 0.4044 Epoch 18/50 24/24 [==============================] - 0s 11ms/step - loss: 0.4086 Epoch 19/50 24/24 [==============================] - 0s 7ms/step - loss: 0.4056 Epoch 20/50 24/24 [==============================] - 0s 9ms/step - loss: 0.4052 Epoch 21/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4043 Epoch 22/50 24/24 [==============================] - 0s 9ms/step - loss: 0.4070 Epoch 23/50 24/24 [==============================] - 0s 9ms/step - loss: 0.4051 Epoch 24/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4038 Epoch 25/50 24/24 [==============================] - 0s 10ms/step - loss: 0.4033 Epoch 26/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4054 Epoch 27/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4050 Epoch 28/50 24/24 [==============================] - 0s 9ms/step - loss: 0.4059 Epoch 29/50 24/24 [==============================] - 0s 10ms/step - loss: 0.4025 Epoch 30/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4032 Epoch 31/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4049 Epoch 32/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4048 Epoch 33/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4054 Epoch 34/50 24/24 [==============================] - 0s 9ms/step - loss: 0.4037 Epoch 35/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4058 Epoch 36/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4049 Epoch 37/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4032 Epoch 38/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4011 Epoch 39/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4022 Epoch 40/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4034 Epoch 41/50 24/24 [==============================] - 0s 15ms/step - loss: 0.4010 Epoch 42/50 24/24 [==============================] - 0s 13ms/step - loss: 0.4032 Epoch 43/50 24/24 [==============================] - 0s 9ms/step - loss: 0.4015 Epoch 44/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4010 Epoch 45/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4028 Epoch 46/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4020 Epoch 47/50 24/24 [==============================] - 0s 10ms/step - loss: 0.3999 Epoch 48/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4000 Epoch 49/50 24/24 [==============================] - 0s 8ms/step - loss: 0.4012 Epoch 50/50 24/24 [==============================] - 0s 8ms/step - loss: 0.3992 Template Eval Error: ValueError(&#39;Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True&#39;) in model 7: DatepartRegression Model Number: 8 with model ETS in generation 0 of 10 Model Number: 9 with model ETS in generation 0 of 10 Model Number: 10 with model GLM in generation 0 of 10 Template Eval Error: TypeError(&#34;ufunc &#39;isfinite&#39; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &#39;&#39;safe&#39;&#39;&#34;) in model 10: GLM Model Number: 11 with model GLM in generation 0 of 10 Model Number: 12 with model GLS in generation 0 of 10 Model Number: 13 with model GLS in generation 0 of 10 Model Number: 14 with model LastValueNaive in generation 0 of 10 Model Number: 15 with model LastValueNaive in generation 0 of 10 Model Number: 16 with model LastValueNaive in generation 0 of 10 Model Number: 17 with model LastValueNaive in generation 0 of 10 Model Number: 18 with model SeasonalNaive in generation 0 of 10 Model Number: 19 with model SeasonalNaive in generation 0 of 10 Model Number: 20 with model SeasonalNaive in generation 0 of 10 Model Number: 21 with model UnobservedComponents in generation 0 of 10 Model Number: 22 with model UnobservedComponents in generation 0 of 10 Model Number: 23 with model UnobservedComponents in generation 0 of 10 Model Number: 24 with model VAR in generation 0 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 24: VAR Model Number: 25 with model VAR in generation 0 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 25: VAR Model Number: 26 with model VECM in generation 0 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 26: VECM Model Number: 27 with model VECM in generation 0 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 27: VECM Model Number: 28 with model WindowRegression in generation 0 of 10 Model Number: 29 with model ConstantNaive in generation 0 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn neural_network _multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html . Model Number: 30 with model FBProphet in generation 0 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 30: FBProphet Model Number: 31 with model MultivariateRegression in generation 0 of 10 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=-2)]: Done 194 tasks | elapsed: 0.5s [Parallel(n_jobs=-2)]: Done 200 out of 200 | elapsed: 0.6s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished . Model Number: 32 with model MultivariateRegression in generation 0 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but not future_regressor supplied.&#34;) in model 32: MultivariateRegression Model Number: 33 with model DatepartRegression in generation 0 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 33: DatepartRegression Model Number: 34 with model SeasonalNaive in generation 0 of 10 Model Number: 35 with model DatepartRegression in generation 0 of 10 Model Number: 36 with model UnobservedComponents in generation 0 of 10 Model Number: 37 with model UnobservedComponents in generation 0 of 10 Model Number: 38 with model ETS in generation 0 of 10 Model Number: 39 with model VECM in generation 0 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 39: VECM Model Number: 40 with model ARDL in generation 0 of 10 Model Number: 41 with model MultivariateMotif in generation 0 of 10 Model Number: 42 with model MultivariateMotif in generation 0 of 10 Model Number: 43 with model UnivariateMotif in generation 0 of 10 Model Number: 44 with model UnivariateMotif in generation 0 of 10 Model Number: 45 with model SectionalMotif in generation 0 of 10 Model Number: 46 with model SectionalMotif in generation 0 of 10 Model Number: 47 with model MultivariateRegression in generation 0 of 10 Model Number: 48 with model FBProphet in generation 0 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 48: FBProphet Model Number: 49 with model SeasonalNaive in generation 0 of 10 Model Number: 50 with model DatepartRegression in generation 0 of 10 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished . Model Number: 51 with model NVAR in generation 0 of 10 Model Number: 52 with model Theta in generation 0 of 10 Model Number: 53 with model ConstantNaive in generation 0 of 10 Model Number: 54 with model LastValueNaive in generation 0 of 10 Model Number: 55 with model AverageValueNaive in generation 0 of 10 Model Number: 56 with model GLS in generation 0 of 10 Model Number: 57 with model SeasonalNaive in generation 0 of 10 Model Number: 58 with model GLM in generation 0 of 10 Template Eval Error: ValueError(&#39;regression_type=user and no future_regressor passed&#39;) in model 58: GLM Model Number: 59 with model ETS in generation 0 of 10 Model Number: 60 with model FBProphet in generation 0 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 60: FBProphet Model Number: 61 with model UnobservedComponents in generation 0 of 10 Model Number: 62 with model VAR in generation 0 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 62: VAR Model Number: 63 with model VECM in generation 0 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 63: VECM Model Number: 64 with model WindowRegression in generation 0 of 10 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.1s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished . Model Number: 65 with model DatepartRegression in generation 0 of 10 Model Number: 66 with model MultivariateRegression in generation 0 of 10 [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000170 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf Model Number: 67 with model UnivariateMotif in generation 0 of 10 Model Number: 68 with model MultivariateMotif in generation 0 of 10 Model Number: 69 with model SectionalMotif in generation 0 of 10 Model Number: 70 with model NVAR in generation 0 of 10 Model Number: 71 with model Theta in generation 0 of 10 Model Number: 72 with model ARDL in generation 0 of 10 Model Number: 73 with model VECM in generation 0 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 73: VECM Model Number: 74 with model MultivariateRegression in generation 0 of 10 Model Number: 75 with model WindowRegression in generation 0 of 10 Model Number: 76 with model SectionalMotif in generation 0 of 10 Model Number: 77 with model Theta in generation 0 of 10 Model Number: 78 with model GLS in generation 0 of 10 Model Number: 79 with model VECM in generation 0 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 79: VECM Model Number: 80 with model WindowRegression in generation 0 of 10 Model Number: 81 with model SectionalMotif in generation 0 of 10 Template Eval Error: ValueError(&#34;regression_type==&#39;User&#39; but no future_regressor supplied&#34;) in model 81: SectionalMotif Model Number: 82 with model MultivariateMotif in generation 0 of 10 Model Number: 83 with model ConstantNaive in generation 0 of 10 Template Eval Error: ValueError(&#34;Model returned NaN due to a preprocessing transformer {&#39;fillna&#39;: &#39;rolling_mean&#39;, &#39;transformations&#39;: {&#39;0&#39;: &#39;StandardScaler&#39;, &#39;1&#39;: &#39;StandardScaler&#39;, &#39;2&#39;: &#39;bkfilter&#39;, &#39;3&#39;: &#39;CumSumTransformer&#39;, &#39;4&#39;: &#39;cffilter&#39;}, &#39;transformation_params&#39;: {&#39;0&#39;: {}, &#39;1&#39;: {}, &#39;2&#39;: {}, &#39;3&#39;: {}, &#39;4&#39;: {}}}. fail_on_forecast_nan=True&#34;) in model 83: ConstantNaive Model Number: 84 with model VECM in generation 0 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor supplied&#34;) in model 84: VECM Model Number: 85 with model SectionalMotif in generation 0 of 10 Model Number: 86 with model LastValueNaive in generation 0 of 10 Model Number: 87 with model LastValueNaive in generation 0 of 10 Model Number: 88 with model UnivariateMotif in generation 0 of 10 Model Number: 89 with model ETS in generation 0 of 10 Model Number: 90 with model ConstantNaive in generation 0 of 10 Model Number: 91 with model MultivariateRegression in generation 0 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but not future_regressor supplied.&#34;) in model 91: MultivariateRegression Model Number: 92 with model ARDL in generation 0 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but future_regressor not supplied&#34;) in model 92: ARDL Model Number: 93 with model NVAR in generation 0 of 10 Model Number: 94 with model SectionalMotif in generation 0 of 10 Model Number: 95 with model UnivariateMotif in generation 0 of 10 Template Eval Error: ValueError(&#39;Model UnivariateMotif returned NaN for one or more series. fail_on_forecast_nan=True&#39;) in model 95: UnivariateMotif Model Number: 96 with model UnobservedComponents in generation 0 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages numpy core _methods.py:48: RuntimeWarning: invalid value encountered in reduce C: Users impep anaconda3 envs OpenCV lib site-packages numpy lib function_base.py:412: RuntimeWarning: invalid value encountered in true_divide . Model Number: 97 with model DatepartRegression in generation 0 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 97: DatepartRegression Model Number: 98 with model Theta in generation 0 of 10 Model Number: 99 with model ConstantNaive in generation 0 of 10 Model Number: 100 with model NVAR in generation 0 of 10 Template Eval Error: ValueError(&#39;Model NVAR returned NaN for one or more series. fail_on_forecast_nan=True&#39;) in model 100: NVAR Model Number: 101 with model ConstantNaive in generation 0 of 10 Model Number: 102 with model FBProphet in generation 0 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 102: FBProphet Model Number: 103 with model ConstantNaive in generation 0 of 10 Model Number: 104 with model DatepartRegression in generation 0 of 10 Epoch 1/50 11/11 [==============================] - 9s 13ms/step - loss: 101.4933 Epoch 2/50 11/11 [==============================] - 0s 15ms/step - loss: 101.4572 Epoch 3/50 11/11 [==============================] - 0s 17ms/step - loss: 100.5987 Epoch 4/50 11/11 [==============================] - 0s 17ms/step - loss: 100.9200 Epoch 5/50 11/11 [==============================] - 0s 17ms/step - loss: 102.1344 Epoch 6/50 11/11 [==============================] - 0s 16ms/step - loss: 100.7201 Epoch 7/50 11/11 [==============================] - 0s 16ms/step - loss: 101.8251 Epoch 8/50 11/11 [==============================] - 0s 18ms/step - loss: 101.3982 Epoch 9/50 11/11 [==============================] - 0s 16ms/step - loss: 100.6088 Epoch 10/50 11/11 [==============================] - 0s 17ms/step - loss: 100.6589 Epoch 11/50 11/11 [==============================] - 0s 17ms/step - loss: 100.9847 Epoch 12/50 11/11 [==============================] - 0s 22ms/step - loss: 100.8364 Epoch 13/50 11/11 [==============================] - 0s 18ms/step - loss: 101.1523 Epoch 14/50 11/11 [==============================] - 0s 18ms/step - loss: 101.1174 Epoch 15/50 11/11 [==============================] - 0s 18ms/step - loss: 100.8739 Epoch 16/50 11/11 [==============================] - 0s 18ms/step - loss: 100.5690 Epoch 17/50 11/11 [==============================] - 0s 17ms/step - loss: 100.2768 Epoch 18/50 11/11 [==============================] - 0s 19ms/step - loss: 100.2152 Epoch 19/50 11/11 [==============================] - 0s 19ms/step - loss: 99.9066 Epoch 20/50 11/11 [==============================] - 0s 17ms/step - loss: 101.0432 Epoch 21/50 11/11 [==============================] - 0s 18ms/step - loss: 100.7747 Epoch 22/50 11/11 [==============================] - 0s 17ms/step - loss: 100.0857 Epoch 23/50 11/11 [==============================] - 0s 17ms/step - loss: 100.1763 Epoch 24/50 11/11 [==============================] - 0s 17ms/step - loss: 99.9129 Epoch 25/50 11/11 [==============================] - 0s 17ms/step - loss: 100.6050 Epoch 26/50 11/11 [==============================] - 0s 17ms/step - loss: 99.7978 Epoch 27/50 11/11 [==============================] - 0s 17ms/step - loss: 100.1098 Epoch 28/50 11/11 [==============================] - 0s 17ms/step - loss: 99.8247 Epoch 29/50 11/11 [==============================] - 0s 16ms/step - loss: 100.2941 Epoch 30/50 11/11 [==============================] - 0s 16ms/step - loss: 100.6660 Epoch 31/50 11/11 [==============================] - 0s 16ms/step - loss: 100.0311 Epoch 32/50 11/11 [==============================] - 0s 16ms/step - loss: 100.3587 Epoch 33/50 11/11 [==============================] - 0s 16ms/step - loss: 99.8673 Epoch 34/50 11/11 [==============================] - 0s 17ms/step - loss: 100.0126 Epoch 35/50 11/11 [==============================] - 0s 19ms/step - loss: 100.2276 Epoch 36/50 11/11 [==============================] - 0s 18ms/step - loss: 99.8899 Epoch 37/50 11/11 [==============================] - 0s 17ms/step - loss: 100.3090 Epoch 38/50 11/11 [==============================] - 0s 21ms/step - loss: 99.8032 Epoch 39/50 11/11 [==============================] - 0s 18ms/step - loss: 100.1839 Epoch 40/50 11/11 [==============================] - 0s 19ms/step - loss: 99.9995 Epoch 41/50 11/11 [==============================] - 0s 19ms/step - loss: 100.4775 Epoch 42/50 11/11 [==============================] - 0s 23ms/step - loss: 99.9295 Epoch 43/50 11/11 [==============================] - 0s 18ms/step - loss: 100.1395 Epoch 44/50 11/11 [==============================] - 0s 17ms/step - loss: 100.1845 Epoch 45/50 11/11 [==============================] - 0s 22ms/step - loss: 99.9617 Epoch 46/50 11/11 [==============================] - 0s 25ms/step - loss: 100.4952 Epoch 47/50 11/11 [==============================] - 0s 20ms/step - loss: 100.1195 Epoch 48/50 11/11 [==============================] - 0s 16ms/step - loss: 100.3531 Epoch 49/50 11/11 [==============================] - 0s 19ms/step - loss: 99.7688 Epoch 50/50 11/11 [==============================] - 0s 21ms/step - loss: 100.1043 Template Eval Error: ValueError(&#39;Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True&#39;) in model 104: DatepartRegression Model Number: 105 with model FBProphet in generation 0 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 105: FBProphet Model Number: 106 with model FBProphet in generation 0 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 106: FBProphet Model Number: 107 with model MultivariateRegression in generation 0 of 10 Model Number: 108 with model Theta in generation 0 of 10 Model Number: 109 with model MultivariateRegression in generation 0 of 10 Model Number: 110 with model LastValueNaive in generation 0 of 10 Model Number: 111 with model GLS in generation 0 of 10 Model Number: 112 with model ConstantNaive in generation 0 of 10 Model Number: 113 with model DatepartRegression in generation 0 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 113: DatepartRegression Model Number: 114 with model GLS in generation 0 of 10 Model Number: 115 with model UnobservedComponents in generation 0 of 10 Template Eval Error: LinAlgError(&#39;Singular matrix&#39;) in model 115: UnobservedComponents Model Number: 116 with model DatepartRegression in generation 0 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn svm _base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. . Model Number: 117 with model DatepartRegression in generation 0 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 117: DatepartRegression Model Number: 118 with model UnobservedComponents in generation 0 of 10 Model Number: 119 with model MultivariateRegression in generation 0 of 10 Epoch 1/50 47/47 [==============================] - 9s 14ms/step - loss: 0.0636 Epoch 2/50 47/47 [==============================] - 1s 13ms/step - loss: 0.0115 Epoch 3/50 47/47 [==============================] - 1s 14ms/step - loss: 0.0016 Epoch 4/50 47/47 [==============================] - 1s 13ms/step - loss: 0.0011 Epoch 5/50 47/47 [==============================] - 1s 14ms/step - loss: 9.5277e-04 Epoch 6/50 47/47 [==============================] - 1s 14ms/step - loss: 7.7466e-04 Epoch 7/50 47/47 [==============================] - 1s 14ms/step - loss: 7.2776e-04 Epoch 8/50 47/47 [==============================] - 1s 14ms/step - loss: 6.6100e-04 Epoch 9/50 47/47 [==============================] - 1s 16ms/step - loss: 5.7689e-04 Epoch 10/50 47/47 [==============================] - 1s 15ms/step - loss: 5.2501e-04 Epoch 11/50 47/47 [==============================] - 1s 16ms/step - loss: 5.3545e-04 Epoch 12/50 47/47 [==============================] - 1s 15ms/step - loss: 4.4294e-04 Epoch 13/50 47/47 [==============================] - 1s 14ms/step - loss: 4.7798e-04 Epoch 14/50 47/47 [==============================] - 1s 13ms/step - loss: 4.0527e-04 Epoch 15/50 47/47 [==============================] - 1s 14ms/step - loss: 3.7283e-04 Epoch 16/50 47/47 [==============================] - 1s 14ms/step - loss: 3.7459e-04 Epoch 17/50 47/47 [==============================] - 1s 14ms/step - loss: 3.7376e-04 Epoch 18/50 47/47 [==============================] - 1s 14ms/step - loss: 3.7424e-04 Epoch 19/50 47/47 [==============================] - 1s 14ms/step - loss: 3.9106e-04 Epoch 20/50 47/47 [==============================] - 1s 14ms/step - loss: 3.7385e-04 Epoch 21/50 47/47 [==============================] - 1s 14ms/step - loss: 3.5949e-04 Epoch 22/50 47/47 [==============================] - 1s 14ms/step - loss: 3.4457e-04 Epoch 23/50 47/47 [==============================] - 1s 16ms/step - loss: 4.1480e-04 Epoch 24/50 47/47 [==============================] - 1s 14ms/step - loss: 3.7439e-04 Epoch 25/50 47/47 [==============================] - 1s 14ms/step - loss: 3.2071e-04 Epoch 26/50 47/47 [==============================] - 1s 14ms/step - loss: 3.9268e-04 Epoch 27/50 47/47 [==============================] - 1s 14ms/step - loss: 3.2402e-04 Epoch 28/50 47/47 [==============================] - 1s 14ms/step - loss: 3.0804e-04 Epoch 29/50 47/47 [==============================] - 1s 14ms/step - loss: 3.7656e-04 Epoch 30/50 47/47 [==============================] - 1s 14ms/step - loss: 4.0517e-04 Epoch 31/50 47/47 [==============================] - 1s 13ms/step - loss: 3.3970e-04 Epoch 32/50 47/47 [==============================] - 1s 13ms/step - loss: 3.5555e-04 Epoch 33/50 47/47 [==============================] - 1s 15ms/step - loss: 3.2561e-04 Epoch 34/50 47/47 [==============================] - 1s 15ms/step - loss: 3.2613e-04 Epoch 35/50 47/47 [==============================] - 1s 15ms/step - loss: 3.7432e-04 Epoch 36/50 47/47 [==============================] - 1s 16ms/step - loss: 3.0226e-04 Epoch 37/50 47/47 [==============================] - 1s 17ms/step - loss: 3.2493e-04 Epoch 38/50 47/47 [==============================] - 1s 18ms/step - loss: 3.6247e-04 Epoch 39/50 47/47 [==============================] - 1s 17ms/step - loss: 3.0538e-04 Epoch 40/50 47/47 [==============================] - 1s 14ms/step - loss: 3.8124e-04 Epoch 41/50 47/47 [==============================] - 1s 14ms/step - loss: 3.3715e-04 Epoch 42/50 47/47 [==============================] - 1s 13ms/step - loss: 3.7144e-04 Epoch 43/50 47/47 [==============================] - 1s 14ms/step - loss: 3.0612e-04 Epoch 44/50 47/47 [==============================] - 1s 16ms/step - loss: 4.1523e-04 Epoch 45/50 47/47 [==============================] - 1s 14ms/step - loss: 2.9862e-04 Epoch 46/50 47/47 [==============================] - 1s 14ms/step - loss: 2.8963e-04 Epoch 47/50 47/47 [==============================] - 1s 14ms/step - loss: 3.0635e-04 Epoch 48/50 47/47 [==============================] - 1s 15ms/step - loss: 3.5204e-04 Epoch 49/50 47/47 [==============================] - 1s 14ms/step - loss: 3.3942e-04 Epoch 50/50 47/47 [==============================] - 1s 14ms/step - loss: 2.9167e-04 Template Eval Error: ValueError(&#39;Model MultivariateRegression returned NaN for one or more series. fail_on_forecast_nan=True&#39;) in model 119: MultivariateRegression Model Number: 120 with model UnivariateMotif in generation 0 of 10 Model Number: 121 with model SeasonalNaive in generation 0 of 10 Model Number: 122 with model GLS in generation 0 of 10 Model Number: 123 with model SectionalMotif in generation 0 of 10 Model Number: 124 with model MultivariateMotif in generation 0 of 10 Template Eval Error: PicklingError(&#39;Could not pickle the task to send it to the workers.&#39;) in model 124: MultivariateMotif Model Number: 125 with model UnivariateMotif in generation 0 of 10 Model Number: 126 with model VAR in generation 0 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 126: VAR Model Number: 127 with model LastValueNaive in generation 0 of 10 Model Number: 128 with model UnivariateMotif in generation 0 of 10 Model Number: 129 with model GLM in generation 0 of 10 Template Eval Error: TypeError(&#34;ufunc &#39;isfinite&#39; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &#39;&#39;safe&#39;&#39;&#34;) in model 129: GLM Model Number: 130 with model MultivariateMotif in generation 0 of 10 Model Number: 131 with model GLM in generation 0 of 10 Template Eval Error: TypeError(&#34;ufunc &#39;isfinite&#39; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &#39;&#39;safe&#39;&#39;&#34;) in model 131: GLM Model Number: 132 with model LastValueNaive in generation 0 of 10 Model Number: 133 with model UnobservedComponents in generation 0 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor supplied&#34;) in model 133: UnobservedComponents Model Number: 134 with model Theta in generation 0 of 10 Model Number: 135 with model VAR in generation 0 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 135: VAR Model Number: 136 with model NVAR in generation 0 of 10 Model Number: 137 with model UnivariateMotif in generation 0 of 10 Model Number: 138 with model ConstantNaive in generation 0 of 10 Model Number: 139 with model SectionalMotif in generation 0 of 10 Model Number: 140 with model ConstantNaive in generation 0 of 10 Model Number: 141 with model AverageValueNaive in generation 0 of 10 Model Number: 142 with model AverageValueNaive in generation 0 of 10 Model Number: 143 with model GLS in generation 0 of 10 Model Number: 144 with model ARDL in generation 0 of 10 Model Number: 145 with model UnivariateMotif in generation 0 of 10 Model Number: 146 with model WindowRegression in generation 0 of 10 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=-2)]: Done 194 tasks | elapsed: 0.5s [Parallel(n_jobs=-2)]: Done 444 tasks | elapsed: 1.4s [Parallel(n_jobs=-2)]: Done 794 tasks | elapsed: 2.5s [Parallel(n_jobs=-2)]: Done 1000 out of 1000 | elapsed: 3.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.1s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.3s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished . Model Number: 147 with model ARDL in generation 0 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but future_regressor not supplied&#34;) in model 147: ARDL Model Number: 148 with model LastValueNaive in generation 0 of 10 Model Number: 149 with model ConstantNaive in generation 0 of 10 Model Number: 150 with model VECM in generation 0 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor supplied&#34;) in model 150: VECM Model Number: 151 with model NVAR in generation 0 of 10 Model Number: 152 with model NVAR in generation 0 of 10 New Generation: 1 of 10 Model Number: 153 with model LastValueNaive in generation 1 of 10 Model Number: 154 with model LastValueNaive in generation 1 of 10 Model Number: 155 with model LastValueNaive in generation 1 of 10 Model Number: 156 with model Theta in generation 1 of 10 Model Number: 157 with model Theta in generation 1 of 10 Model Number: 158 with model Theta in generation 1 of 10 Model Number: 159 with model Theta in generation 1 of 10 Model Number: 160 with model NVAR in generation 1 of 10 Model Number: 161 with model NVAR in generation 1 of 10 Model Number: 162 with model NVAR in generation 1 of 10 Model Number: 163 with model NVAR in generation 1 of 10 Model Number: 164 with model ConstantNaive in generation 1 of 10 Model Number: 165 with model ConstantNaive in generation 1 of 10 Model Number: 166 with model ConstantNaive in generation 1 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn utils validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: [&#39;Timestamp&#39;, &#39;str&#39;]. An error will be raised in 1.2. C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:1187: RuntimeWarning: All-NaN slice encountered C: Users impep anaconda3 envs OpenCV lib site-packages sklearn utils validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: [&#39;Timestamp&#39;, &#39;str&#39;]. An error will be raised in 1.2. . Template Eval Error: Exception(&#39;Transformer MaxAbsScaler failed on fit&#39;) in model 166: ConstantNaive Model Number: 167 with model UnobservedComponents in generation 1 of 10 Model Number: 168 with model UnobservedComponents in generation 1 of 10 Model Number: 169 with model UnobservedComponents in generation 1 of 10 Model Number: 170 with model GLS in generation 1 of 10 Model Number: 171 with model GLS in generation 1 of 10 Model Number: 172 with model GLS in generation 1 of 10 Model Number: 173 with model SeasonalNaive in generation 1 of 10 Model Number: 174 with model SeasonalNaive in generation 1 of 10 Model Number: 175 with model SeasonalNaive in generation 1 of 10 Model Number: 176 with model SeasonalNaive in generation 1 of 10 Model Number: 177 with model WindowRegression in generation 1 of 10 Model Number: 178 with model WindowRegression in generation 1 of 10 Epoch 1/50 18/18 [==============================] - 20s 422ms/step - loss: 103.9447 - val_loss: 121.9874 Epoch 2/50 18/18 [==============================] - 8s 458ms/step - loss: 94.0657 - val_loss: 125.3344 Epoch 3/50 18/18 [==============================] - 8s 418ms/step - loss: 91.0484 - val_loss: 125.4194 Epoch 4/50 18/18 [==============================] - 7s 420ms/step - loss: 86.9609 - val_loss: 128.9946 Epoch 5/50 18/18 [==============================] - 7s 414ms/step - loss: 86.0640 - val_loss: 132.0940 Epoch 6/50 18/18 [==============================] - 8s 461ms/step - loss: 84.7157 - val_loss: 132.6775 Epoch 7/50 18/18 [==============================] - 8s 438ms/step - loss: 83.3668 - val_loss: 136.1450 Epoch 8/50 18/18 [==============================] - 8s 433ms/step - loss: 82.9913 - val_loss: 132.7413 Epoch 9/50 18/18 [==============================] - 8s 435ms/step - loss: 82.3165 - val_loss: 139.5983 Epoch 10/50 18/18 [==============================] - 7s 405ms/step - loss: 82.2243 - val_loss: 137.6282 Epoch 11/50 18/18 [==============================] - 8s 437ms/step - loss: 81.0812 - val_loss: 140.9158 Model Number: 179 with model WindowRegression in generation 1 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 179: WindowRegression Model Number: 180 with model AverageValueNaive in generation 1 of 10 Model Number: 181 with model AverageValueNaive in generation 1 of 10 Model Number: 182 with model AverageValueNaive in generation 1 of 10 Model Number: 183 with model ARDL in generation 1 of 10 Model Number: 184 with model ARDL in generation 1 of 10 Model Number: 185 with model ARDL in generation 1 of 10 Model Number: 186 with model ARDL in generation 1 of 10 Model Number: 187 with model SectionalMotif in generation 1 of 10 Model Number: 188 with model SectionalMotif in generation 1 of 10 Model Number: 189 with model SectionalMotif in generation 1 of 10 Model Number: 190 with model SectionalMotif in generation 1 of 10 Model Number: 191 with model ETS in generation 1 of 10 Model Number: 192 with model ETS in generation 1 of 10 Model Number: 193 with model ETS in generation 1 of 10 ETS error ValueError(&#39;endog must be strictly positive when usingmultiplicative trend or seasonal components.&#39;) ETS failed on Close with ValueError(&#39;endog must be strictly positive when usingmultiplicative trend or seasonal components.&#39;) Model Number: 194 with model ETS in generation 1 of 10 Model Number: 195 with model UnivariateMotif in generation 1 of 10 Model Number: 196 with model UnivariateMotif in generation 1 of 10 Model Number: 197 with model UnivariateMotif in generation 1 of 10 Model Number: 198 with model UnivariateMotif in generation 1 of 10 Model Number: 199 with model MultivariateMotif in generation 1 of 10 Model Number: 200 with model MultivariateMotif in generation 1 of 10 Model Number: 201 with model MultivariateMotif in generation 1 of 10 Model Number: 202 with model MultivariateRegression in generation 1 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.142e+01, tolerance: 5.856e-03 . Model Number: 203 with model MultivariateRegression in generation 1 of 10 Template Eval Error: ValueError(&#39;Some value(s) of y are out of the valid range for family PoissonDistribution&#39;) in model 203: MultivariateRegression Model Number: 204 with model MultivariateRegression in generation 1 of 10 Template Eval Error: ValueError(&#39;Some value(s) of y are out of the valid range for family PoissonDistribution&#39;) in model 204: MultivariateRegression Model Number: 205 with model MultivariateRegression in generation 1 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but not future_regressor supplied.&#34;) in model 205: MultivariateRegression Model Number: 206 with model DatepartRegression in generation 1 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 206: DatepartRegression Model Number: 207 with model DatepartRegression in generation 1 of 10 Template Eval Error: ValueError(&#39;Failed to convert a NumPy array to a Tensor (Unsupported object type int).&#39;) in model 207: DatepartRegression Model Number: 208 with model DatepartRegression in generation 1 of 10 Model Number: 209 with model GLM in generation 1 of 10 Model Number: 210 with model GLM in generation 1 of 10 Template Eval Error: ValueError(&#39;regression_type=user and no future_regressor passed&#39;) in model 210: GLM Model Number: 211 with model GLM in generation 1 of 10 Template Eval Error: ValueError(&#39;regression_type=user and no future_regressor passed&#39;) in model 211: GLM Model Number: 212 with model GLM in generation 1 of 10 Model Number: 213 with model VAR in generation 1 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 213: VAR Model Number: 214 with model VAR in generation 1 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 214: VAR Model Number: 215 with model VAR in generation 1 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 215: VAR Model Number: 216 with model VAR in generation 1 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 216: VAR Model Number: 217 with model VECM in generation 1 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 217: VECM Model Number: 218 with model VECM in generation 1 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 218: VECM Model Number: 219 with model VECM in generation 1 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 219: VECM Model Number: 220 with model VECM in generation 1 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 220: VECM Model Number: 221 with model FBProphet in generation 1 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 221: FBProphet Model Number: 222 with model FBProphet in generation 1 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 222: FBProphet Model Number: 223 with model FBProphet in generation 1 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 223: FBProphet Model Number: 224 with model FBProphet in generation 1 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 224: FBProphet New Generation: 2 of 10 Model Number: 225 with model LastValueNaive in generation 2 of 10 Model Number: 226 with model LastValueNaive in generation 2 of 10 Model Number: 227 with model LastValueNaive in generation 2 of 10 Model Number: 228 with model NVAR in generation 2 of 10 Model Number: 229 with model NVAR in generation 2 of 10 Model Number: 230 with model NVAR in generation 2 of 10 Model Number: 231 with model UnivariateMotif in generation 2 of 10 Model Number: 232 with model UnivariateMotif in generation 2 of 10 Model Number: 233 with model UnivariateMotif in generation 2 of 10 Model Number: 234 with model UnivariateMotif in generation 2 of 10 Model Number: 235 with model Theta in generation 2 of 10 Model Number: 236 with model Theta in generation 2 of 10 Model Number: 237 with model Theta in generation 2 of 10 Model Number: 238 with model Theta in generation 2 of 10 Model Number: 239 with model ConstantNaive in generation 2 of 10 Model Number: 240 with model ConstantNaive in generation 2 of 10 Model Number: 241 with model ConstantNaive in generation 2 of 10 Model Number: 242 with model UnobservedComponents in generation 2 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor supplied&#34;) in model 242: UnobservedComponents Model Number: 243 with model UnobservedComponents in generation 2 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor supplied&#34;) in model 243: UnobservedComponents Model Number: 244 with model UnobservedComponents in generation 2 of 10 Template Eval Error: LinAlgError(&#39;Schur decomposition solver error.&#39;) in model 244: UnobservedComponents Model Number: 245 with model GLS in generation 2 of 10 Model Number: 246 with model GLS in generation 2 of 10 Model Number: 247 with model GLS in generation 2 of 10 Model Number: 248 with model SeasonalNaive in generation 2 of 10 Model Number: 249 with model SeasonalNaive in generation 2 of 10 Model Number: 250 with model SeasonalNaive in generation 2 of 10 Model Number: 251 with model SeasonalNaive in generation 2 of 10 Model Number: 252 with model WindowRegression in generation 2 of 10 Model Number: 253 with model WindowRegression in generation 2 of 10 Model Number: 254 with model WindowRegression in generation 2 of 10 Model Number: 255 with model AverageValueNaive in generation 2 of 10 Model Number: 256 with model AverageValueNaive in generation 2 of 10 Model Number: 257 with model AverageValueNaive in generation 2 of 10 Model Number: 258 with model ARDL in generation 2 of 10 Model Number: 259 with model ARDL in generation 2 of 10 Model Number: 260 with model ARDL in generation 2 of 10 Model Number: 261 with model ARDL in generation 2 of 10 Model Number: 262 with model SectionalMotif in generation 2 of 10 Model Number: 263 with model SectionalMotif in generation 2 of 10 Model Number: 264 with model SectionalMotif in generation 2 of 10 Model Number: 265 with model SectionalMotif in generation 2 of 10 Template Eval Error: ValueError(&#34;regression_type==&#39;User&#39; but no future_regressor supplied&#34;) in model 265: SectionalMotif Model Number: 266 with model ETS in generation 2 of 10 Model Number: 267 with model ETS in generation 2 of 10 Model Number: 268 with model ETS in generation 2 of 10 Model Number: 269 with model ETS in generation 2 of 10 Model Number: 270 with model MultivariateMotif in generation 2 of 10 Model Number: 271 with model MultivariateMotif in generation 2 of 10 Model Number: 272 with model MultivariateMotif in generation 2 of 10 Model Number: 273 with model MultivariateMotif in generation 2 of 10 Model Number: 274 with model MultivariateRegression in generation 2 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.359e-02, tolerance: 1.304e-05 . Model Number: 275 with model MultivariateRegression in generation 2 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but not future_regressor supplied.&#34;) in model 275: MultivariateRegression Model Number: 276 with model MultivariateRegression in generation 2 of 10 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=-2)]: Done 194 tasks | elapsed: 0.7s [Parallel(n_jobs=-2)]: Done 200 out of 200 | elapsed: 0.7s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed: 0.0s finished . Model Number: 277 with model MultivariateRegression in generation 2 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but not future_regressor supplied.&#34;) in model 277: MultivariateRegression Model Number: 278 with model DatepartRegression in generation 2 of 10 Model Number: 279 with model DatepartRegression in generation 2 of 10 Model Number: 280 with model DatepartRegression in generation 2 of 10 Template Eval Error: ValueError(&#39;Failed to convert a NumPy array to a Tensor (Unsupported object type int).&#39;) in model 280: DatepartRegression Model Number: 281 with model GLM in generation 2 of 10 Model Number: 282 with model GLM in generation 2 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . Model Number: 283 with model GLM in generation 2 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . Model Number: 284 with model GLM in generation 2 of 10 Model Number: 285 with model VAR in generation 2 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 285: VAR . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families links.py:187: RuntimeWarning: overflow encountered in exp . Model Number: 286 with model VAR in generation 2 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 286: VAR Model Number: 287 with model VAR in generation 2 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 287: VAR Model Number: 288 with model VAR in generation 2 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 288: VAR Model Number: 289 with model VECM in generation 2 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 289: VECM Model Number: 290 with model VECM in generation 2 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 290: VECM Model Number: 291 with model VECM in generation 2 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 291: VECM Model Number: 292 with model VECM in generation 2 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 292: VECM Model Number: 293 with model FBProphet in generation 2 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 293: FBProphet Model Number: 294 with model FBProphet in generation 2 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 294: FBProphet Model Number: 295 with model FBProphet in generation 2 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 295: FBProphet Model Number: 296 with model FBProphet in generation 2 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 296: FBProphet New Generation: 3 of 10 Model Number: 297 with model LastValueNaive in generation 3 of 10 Model Number: 298 with model LastValueNaive in generation 3 of 10 Model Number: 299 with model LastValueNaive in generation 3 of 10 Model Number: 300 with model NVAR in generation 3 of 10 Model Number: 301 with model NVAR in generation 3 of 10 Model Number: 302 with model NVAR in generation 3 of 10 Model Number: 303 with model NVAR in generation 3 of 10 Model Number: 304 with model ARDL in generation 3 of 10 Model Number: 305 with model ARDL in generation 3 of 10 Model Number: 306 with model ARDL in generation 3 of 10 Model Number: 307 with model ARDL in generation 3 of 10 Model Number: 308 with model UnivariateMotif in generation 3 of 10 Model Number: 309 with model UnivariateMotif in generation 3 of 10 Model Number: 310 with model UnivariateMotif in generation 3 of 10 Model Number: 311 with model UnivariateMotif in generation 3 of 10 Model Number: 312 with model Theta in generation 3 of 10 Model Number: 313 with model Theta in generation 3 of 10 Model Number: 314 with model Theta in generation 3 of 10 Model Number: 315 with model Theta in generation 3 of 10 Model Number: 316 with model AverageValueNaive in generation 3 of 10 Model Number: 317 with model AverageValueNaive in generation 3 of 10 Model Number: 318 with model AverageValueNaive in generation 3 of 10 Model Number: 319 with model ConstantNaive in generation 3 of 10 Model Number: 320 with model ConstantNaive in generation 3 of 10 Model Number: 321 with model ConstantNaive in generation 3 of 10 Model Number: 322 with model UnobservedComponents in generation 3 of 10 Model Number: 323 with model UnobservedComponents in generation 3 of 10 Model Number: 324 with model UnobservedComponents in generation 3 of 10 Model Number: 325 with model GLM in generation 3 of 10 Template Eval Error: TypeError(&#34;ufunc &#39;isfinite&#39; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &#39;&#39;safe&#39;&#39;&#34;) in model 325: GLM Model Number: 326 with model GLM in generation 3 of 10 Model Number: 327 with model GLM in generation 3 of 10 Model Number: 328 with model GLM in generation 3 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families links.py:187: RuntimeWarning: overflow encountered in exp . Template Eval Error: TypeError(&#34;ufunc &#39;isfinite&#39; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &#39;&#39;safe&#39;&#39;&#34;) in model 328: GLM Model Number: 329 with model GLS in generation 3 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:67: RuntimeWarning: divide by zero encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:68: RuntimeWarning: divide by zero encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:68: RuntimeWarning: invalid value encountered in true_divide . Model Number: 330 with model GLS in generation 3 of 10 Model Number: 331 with model GLS in generation 3 of 10 Model Number: 332 with model SeasonalNaive in generation 3 of 10 Model Number: 333 with model SeasonalNaive in generation 3 of 10 Model Number: 334 with model SeasonalNaive in generation 3 of 10 Model Number: 335 with model SeasonalNaive in generation 3 of 10 Model Number: 336 with model WindowRegression in generation 3 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.833e+06, tolerance: 6.087e+05 . Model Number: 337 with model WindowRegression in generation 3 of 10 Model Number: 338 with model WindowRegression in generation 3 of 10 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.5s [Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed: 1.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished . Model Number: 339 with model SectionalMotif in generation 3 of 10 Model Number: 340 with model SectionalMotif in generation 3 of 10 Model Number: 341 with model SectionalMotif in generation 3 of 10 Model Number: 342 with model SectionalMotif in generation 3 of 10 Model Number: 343 with model ETS in generation 3 of 10 Model Number: 344 with model ETS in generation 3 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn utils validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: [&#39;Timestamp&#39;, &#39;str&#39;]. An error will be raised in 1.2. C: Users impep anaconda3 envs OpenCV lib site-packages numpy lib nanfunctions.py:997: RuntimeWarning: All-NaN slice encountered C: Users impep anaconda3 envs OpenCV lib site-packages numpy lib nanfunctions.py:1376: RuntimeWarning: All-NaN slice encountered C: Users impep anaconda3 envs OpenCV lib site-packages sklearn utils validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: [&#39;Timestamp&#39;, &#39;str&#39;]. An error will be raised in 1.2. . Template Eval Error: Exception(&#39;Transformer RobustScaler failed on fit&#39;) in model 344: ETS Model Number: 345 with model ETS in generation 3 of 10 Model Number: 346 with model ETS in generation 3 of 10 Model Number: 347 with model MultivariateMotif in generation 3 of 10 Model Number: 348 with model MultivariateMotif in generation 3 of 10 Model Number: 349 with model MultivariateMotif in generation 3 of 10 Model Number: 350 with model MultivariateMotif in generation 3 of 10 Model Number: 351 with model MultivariateRegression in generation 3 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn experimental enable_hist_gradient_boosting.py:17: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble. C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:67: RuntimeWarning: divide by zero encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:68: RuntimeWarning: divide by zero encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:68: RuntimeWarning: invalid value encountered in true_divide . Model Number: 352 with model MultivariateRegression in generation 3 of 10 Model Number: 353 with model MultivariateRegression in generation 3 of 10 Model Number: 354 with model MultivariateRegression in generation 3 of 10 Model Number: 355 with model DatepartRegression in generation 3 of 10 Model Number: 356 with model DatepartRegression in generation 3 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 356: DatepartRegression Model Number: 357 with model DatepartRegression in generation 3 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names C: Users impep anaconda3 envs OpenCV lib site-packages sklearn base.py:451: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names . Model Number: 358 with model VAR in generation 3 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 358: VAR Model Number: 359 with model VAR in generation 3 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 359: VAR Model Number: 360 with model VAR in generation 3 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 360: VAR Model Number: 361 with model VAR in generation 3 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 361: VAR Model Number: 362 with model VECM in generation 3 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 362: VECM Model Number: 363 with model VECM in generation 3 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 363: VECM Model Number: 364 with model VECM in generation 3 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 364: VECM Model Number: 365 with model VECM in generation 3 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 365: VECM Model Number: 366 with model FBProphet in generation 3 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 366: FBProphet Model Number: 367 with model FBProphet in generation 3 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 367: FBProphet Model Number: 368 with model FBProphet in generation 3 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 368: FBProphet Model Number: 369 with model FBProphet in generation 3 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 369: FBProphet New Generation: 4 of 10 Model Number: 370 with model LastValueNaive in generation 4 of 10 Model Number: 371 with model LastValueNaive in generation 4 of 10 Model Number: 372 with model NVAR in generation 4 of 10 Model Number: 373 with model NVAR in generation 4 of 10 Model Number: 374 with model NVAR in generation 4 of 10 Model Number: 375 with model NVAR in generation 4 of 10 Model Number: 376 with model ARDL in generation 4 of 10 Model Number: 377 with model ARDL in generation 4 of 10 Model Number: 378 with model ARDL in generation 4 of 10 Model Number: 379 with model ARDL in generation 4 of 10 Model Number: 380 with model Theta in generation 4 of 10 Model Number: 381 with model Theta in generation 4 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.295e+01, tolerance: 7.530e-02 . Model Number: 382 with model Theta in generation 4 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.194e+00, tolerance: 1.905e-02 . Model Number: 383 with model UnivariateMotif in generation 4 of 10 Model Number: 384 with model UnivariateMotif in generation 4 of 10 Model Number: 385 with model UnivariateMotif in generation 4 of 10 Model Number: 386 with model UnivariateMotif in generation 4 of 10 Model Number: 387 with model AverageValueNaive in generation 4 of 10 Model Number: 388 with model AverageValueNaive in generation 4 of 10 Model Number: 389 with model AverageValueNaive in generation 4 of 10 Model Number: 390 with model ConstantNaive in generation 4 of 10 Model Number: 391 with model ConstantNaive in generation 4 of 10 Model Number: 392 with model ConstantNaive in generation 4 of 10 Model Number: 393 with model GLM in generation 4 of 10 Model Number: 394 with model GLM in generation 4 of 10 Model Number: 395 with model GLM in generation 4 of 10 Model Number: 396 with model GLM in generation 4 of 10 Model Number: 397 with model UnobservedComponents in generation 4 of 10 Model Number: 398 with model UnobservedComponents in generation 4 of 10 Model Number: 399 with model UnobservedComponents in generation 4 of 10 Model Number: 400 with model MultivariateRegression in generation 4 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but not future_regressor supplied.&#34;) in model 400: MultivariateRegression Model Number: 401 with model MultivariateRegression in generation 4 of 10 Model Number: 402 with model MultivariateRegression in generation 4 of 10 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed: 0.1s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:67: RuntimeWarning: divide by zero encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:68: RuntimeWarning: divide by zero encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:68: RuntimeWarning: invalid value encountered in true_divide . Model Number: 403 with model MultivariateRegression in generation 4 of 10 Model Number: 404 with model GLS in generation 4 of 10 Model Number: 405 with model GLS in generation 4 of 10 Model Number: 406 with model GLS in generation 4 of 10 Template Eval Error: ValueError(&#39;zero-size array to reduction operation maximum which has no identity&#39;) in model 406: GLS Model Number: 407 with model SeasonalNaive in generation 4 of 10 Model Number: 408 with model SeasonalNaive in generation 4 of 10 Model Number: 409 with model SeasonalNaive in generation 4 of 10 Model Number: 410 with model SeasonalNaive in generation 4 of 10 Model Number: 411 with model WindowRegression in generation 4 of 10 Model Number: 412 with model WindowRegression in generation 4 of 10 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000148 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf Model Number: 413 with model WindowRegression in generation 4 of 10 Model Number: 414 with model SectionalMotif in generation 4 of 10 Model Number: 415 with model SectionalMotif in generation 4 of 10 Model Number: 416 with model SectionalMotif in generation 4 of 10 Model Number: 417 with model SectionalMotif in generation 4 of 10 Model Number: 418 with model MultivariateMotif in generation 4 of 10 Model Number: 419 with model MultivariateMotif in generation 4 of 10 Model Number: 420 with model MultivariateMotif in generation 4 of 10 Model Number: 421 with model MultivariateMotif in generation 4 of 10 Model Number: 422 with model ETS in generation 4 of 10 Model Number: 423 with model ETS in generation 4 of 10 Model Number: 424 with model ETS in generation 4 of 10 Model Number: 425 with model ETS in generation 4 of 10 Model Number: 426 with model DatepartRegression in generation 4 of 10 Template Eval Error: ValueError(&#39;Failed to convert a NumPy array to a Tensor (Unsupported object type int).&#39;) in model 426: DatepartRegression Model Number: 427 with model DatepartRegression in generation 4 of 10 Model Number: 428 with model DatepartRegression in generation 4 of 10 Epoch 1/100 Template Eval Error: TypeError(&#39;in user code: n n File &#34;C: Users impep anaconda3 envs OpenCV lib site-packages keras engine training.py&#34;, line 1021, in train_function * n return step_function(self, iterator) n File &#34;C: Users impep anaconda3 envs OpenCV lib site-packages keras engine training.py&#34;, line 1010, in step_function ** n outputs = model.distribute_strategy.run(run_step, args=(data,)) n File &#34;C: Users impep anaconda3 envs OpenCV lib site-packages keras engine training.py&#34;, line 1000, in run_step ** n outputs = model.train_step(data) n File &#34;C: Users impep anaconda3 envs OpenCV lib site-packages keras engine training.py&#34;, line 859, in train_step n y_pred = self(x, training=True) n File &#34;C: Users impep anaconda3 envs OpenCV lib site-packages keras utils traceback_utils.py&#34;, line 67, in error_handler n raise e.with_traceback(filtered_tb) from None n File &#34;C: Users impep anaconda3 envs OpenCV lib site-packages keras backend.py&#34;, line 2223, in dot n out = tf.matmul(x, y) n n TypeError: Exception encountered when calling layer &#34;forward_lstm&#34; (type LSTM). n n Input &#39;b &#39; of &#39;MatMul &#39; Op has type float32 that does not match type int64 of argument &#39;a &#39;. n n Call arguments received: n • inputs=tf.Tensor(shape=(None, 1, 7), dtype=int64) n • mask=None n • training=True n • initial_state=None n&#39;) in model 428: DatepartRegression Model Number: 429 with model VAR in generation 4 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 429: VAR Model Number: 430 with model VAR in generation 4 of 10 Template Eval Error: IndexError(&#39;tuple index out of range&#39;) in model 430: VAR Model Number: 431 with model VAR in generation 4 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 431: VAR Model Number: 432 with model VAR in generation 4 of 10 Template Eval Error: IndexError(&#39;tuple index out of range&#39;) in model 432: VAR Model Number: 433 with model VECM in generation 4 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 433: VECM Model Number: 434 with model VECM in generation 4 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 434: VECM Model Number: 435 with model VECM in generation 4 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 435: VECM Model Number: 436 with model VECM in generation 4 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 436: VECM Model Number: 437 with model FBProphet in generation 4 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 437: FBProphet Model Number: 438 with model FBProphet in generation 4 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 438: FBProphet Model Number: 439 with model FBProphet in generation 4 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 439: FBProphet Model Number: 440 with model FBProphet in generation 4 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 440: FBProphet . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.924e+08, tolerance: 6.133e+05 C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.669e+13, tolerance: 7.233e+11 . New Generation: 5 of 10 Model Number: 441 with model LastValueNaive in generation 5 of 10 Model Number: 442 with model LastValueNaive in generation 5 of 10 Model Number: 443 with model NVAR in generation 5 of 10 Model Number: 444 with model NVAR in generation 5 of 10 Model Number: 445 with model NVAR in generation 5 of 10 Model Number: 446 with model NVAR in generation 5 of 10 Model Number: 447 with model ARDL in generation 5 of 10 Model Number: 448 with model ARDL in generation 5 of 10 Model Number: 449 with model ARDL in generation 5 of 10 Model Number: 450 with model ARDL in generation 5 of 10 Model Number: 451 with model Theta in generation 5 of 10 Model Number: 452 with model Theta in generation 5 of 10 Model Number: 453 with model Theta in generation 5 of 10 Model Number: 454 with model Theta in generation 5 of 10 Model Number: 455 with model UnivariateMotif in generation 5 of 10 Model Number: 456 with model UnivariateMotif in generation 5 of 10 Template Eval Error: ValueError(&#39;kth(=20) out of bounds (11)&#39;) in model 456: UnivariateMotif Model Number: 457 with model UnivariateMotif in generation 5 of 10 Model Number: 458 with model UnivariateMotif in generation 5 of 10 Model Number: 459 with model AverageValueNaive in generation 5 of 10 Model Number: 460 with model AverageValueNaive in generation 5 of 10 Model Number: 461 with model AverageValueNaive in generation 5 of 10 Model Number: 462 with model ConstantNaive in generation 5 of 10 Model Number: 463 with model ConstantNaive in generation 5 of 10 Model Number: 464 with model ConstantNaive in generation 5 of 10 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-06-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-06-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-06-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-06-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-06-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-06-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-06-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-06-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-06-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-06-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-07-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-08-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-09-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-10-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-11-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2019-12-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-01-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-02-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-03-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-04-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-05-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-06-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-07-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-08-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-09-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-10-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-11-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2020-12-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-01-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-02-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-03-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-04-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-05-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-06-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-07-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-08-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-09-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-10-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-11-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2021-12-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-01-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-02-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-09 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-10 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-16 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-17 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-23 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-24 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-30 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-03-31 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-01 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-05 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-06 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-07 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-08 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-11 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-12 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-13 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-14 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-15 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-18 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-19 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-20 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-21 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-22 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-25 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-26 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-27 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-28 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-04-29 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-05-02 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-05-03 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for 2022-05-04 00:00:00 with 0 SinTrend failed with ValueError(&#39;array must not contain infs or NaNs&#39;) for Close with 0 Template Eval Error: Exception(&#39;Transformer SinTrend failed on fit&#39;) in model 464: ConstantNaive Model Number: 465 with model GLM in generation 5 of 10 Template Eval Error: TypeError(&#34;ufunc &#39;isfinite&#39; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &#39;&#39;safe&#39;&#39;&#34;) in model 465: GLM Model Number: 466 with model GLM in generation 5 of 10 Template Eval Error: TypeError(&#34;ufunc &#39;isfinite&#39; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &#39;&#39;safe&#39;&#39;&#34;) in model 466: GLM Model Number: 467 with model GLM in generation 5 of 10 Template Eval Error: TypeError(&#34;ufunc &#39;isfinite&#39; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &#39;&#39;safe&#39;&#39;&#34;) in model 467: GLM Model Number: 468 with model GLM in generation 5 of 10 Model Number: 469 with model UnobservedComponents in generation 5 of 10 Model Number: 470 with model UnobservedComponents in generation 5 of 10 Model Number: 471 with model UnobservedComponents in generation 5 of 10 Model Number: 472 with model MultivariateRegression in generation 5 of 10 Model Number: 473 with model MultivariateRegression in generation 5 of 10 Template Eval Error: ValueError(&#34;Input contains NaN, infinity or a value too large for dtype(&#39;float64&#39;).&#34;) in model 473: MultivariateRegression Model Number: 474 with model MultivariateRegression in generation 5 of 10 Model Number: 475 with model MultivariateRegression in generation 5 of 10 Model Number: 476 with model GLS in generation 5 of 10 Model Number: 477 with model GLS in generation 5 of 10 Model Number: 478 with model GLS in generation 5 of 10 Model Number: 479 with model SeasonalNaive in generation 5 of 10 Model Number: 480 with model SeasonalNaive in generation 5 of 10 Model Number: 481 with model SeasonalNaive in generation 5 of 10 Model Number: 482 with model SeasonalNaive in generation 5 of 10 Model Number: 483 with model WindowRegression in generation 5 of 10 Template Eval Error: ValueError(&#34;Input contains NaN, infinity or a value too large for dtype(&#39;float64&#39;).&#34;) in model 483: WindowRegression Model Number: 484 with model WindowRegression in generation 5 of 10 Template Eval Error: ValueError(&#34;Input contains NaN, infinity or a value too large for dtype(&#39;float64&#39;).&#34;) in model 484: WindowRegression Model Number: 485 with model WindowRegression in generation 5 of 10 Model Number: 486 with model SectionalMotif in generation 5 of 10 Model Number: 487 with model SectionalMotif in generation 5 of 10 Model Number: 488 with model SectionalMotif in generation 5 of 10 Model Number: 489 with model SectionalMotif in generation 5 of 10 Model Number: 490 with model MultivariateMotif in generation 5 of 10 Model Number: 491 with model MultivariateMotif in generation 5 of 10 Model Number: 492 with model MultivariateMotif in generation 5 of 10 Model Number: 493 with model MultivariateMotif in generation 5 of 10 Model Number: 494 with model ETS in generation 5 of 10 Model Number: 495 with model ETS in generation 5 of 10 ETS error ValueError(&#39;endog must be strictly positive when usingmultiplicative trend or seasonal components.&#39;) ETS failed on Close with ValueError(&#39;endog must be strictly positive when usingmultiplicative trend or seasonal components.&#39;) Model Number: 496 with model ETS in generation 5 of 10 Model Number: 497 with model ETS in generation 5 of 10 ETS error ValueError(&#39;endog must be strictly positive when usingmultiplicative trend or seasonal components.&#39;) ETS failed on Close with ValueError(&#39;endog must be strictly positive when usingmultiplicative trend or seasonal components.&#39;) Model Number: 498 with model DatepartRegression in generation 5 of 10 Template Eval Error: ValueError(&#39;Failed to convert a NumPy array to a Tensor (Unsupported object type int).&#39;) in model 498: DatepartRegression Model Number: 499 with model DatepartRegression in generation 5 of 10 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=-2)]: Done 194 tasks | elapsed: 0.7s [Parallel(n_jobs=-2)]: Done 300 out of 300 | elapsed: 1.1s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed: 0.0s finished . Model Number: 500 with model DatepartRegression in generation 5 of 10 Model Number: 501 with model VAR in generation 5 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 501: VAR Model Number: 502 with model VAR in generation 5 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 502: VAR Model Number: 503 with model VAR in generation 5 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 503: VAR Model Number: 504 with model VAR in generation 5 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 504: VAR Model Number: 505 with model VECM in generation 5 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 505: VECM Model Number: 506 with model VECM in generation 5 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 506: VECM Model Number: 507 with model VECM in generation 5 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 507: VECM Model Number: 508 with model VECM in generation 5 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 508: VECM Model Number: 509 with model FBProphet in generation 5 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 509: FBProphet Model Number: 510 with model FBProphet in generation 5 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 510: FBProphet Model Number: 511 with model FBProphet in generation 5 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 511: FBProphet Model Number: 512 with model FBProphet in generation 5 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 512: FBProphet New Generation: 6 of 10 Model Number: 513 with model LastValueNaive in generation 6 of 10 Model Number: 514 with model LastValueNaive in generation 6 of 10 Model Number: 515 with model LastValueNaive in generation 6 of 10 Model Number: 516 with model NVAR in generation 6 of 10 Model Number: 517 with model NVAR in generation 6 of 10 Model Number: 518 with model NVAR in generation 6 of 10 Template Eval Error: ValueError(&#34;Model returned NaN due to a preprocessing transformer {&#39;fillna&#39;: &#39;quadratic&#39;, &#39;transformations&#39;: {&#39;0&#39;: &#39;cffilter&#39;, &#39;1&#39;: &#39;IntermittentOccurrence&#39;, &#39;2&#39;: &#39;PositiveShift&#39;, &#39;3&#39;: &#39;Round&#39;}, &#39;transformation_params&#39;: {&#39;0&#39;: {}, &#39;1&#39;: {&#39;center&#39;: &#39;mean&#39;}, &#39;2&#39;: {}, &#39;3&#39;: {&#39;decimals&#39;: 2, &#39;on_transform&#39;: True, &#39;on_inverse&#39;: False}}}. fail_on_forecast_nan=True&#34;) in model 518: NVAR Model Number: 519 with model NVAR in generation 6 of 10 Model Number: 520 with model ARDL in generation 6 of 10 Model Number: 521 with model ARDL in generation 6 of 10 Model Number: 522 with model ARDL in generation 6 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but future_regressor not supplied&#34;) in model 522: ARDL Model Number: 523 with model ARDL in generation 6 of 10 Model Number: 524 with model Theta in generation 6 of 10 Model Number: 525 with model Theta in generation 6 of 10 Model Number: 526 with model Theta in generation 6 of 10 Model Number: 527 with model Theta in generation 6 of 10 Model Number: 528 with model UnivariateMotif in generation 6 of 10 Model Number: 529 with model UnivariateMotif in generation 6 of 10 Model Number: 530 with model UnivariateMotif in generation 6 of 10 Model Number: 531 with model UnivariateMotif in generation 6 of 10 Model Number: 532 with model AverageValueNaive in generation 6 of 10 Model Number: 533 with model AverageValueNaive in generation 6 of 10 Model Number: 534 with model AverageValueNaive in generation 6 of 10 Model Number: 535 with model ConstantNaive in generation 6 of 10 Model Number: 536 with model ConstantNaive in generation 6 of 10 Model Number: 537 with model ConstantNaive in generation 6 of 10 Model Number: 538 with model GLM in generation 6 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . Model Number: 539 with model GLM in generation 6 of 10 Model Number: 540 with model GLM in generation 6 of 10 Model Number: 541 with model GLM in generation 6 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . Model Number: 542 with model UnobservedComponents in generation 6 of 10 Model Number: 543 with model UnobservedComponents in generation 6 of 10 Model Number: 544 with model UnobservedComponents in generation 6 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor supplied&#34;) in model 544: UnobservedComponents Model Number: 545 with model MultivariateRegression in generation 6 of 10 Model Number: 546 with model MultivariateRegression in generation 6 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but not future_regressor supplied.&#34;) in model 546: MultivariateRegression Model Number: 547 with model MultivariateRegression in generation 6 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but not future_regressor supplied.&#34;) in model 547: MultivariateRegression Model Number: 548 with model MultivariateRegression in generation 6 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but not future_regressor supplied.&#34;) in model 548: MultivariateRegression Model Number: 549 with model GLS in generation 6 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn utils validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: [&#39;Timestamp&#39;, &#39;str&#39;]. An error will be raised in 1.2. C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:461: RuntimeWarning: All-NaN slice encountered C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:462: RuntimeWarning: All-NaN slice encountered C: Users impep anaconda3 envs OpenCV lib site-packages sklearn utils validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: [&#39;Timestamp&#39;, &#39;str&#39;]. An error will be raised in 1.2. . Template Eval Error: Exception(&#39;Transformer MinMaxScaler failed on fit&#39;) in model 549: GLS Model Number: 550 with model GLS in generation 6 of 10 Model Number: 551 with model GLS in generation 6 of 10 Model Number: 552 with model SeasonalNaive in generation 6 of 10 Model Number: 553 with model SeasonalNaive in generation 6 of 10 Model Number: 554 with model SeasonalNaive in generation 6 of 10 Model Number: 555 with model SeasonalNaive in generation 6 of 10 Model Number: 556 with model WindowRegression in generation 6 of 10 Model Number: 557 with model WindowRegression in generation 6 of 10 Model Number: 558 with model WindowRegression in generation 6 of 10 Epoch 1/100 Template Eval Error: ValueError(&#39;in user code: n n File &#34;C: Users impep anaconda3 envs OpenCV lib site-packages keras engine training.py&#34;, line 1021, in train_function * n return step_function(self, iterator) n File &#34;C: Users impep anaconda3 envs OpenCV lib site-packages keras engine training.py&#34;, line 1010, in step_function ** n outputs = model.distribute_strategy.run(run_step, args=(data,)) n File &#34;C: Users impep anaconda3 envs OpenCV lib site-packages keras engine training.py&#34;, line 1000, in run_step ** n outputs = model.train_step(data) n File &#34;C: Users impep anaconda3 envs OpenCV lib site-packages keras engine training.py&#34;, line 859, in train_step n y_pred = self(x, training=True) n File &#34;C: Users impep anaconda3 envs OpenCV lib site-packages keras utils traceback_utils.py&#34;, line 67, in error_handler n raise e.with_traceback(filtered_tb) from None n n ValueError: Exception encountered when calling layer &#34;residual_wrapper&#34; (type ResidualWrapper). n n in user code: n n File &#34;C: Users impep anaconda3 envs OpenCV lib site-packages autots models dnn.py&#34;, line 31, in call * n return inputs + delta n n ValueError: Dimensions must be equal, but are 24 and 30 for &#39;{{node residual_wrapper/add}} = AddV2[T=DT_FLOAT](IteratorGetNext, residual_wrapper/sequential/dense_1/BiasAdd) &#39; with input shapes: [?,1,24], [?,30]. n n n Call arguments received: n • inputs=tf.Tensor(shape=(None, 1, 24), dtype=float32) n • args=&lt;class &#39;inspect._empty &#39;&gt; n • kwargs={ &#39;training &#39;: &#39;True &#39;} n&#39;) in model 558: WindowRegression Model Number: 559 with model MultivariateMotif in generation 6 of 10 Model Number: 560 with model MultivariateMotif in generation 6 of 10 Model Number: 561 with model MultivariateMotif in generation 6 of 10 Model Number: 562 with model MultivariateMotif in generation 6 of 10 Model Number: 563 with model SectionalMotif in generation 6 of 10 Model Number: 564 with model SectionalMotif in generation 6 of 10 Model Number: 565 with model SectionalMotif in generation 6 of 10 Model Number: 566 with model SectionalMotif in generation 6 of 10 Model Number: 567 with model ETS in generation 6 of 10 Model Number: 568 with model ETS in generation 6 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.018e+00, tolerance: 1.951e-02 . Model Number: 569 with model ETS in generation 6 of 10 Model Number: 570 with model ETS in generation 6 of 10 Model Number: 571 with model DatepartRegression in generation 6 of 10 Template Eval Error: ValueError(&#39;Failed to convert a NumPy array to a Tensor (Unsupported object type int).&#39;) in model 571: DatepartRegression Model Number: 572 with model DatepartRegression in generation 6 of 10 Model Number: 573 with model DatepartRegression in generation 6 of 10 Model Number: 574 with model VAR in generation 6 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 574: VAR Model Number: 575 with model VAR in generation 6 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 575: VAR Model Number: 576 with model VAR in generation 6 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 576: VAR Model Number: 577 with model VAR in generation 6 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 577: VAR Model Number: 578 with model VECM in generation 6 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 578: VECM Model Number: 579 with model VECM in generation 6 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 579: VECM Model Number: 580 with model VECM in generation 6 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 580: VECM Model Number: 581 with model VECM in generation 6 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 581: VECM Model Number: 582 with model FBProphet in generation 6 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 582: FBProphet Model Number: 583 with model FBProphet in generation 6 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 583: FBProphet Model Number: 584 with model FBProphet in generation 6 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 584: FBProphet Model Number: 585 with model FBProphet in generation 6 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 585: FBProphet New Generation: 7 of 10 Model Number: 586 with model LastValueNaive in generation 7 of 10 Model Number: 587 with model LastValueNaive in generation 7 of 10 Model Number: 588 with model LastValueNaive in generation 7 of 10 Model Number: 589 with model ARDL in generation 7 of 10 Model Number: 590 with model ARDL in generation 7 of 10 Model Number: 591 with model ARDL in generation 7 of 10 Model Number: 592 with model ARDL in generation 7 of 10 Model Number: 593 with model NVAR in generation 7 of 10 Model Number: 594 with model NVAR in generation 7 of 10 Model Number: 595 with model NVAR in generation 7 of 10 Model Number: 596 with model NVAR in generation 7 of 10 Model Number: 597 with model Theta in generation 7 of 10 Model Number: 598 with model Theta in generation 7 of 10 Model Number: 599 with model Theta in generation 7 of 10 Model Number: 600 with model Theta in generation 7 of 10 Model Number: 601 with model UnivariateMotif in generation 7 of 10 Model Number: 602 with model UnivariateMotif in generation 7 of 10 Model Number: 603 with model UnivariateMotif in generation 7 of 10 Model Number: 604 with model UnivariateMotif in generation 7 of 10 Model Number: 605 with model AverageValueNaive in generation 7 of 10 Model Number: 606 with model AverageValueNaive in generation 7 of 10 Model Number: 607 with model AverageValueNaive in generation 7 of 10 Model Number: 608 with model ConstantNaive in generation 7 of 10 Model Number: 609 with model ConstantNaive in generation 7 of 10 Model Number: 610 with model ConstantNaive in generation 7 of 10 Model Number: 611 with model GLM in generation 7 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.654e+08, tolerance: 6.081e+05 C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families links.py:187: RuntimeWarning: overflow encountered in exp . Model Number: 612 with model GLM in generation 7 of 10 Model Number: 613 with model GLM in generation 7 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.748e+08, tolerance: 5.158e+05 . Model Number: 614 with model GLM in generation 7 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families links.py:187: RuntimeWarning: overflow encountered in exp . Model Number: 615 with model UnobservedComponents in generation 7 of 10 Model Number: 616 with model UnobservedComponents in generation 7 of 10 Model Number: 617 with model UnobservedComponents in generation 7 of 10 Model Number: 618 with model MultivariateRegression in generation 7 of 10 Template Eval Error: ValueError(&#34;Input contains NaN, infinity or a value too large for dtype(&#39;float64&#39;).&#34;) in model 618: MultivariateRegression Model Number: 619 with model MultivariateRegression in generation 7 of 10 Model Number: 620 with model MultivariateRegression in generation 7 of 10 Model Number: 621 with model MultivariateRegression in generation 7 of 10 Model Number: 622 with model GLS in generation 7 of 10 Model Number: 623 with model GLS in generation 7 of 10 Model Number: 624 with model GLS in generation 7 of 10 Model Number: 625 with model ETS in generation 7 of 10 Model Number: 626 with model ETS in generation 7 of 10 Model Number: 627 with model ETS in generation 7 of 10 Model Number: 628 with model ETS in generation 7 of 10 Model Number: 629 with model SeasonalNaive in generation 7 of 10 Model Number: 630 with model SeasonalNaive in generation 7 of 10 Model Number: 631 with model SeasonalNaive in generation 7 of 10 Model Number: 632 with model SeasonalNaive in generation 7 of 10 Model Number: 633 with model WindowRegression in generation 7 of 10 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.4s [Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed: 1.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished . Model Number: 634 with model WindowRegression in generation 7 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 634: WindowRegression Model Number: 635 with model WindowRegression in generation 7 of 10 Model Number: 636 with model MultivariateMotif in generation 7 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn neural_network _multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn&#39;t converged yet. . Model Number: 637 with model MultivariateMotif in generation 7 of 10 Model Number: 638 with model MultivariateMotif in generation 7 of 10 Model Number: 639 with model MultivariateMotif in generation 7 of 10 Model Number: 640 with model SectionalMotif in generation 7 of 10 Model Number: 641 with model SectionalMotif in generation 7 of 10 Model Number: 642 with model SectionalMotif in generation 7 of 10 Model Number: 643 with model SectionalMotif in generation 7 of 10 Model Number: 644 with model DatepartRegression in generation 7 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 644: DatepartRegression Model Number: 645 with model DatepartRegression in generation 7 of 10 Model Number: 646 with model DatepartRegression in generation 7 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 646: DatepartRegression Model Number: 647 with model VAR in generation 7 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 647: VAR Model Number: 648 with model VAR in generation 7 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 648: VAR Model Number: 649 with model VAR in generation 7 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 649: VAR Model Number: 650 with model VAR in generation 7 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 650: VAR Model Number: 651 with model VECM in generation 7 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 651: VECM Model Number: 652 with model VECM in generation 7 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 652: VECM Model Number: 653 with model VECM in generation 7 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 653: VECM Model Number: 654 with model VECM in generation 7 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 654: VECM Model Number: 655 with model FBProphet in generation 7 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 655: FBProphet Model Number: 656 with model FBProphet in generation 7 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 656: FBProphet Model Number: 657 with model FBProphet in generation 7 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 657: FBProphet Model Number: 658 with model FBProphet in generation 7 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 658: FBProphet New Generation: 8 of 10 Model Number: 659 with model LastValueNaive in generation 8 of 10 Model Number: 660 with model LastValueNaive in generation 8 of 10 Model Number: 661 with model LastValueNaive in generation 8 of 10 Model Number: 662 with model ARDL in generation 8 of 10 Model Number: 663 with model ARDL in generation 8 of 10 Model Number: 664 with model ARDL in generation 8 of 10 Model Number: 665 with model ARDL in generation 8 of 10 Model Number: 666 with model NVAR in generation 8 of 10 Model Number: 667 with model NVAR in generation 8 of 10 Model Number: 668 with model NVAR in generation 8 of 10 Model Number: 669 with model NVAR in generation 8 of 10 Model Number: 670 with model Theta in generation 8 of 10 Model Number: 671 with model Theta in generation 8 of 10 Model Number: 672 with model Theta in generation 8 of 10 Model Number: 673 with model Theta in generation 8 of 10 Model Number: 674 with model UnivariateMotif in generation 8 of 10 Model Number: 675 with model UnivariateMotif in generation 8 of 10 Model Number: 676 with model UnivariateMotif in generation 8 of 10 Model Number: 677 with model AverageValueNaive in generation 8 of 10 Model Number: 678 with model AverageValueNaive in generation 8 of 10 Model Number: 679 with model AverageValueNaive in generation 8 of 10 Model Number: 680 with model ConstantNaive in generation 8 of 10 Model Number: 681 with model ConstantNaive in generation 8 of 10 Model Number: 682 with model ConstantNaive in generation 8 of 10 Model Number: 683 with model GLM in generation 8 of 10 Model Number: 684 with model GLM in generation 8 of 10 Template Eval Error: ValueError(&#39;NaN, inf or invalid value detected in weights, estimation infeasible.&#39;) in model 684: GLM Model Number: 685 with model GLM in generation 8 of 10 Template Eval Error: ValueError(&#39;regression_type=user and no future_regressor passed&#39;) in model 685: GLM Model Number: 686 with model GLM in generation 8 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families family.py:1346: RuntimeWarning: invalid value encountered in log C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families links.py:516: RuntimeWarning: overflow encountered in exp C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families family.py:1346: RuntimeWarning: divide by zero encountered in log C: Users impep anaconda3 envs OpenCV lib site-packages numpy core fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families family.py:132: RuntimeWarning: invalid value encountered in multiply C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:1212: RuntimeWarning: invalid value encountered in multiply . Model Number: 687 with model UnobservedComponents in generation 8 of 10 Model Number: 688 with model UnobservedComponents in generation 8 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor supplied&#34;) in model 688: UnobservedComponents Model Number: 689 with model UnobservedComponents in generation 8 of 10 Model Number: 690 with model MultivariateRegression in generation 8 of 10 Model Number: 691 with model MultivariateRegression in generation 8 of 10 Model Number: 692 with model MultivariateRegression in generation 8 of 10 Template Eval Error: ValueError(&#34;Input contains NaN, infinity or a value too large for dtype(&#39;float64&#39;).&#34;) in model 692: MultivariateRegression Model Number: 693 with model MultivariateRegression in generation 8 of 10 Model Number: 694 with model SeasonalNaive in generation 8 of 10 Model Number: 695 with model SeasonalNaive in generation 8 of 10 Model Number: 696 with model SeasonalNaive in generation 8 of 10 Model Number: 697 with model SeasonalNaive in generation 8 of 10 Model Number: 698 with model GLS in generation 8 of 10 Model Number: 699 with model GLS in generation 8 of 10 Model Number: 700 with model GLS in generation 8 of 10 Model Number: 701 with model ETS in generation 8 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.961e+08, tolerance: 6.167e+05 . Model Number: 702 with model ETS in generation 8 of 10 Model Number: 703 with model ETS in generation 8 of 10 Model Number: 704 with model ETS in generation 8 of 10 Model Number: 705 with model WindowRegression in generation 8 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 705: WindowRegression Model Number: 706 with model WindowRegression in generation 8 of 10 Model Number: 707 with model WindowRegression in generation 8 of 10 Model Number: 708 with model MultivariateMotif in generation 8 of 10 Model Number: 709 with model MultivariateMotif in generation 8 of 10 Model Number: 710 with model MultivariateMotif in generation 8 of 10 Model Number: 711 with model MultivariateMotif in generation 8 of 10 Model Number: 712 with model SectionalMotif in generation 8 of 10 Template Eval Error: ValueError(&#34;regression_type==&#39;User&#39; but no future_regressor supplied&#34;) in model 712: SectionalMotif Model Number: 713 with model SectionalMotif in generation 8 of 10 Model Number: 714 with model SectionalMotif in generation 8 of 10 Model Number: 715 with model SectionalMotif in generation 8 of 10 Template Eval Error: ValueError(&#34;regression_type==&#39;User&#39; but no future_regressor supplied&#34;) in model 715: SectionalMotif Model Number: 716 with model DatepartRegression in generation 8 of 10 Template Eval Error: ValueError(&#39;Failed to convert a NumPy array to a Tensor (Unsupported object type int).&#39;) in model 716: DatepartRegression Model Number: 717 with model DatepartRegression in generation 8 of 10 Model Number: 718 with model DatepartRegression in generation 8 of 10 Model Number: 719 with model VAR in generation 8 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 719: VAR Model Number: 720 with model VAR in generation 8 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 720: VAR Model Number: 721 with model VAR in generation 8 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 721: VAR Model Number: 722 with model VAR in generation 8 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 722: VAR Model Number: 723 with model VECM in generation 8 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 723: VECM Model Number: 724 with model VECM in generation 8 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 724: VECM Model Number: 725 with model VECM in generation 8 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 725: VECM Model Number: 726 with model VECM in generation 8 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 726: VECM Model Number: 727 with model FBProphet in generation 8 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 727: FBProphet Model Number: 728 with model FBProphet in generation 8 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 728: FBProphet Model Number: 729 with model FBProphet in generation 8 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 729: FBProphet Model Number: 730 with model FBProphet in generation 8 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 730: FBProphet New Generation: 9 of 10 Model Number: 731 with model LastValueNaive in generation 9 of 10 Model Number: 732 with model LastValueNaive in generation 9 of 10 Model Number: 733 with model LastValueNaive in generation 9 of 10 Model Number: 734 with model MultivariateMotif in generation 9 of 10 Model Number: 735 with model MultivariateMotif in generation 9 of 10 Model Number: 736 with model MultivariateMotif in generation 9 of 10 Model Number: 737 with model MultivariateMotif in generation 9 of 10 Model Number: 738 with model ARDL in generation 9 of 10 Model Number: 739 with model ARDL in generation 9 of 10 Model Number: 740 with model ARDL in generation 9 of 10 Model Number: 741 with model ARDL in generation 9 of 10 Model Number: 742 with model NVAR in generation 9 of 10 Model Number: 743 with model NVAR in generation 9 of 10 Model Number: 744 with model NVAR in generation 9 of 10 Model Number: 745 with model NVAR in generation 9 of 10 Model Number: 746 with model Theta in generation 9 of 10 Model Number: 747 with model Theta in generation 9 of 10 Model Number: 748 with model Theta in generation 9 of 10 Model Number: 749 with model Theta in generation 9 of 10 Model Number: 750 with model UnivariateMotif in generation 9 of 10 Model Number: 751 with model UnivariateMotif in generation 9 of 10 Model Number: 752 with model UnivariateMotif in generation 9 of 10 Model Number: 753 with model UnivariateMotif in generation 9 of 10 Model Number: 754 with model ConstantNaive in generation 9 of 10 Model Number: 755 with model ConstantNaive in generation 9 of 10 Model Number: 756 with model AverageValueNaive in generation 9 of 10 Model Number: 757 with model AverageValueNaive in generation 9 of 10 Model Number: 758 with model AverageValueNaive in generation 9 of 10 Model Number: 759 with model GLM in generation 9 of 10 Model Number: 760 with model GLM in generation 9 of 10 Template Eval Error: ValueError(&#39;regression_type=user and no future_regressor passed&#39;) in model 760: GLM Model Number: 761 with model GLM in generation 9 of 10 Model Number: 762 with model UnobservedComponents in generation 9 of 10 Model Number: 763 with model UnobservedComponents in generation 9 of 10 Model Number: 764 with model UnobservedComponents in generation 9 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor supplied&#34;) in model 764: UnobservedComponents Model Number: 765 with model MultivariateRegression in generation 9 of 10 Model Number: 766 with model MultivariateRegression in generation 9 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but not future_regressor supplied.&#34;) in model 766: MultivariateRegression Model Number: 767 with model MultivariateRegression in generation 9 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but not future_regressor supplied.&#34;) in model 767: MultivariateRegression Model Number: 768 with model MultivariateRegression in generation 9 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but not future_regressor supplied.&#34;) in model 768: MultivariateRegression Model Number: 769 with model SeasonalNaive in generation 9 of 10 Model Number: 770 with model SeasonalNaive in generation 9 of 10 Model Number: 771 with model SeasonalNaive in generation 9 of 10 Model Number: 772 with model SeasonalNaive in generation 9 of 10 Model Number: 773 with model ETS in generation 9 of 10 Model Number: 774 with model ETS in generation 9 of 10 Template Eval Error: KeyError(Timestamp(&#39;2019-06-17 00:00:00&#39;, freq=&#39;B&#39;)) in model 774: ETS Model Number: 775 with model ETS in generation 9 of 10 Model Number: 776 with model ETS in generation 9 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn utils validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: [&#39;Timestamp&#39;, &#39;str&#39;]. An error will be raised in 1.2. C: Users impep anaconda3 envs OpenCV lib site-packages sklearn utils extmath.py:985: RuntimeWarning: invalid value encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages sklearn utils extmath.py:990: RuntimeWarning: invalid value encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages sklearn utils extmath.py:1020: RuntimeWarning: invalid value encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages sklearn utils validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: [&#39;Timestamp&#39;, &#39;str&#39;]. An error will be raised in 1.2. . Template Eval Error: Exception(&#39;Transformer StandardScaler failed on fit&#39;) in model 776: ETS Model Number: 777 with model GLS in generation 9 of 10 Model Number: 778 with model GLS in generation 9 of 10 Model Number: 779 with model GLS in generation 9 of 10 Model Number: 780 with model WindowRegression in generation 9 of 10 Epoch 1/50 19/19 [==============================] - 17s 162ms/step - loss: 101.1094 - val_loss: 113.5622 Epoch 2/50 19/19 [==============================] - 0s 22ms/step - loss: 100.5323 - val_loss: 113.5624 Epoch 3/50 19/19 [==============================] - 0s 24ms/step - loss: 100.6309 - val_loss: 113.5632 Epoch 4/50 19/19 [==============================] - 0s 22ms/step - loss: 100.4794 - val_loss: 113.5643 Epoch 5/50 19/19 [==============================] - 0s 26ms/step - loss: 100.5529 - val_loss: 113.5644 Epoch 6/50 19/19 [==============================] - 1s 28ms/step - loss: 100.2369 - val_loss: 113.5649 Epoch 7/50 19/19 [==============================] - 1s 31ms/step - loss: 100.0577 - val_loss: 113.5652 Epoch 8/50 19/19 [==============================] - 1s 27ms/step - loss: 100.1277 - val_loss: 113.5663 Epoch 9/50 19/19 [==============================] - 1s 30ms/step - loss: 100.4871 - val_loss: 113.5661 Epoch 10/50 19/19 [==============================] - 0s 22ms/step - loss: 100.5493 - val_loss: 113.5679 Epoch 11/50 19/19 [==============================] - 0s 23ms/step - loss: 100.5940 - val_loss: 113.5677 Model Number: 781 with model WindowRegression in generation 9 of 10 Model Number: 782 with model WindowRegression in generation 9 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 782: WindowRegression Model Number: 783 with model SectionalMotif in generation 9 of 10 Model Number: 784 with model SectionalMotif in generation 9 of 10 Model Number: 785 with model SectionalMotif in generation 9 of 10 Model Number: 786 with model SectionalMotif in generation 9 of 10 Model Number: 787 with model DatepartRegression in generation 9 of 10 Model Number: 788 with model DatepartRegression in generation 9 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor passed&#34;) in model 788: DatepartRegression Model Number: 789 with model DatepartRegression in generation 9 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.609e+08, tolerance: 5.206e+05 . Model Number: 790 with model VAR in generation 9 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 790: VAR Model Number: 791 with model VAR in generation 9 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 791: VAR Model Number: 792 with model VAR in generation 9 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 792: VAR Model Number: 793 with model VAR in generation 9 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VAR&#39;) in model 793: VAR Model Number: 794 with model VECM in generation 9 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 794: VECM Model Number: 795 with model VECM in generation 9 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but no future_regressor supplied&#34;) in model 795: VECM Model Number: 796 with model VECM in generation 9 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 796: VECM Model Number: 797 with model VECM in generation 9 of 10 Template Eval Error: ValueError(&#39;Only gave one variable to VECM&#39;) in model 797: VECM Model Number: 798 with model FBProphet in generation 9 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 798: FBProphet Model Number: 799 with model FBProphet in generation 9 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 799: FBProphet Model Number: 800 with model FBProphet in generation 9 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 800: FBProphet Model Number: 801 with model FBProphet in generation 9 of 10 Template Eval Error: ModuleNotFoundError(&#34;No module named &#39;fbprophet&#39;&#34;) in model 801: FBProphet New Generation: 10 of 10 Model Number: 802 with model LastValueNaive in generation 10 of 10 Model Number: 803 with model LastValueNaive in generation 10 of 10 Model Number: 804 with model LastValueNaive in generation 10 of 10 Model Number: 805 with model MultivariateMotif in generation 10 of 10 Model Number: 806 with model MultivariateMotif in generation 10 of 10 Model Number: 807 with model MultivariateMotif in generation 10 of 10 Model Number: 808 with model MultivariateMotif in generation 10 of 10 Model Number: 809 with model ARDL in generation 10 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but future_regressor not supplied&#34;) in model 809: ARDL Model Number: 810 with model ARDL in generation 10 of 10 Model Number: 811 with model ARDL in generation 10 of 10 Template Eval Error: ValueError(&#34;regression_type=&#39;User&#39; but future_regressor not supplied&#34;) in model 811: ARDL Model Number: 812 with model ARDL in generation 10 of 10 Model Number: 813 with model NVAR in generation 10 of 10 Model Number: 814 with model NVAR in generation 10 of 10 Model Number: 815 with model NVAR in generation 10 of 10 Model Number: 816 with model NVAR in generation 10 of 10 Model Number: 817 with model Theta in generation 10 of 10 Model Number: 818 with model Theta in generation 10 of 10 Model Number: 819 with model Theta in generation 10 of 10 Model Number: 820 with model Theta in generation 10 of 10 Model Number: 821 with model UnivariateMotif in generation 10 of 10 Model Number: 822 with model UnivariateMotif in generation 10 of 10 Model Number: 823 with model UnivariateMotif in generation 10 of 10 Model Number: 824 with model UnivariateMotif in generation 10 of 10 Model Number: 825 with model ConstantNaive in generation 10 of 10 Model Number: 826 with model ConstantNaive in generation 10 of 10 Model Number: 827 with model ConstantNaive in generation 10 of 10 Model Number: 828 with model AverageValueNaive in generation 10 of 10 Model Number: 829 with model AverageValueNaive in generation 10 of 10 Model Number: 830 with model AverageValueNaive in generation 10 of 10 Model Number: 831 with model UnobservedComponents in generation 10 of 10 Template Eval Error: Exception(&#39;Transformer DatepartRegression failed on fit&#39;) in model 831: UnobservedComponents Model Number: 832 with model UnobservedComponents in generation 10 of 10 Template Eval Error: ValueError(&#34;&#39;shape&#39; elements cannot be negative&#34;) in model 832: UnobservedComponents Model Number: 833 with model UnobservedComponents in generation 10 of 10 Model Number: 834 with model GLM in generation 10 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages scipy interpolate polyint.py:545: RuntimeWarning: overflow encountered in multiply C: Users impep anaconda3 envs OpenCV lib site-packages scipy interpolate polyint.py:546: RuntimeWarning: overflow encountered in reduce C: Users impep anaconda3 envs OpenCV lib site-packages scipy interpolate polyint.py:643: RuntimeWarning: invalid value encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . Model Number: 835 with model GLM in generation 10 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . Model Number: 836 with model GLM in generation 10 of 10 . C: Users impep anaconda3 envs OpenCV lib site-packages scipy interpolate polyint.py:545: RuntimeWarning: overflow encountered in multiply C: Users impep anaconda3 envs OpenCV lib site-packages scipy interpolate polyint.py:546: RuntimeWarning: overflow encountered in reduce C: Users impep anaconda3 envs OpenCV lib site-packages scipy interpolate polyint.py:643: RuntimeWarning: invalid value encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . Template Eval Error: TypeError(&#34;ufunc &#39;isfinite&#39; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &#39;&#39;safe&#39;&#39;&#34;) in model 836: GLM Model Number: 837 with model GLM in generation 10 of 10 Model Number: 838 with model MultivariateRegression in generation 10 of 10 Model Number: 839 with model MultivariateRegression in generation 10 of 10 Model Number: 840 with model MultivariateRegression in generation 10 of 10 Model Number: 841 with model MultivariateRegression in generation 10 of 10 Model Number: 842 with model SeasonalNaive in generation 10 of 10 Model Number: 843 with model SeasonalNaive in generation 10 of 10 Model Number: 844 with model SeasonalNaive in generation 10 of 10 Model Number: 845 with model SeasonalNaive in generation 10 of 10 Model Number: 846 with model ETS in generation 10 of 10 Model Number: 847 with model ETS in generation 10 of 10 Model Number: 848 with model ETS in generation 10 of 10 Model Number: 849 with model ETS in generation 10 of 10 Model Number: 850 with model GLS in generation 10 of 10 Model Number: 851 with model GLS in generation 10 of 10 Model Number: 852 with model GLS in generation 10 of 10 Model Number: 853 with model WindowRegression in generation 10 of 10 Template Eval Error: ValueError(&#34;Input contains NaN, infinity or a value too large for dtype(&#39;float64&#39;).&#34;) in model 853: WindowRegression Model Number: 854 with model WindowRegression in generation 10 of 10 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002516 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf Model Number: 855 with model WindowRegression in generation 10 of 10 Template Eval Error: ValueError(&#34;Input contains NaN, infinity or a value too large for dtype(&#39;float64&#39;).&#34;) in model 855: WindowRegression Model Number: 856 with model Ensemble in generation 11 of Ensembles Model Number: 857 with model Ensemble in generation 11 of Ensembles Model Number: 858 with model Ensemble in generation 11 of Ensembles Model Number: 859 with model Ensemble in generation 11 of Ensembles Model Number: 860 with model Ensemble in generation 11 of Ensembles Model Number: 861 with model Ensemble in generation 11 of Ensembles Model Number: 862 with model Ensemble in generation 11 of Ensembles Model Number: 863 with model Ensemble in generation 11 of Ensembles Validation Round: 1 Model Number: 1 of 130 with model Ensemble for Validation 1 📈 1 - Ensemble with avg smape 2.42: Model Number: 2 of 130 with model Ensemble for Validation 1 2 - Ensemble with avg smape 2.42: Model Number: 3 of 130 with model Ensemble for Validation 1 📈 3 - Ensemble with avg smape 2.31: Model Number: 4 of 130 with model Ensemble for Validation 1 📈 4 - Ensemble with avg smape 2.18: Model Number: 5 of 130 with model Ensemble for Validation 1 📈 5 - Ensemble with avg smape 2.14: Model Number: 6 of 130 with model LastValueNaive for Validation 1 6 - LastValueNaive with avg smape 2.27: Model Number: 7 of 130 with model MultivariateRegression for Validation 1 7 - MultivariateRegression with avg smape 2.2: Model Number: 8 of 130 with model LastValueNaive for Validation 1 8 - LastValueNaive with avg smape 2.57: Model Number: 9 of 130 with model LastValueNaive for Validation 1 📈 9 - LastValueNaive with avg smape 2.05: Model Number: 10 of 130 with model LastValueNaive for Validation 1 10 - LastValueNaive with avg smape 2.05: Model Number: 11 of 130 with model Ensemble for Validation 1 11 - Ensemble with avg smape 2.05: Model Number: 12 of 130 with model LastValueNaive for Validation 1 12 - LastValueNaive with avg smape 2.05: Model Number: 13 of 130 with model LastValueNaive for Validation 1 13 - LastValueNaive with avg smape 2.06: Model Number: 14 of 130 with model LastValueNaive for Validation 1 14 - LastValueNaive with avg smape 2.1: Model Number: 15 of 130 with model LastValueNaive for Validation 1 15 - LastValueNaive with avg smape 2.29: Model Number: 16 of 130 with model MultivariateMotif for Validation 1 16 - MultivariateMotif with avg smape 8.22: Model Number: 17 of 130 with model ARDL for Validation 1 17 - ARDL with avg smape 2.7: Model Number: 18 of 130 with model NVAR for Validation 1 📈 18 - NVAR with avg smape 1.93: Model Number: 19 of 130 with model NVAR for Validation 1 19 - NVAR with avg smape 1.93: Model Number: 20 of 130 with model NVAR for Validation 1 20 - NVAR with avg smape 1.93: Model Number: 21 of 130 with model NVAR for Validation 1 21 - NVAR with avg smape 1.93: Model Number: 22 of 130 with model Ensemble for Validation 1 22 - Ensemble with avg smape 2.09: Model Number: 23 of 130 with model NVAR for Validation 1 📈 23 - NVAR with avg smape 1.85: Model Number: 24 of 130 with model NVAR for Validation 1 24 - NVAR with avg smape 2.29: Model Number: 25 of 130 with model Theta for Validation 1 25 - Theta with avg smape 3.06: Model Number: 26 of 130 with model Theta for Validation 1 26 - Theta with avg smape 3.06: Model Number: 27 of 130 with model NVAR for Validation 1 27 - NVAR with avg smape 2.04: Model Number: 28 of 130 with model NVAR for Validation 1 28 - NVAR with avg smape 2.04: Model Number: 29 of 130 with model Theta for Validation 1 📈 29 - Theta with avg smape 1.65: Model Number: 30 of 130 with model ARDL for Validation 1 30 - ARDL with avg smape 3.73: Model Number: 31 of 130 with model Theta for Validation 1 31 - Theta with avg smape 2.69: Model Number: 32 of 130 with model Theta for Validation 1 32 - Theta with avg smape 2.66: Model Number: 33 of 130 with model UnivariateMotif for Validation 1 33 - UnivariateMotif with avg smape 2.8: Model Number: 34 of 130 with model UnivariateMotif for Validation 1 34 - UnivariateMotif with avg smape 2.54: Model Number: 35 of 130 with model Theta for Validation 1 35 - Theta with avg smape 2.63: Model Number: 36 of 130 with model Theta for Validation 1 36 - Theta with avg smape 2.07: Model Number: 37 of 130 with model Theta for Validation 1 37 - Theta with avg smape 2.7: Model Number: 38 of 130 with model ARDL for Validation 1 38 - ARDL with avg smape 4.06: Model Number: 39 of 130 with model ARDL for Validation 1 39 - ARDL with avg smape 3.61: Model Number: 40 of 130 with model ConstantNaive for Validation 1 40 - ConstantNaive with avg smape 2.11: Model Number: 41 of 130 with model ConstantNaive for Validation 1 41 - ConstantNaive with avg smape 2.11: Model Number: 42 of 130 with model ARDL for Validation 1 42 - ARDL with avg smape 4.06: Model Number: 43 of 130 with model ARDL for Validation 1 43 - ARDL with avg smape 3.69: Model Number: 44 of 130 with model ARDL for Validation 1 44 - ARDL with avg smape 3.7: Model Number: 45 of 130 with model AverageValueNaive for Validation 1 45 - AverageValueNaive with avg smape 1.73: Model Number: 46 of 130 with model ARDL for Validation 1 46 - ARDL with avg smape 2.76: Model Number: 47 of 130 with model Ensemble for Validation 1 47 - Ensemble with avg smape 2.29: Model Number: 48 of 130 with model ConstantNaive for Validation 1 48 - ConstantNaive with avg smape 2.7: Model Number: 49 of 130 with model ConstantNaive for Validation 1 49 - ConstantNaive with avg smape 2.7: Model Number: 50 of 130 with model ConstantNaive for Validation 1 50 - ConstantNaive with avg smape 2.7: Model Number: 51 of 130 with model ConstantNaive for Validation 1 51 - ConstantNaive with avg smape 2.7: Model Number: 52 of 130 with model ConstantNaive for Validation 1 52 - ConstantNaive with avg smape 2.7: Model Number: 53 of 130 with model ConstantNaive for Validation 1 53 - ConstantNaive with avg smape 2.7: Model Number: 54 of 130 with model AverageValueNaive for Validation 1 54 - AverageValueNaive with avg smape 3.89: Model Number: 55 of 130 with model UnobservedComponents for Validation 1 55 - UnobservedComponents with avg smape 2.76: Model Number: 56 of 130 with model MultivariateRegression for Validation 1 56 - MultivariateRegression with avg smape 1.7: Model Number: 57 of 130 with model UnobservedComponents for Validation 1 57 - UnobservedComponents with avg smape 3.54: Model Number: 58 of 130 with model GLM for Validation 1 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . 58 - GLM with avg smape 1.79: Model Number: 59 of 130 with model GLM for Validation 1 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families links.py:187: RuntimeWarning: overflow encountered in exp C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families family.py:426: RuntimeWarning: divide by zero encountered in true_divide . 59 - GLM with avg smape 1.79: Model Number: 60 of 130 with model GLM for Validation 1 Template Eval Error: ValueError(&#39;NaN, inf or invalid value detected in weights, estimation infeasible.&#39;) in model 60: GLM Model Number: 61 of 130 with model GLM for Validation 1 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families family.py:132: RuntimeWarning: divide by zero encountered in true_divide . 61 - GLM with avg smape 1.79: Model Number: 62 of 130 with model GLM for Validation 1 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . 62 - GLM with avg smape 1.79: Model Number: 63 of 130 with model GLM for Validation 1 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . 63 - GLM with avg smape 1.79: Model Number: 64 of 130 with model GLM for Validation 1 64 - GLM with avg smape 1.79: Model Number: 65 of 130 with model GLM for Validation 1 65 - GLM with avg smape 1.78: Model Number: 66 of 130 with model MultivariateRegression for Validation 1 . C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:67: RuntimeWarning: divide by zero encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:68: RuntimeWarning: divide by zero encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:68: RuntimeWarning: invalid value encountered in true_divide . 66 - MultivariateRegression with avg smape 8.45: Model Number: 67 of 130 with model AverageValueNaive for Validation 1 67 - AverageValueNaive with avg smape 6.95: Model Number: 68 of 130 with model UnivariateMotif for Validation 1 68 - UnivariateMotif with avg smape 3.42: Model Number: 69 of 130 with model SeasonalNaive for Validation 1 69 - SeasonalNaive with avg smape 2.5: Model Number: 70 of 130 with model WindowRegression for Validation 1 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003725 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf 70 - WindowRegression with avg smape 2.98: Model Number: 71 of 130 with model UnobservedComponents for Validation 1 📈 71 - UnobservedComponents with avg smape 1.57: Model Number: 72 of 130 with model GLS for Validation 1 72 - GLS with avg smape 3.55: Model Number: 73 of 130 with model UnobservedComponents for Validation 1 73 - UnobservedComponents with avg smape 3.71: Model Number: 74 of 130 with model UnivariateMotif for Validation 1 74 - UnivariateMotif with avg smape 4.18: Model Number: 75 of 130 with model ETS for Validation 1 75 - ETS with avg smape 3.49: Model Number: 76 of 130 with model ETS for Validation 1 76 - ETS with avg smape 3.5: Model Number: 77 of 130 with model UnobservedComponents for Validation 1 77 - UnobservedComponents with avg smape 4.27: Model Number: 78 of 130 with model MultivariateRegression for Validation 1 Template Eval Error: ValueError(&#34;Input contains NaN, infinity or a value too large for dtype(&#39;float64&#39;).&#34;) in model 78: MultivariateRegression Model Number: 79 of 130 with model ETS for Validation 1 79 - ETS with avg smape 3.49: Model Number: 80 of 130 with model GLS for Validation 1 80 - GLS with avg smape 3.54: Model Number: 81 of 130 with model SeasonalNaive for Validation 1 📈 81 - SeasonalNaive with avg smape 1.49: Model Number: 82 of 130 with model ETS for Validation 1 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.766e+08, tolerance: 5.935e+05 . 82 - ETS with avg smape 5.09: Model Number: 83 of 130 with model MultivariateRegression for Validation 1 83 - MultivariateRegression with avg smape 3.25: Model Number: 84 of 130 with model ETS for Validation 1 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.058e-01, tolerance: 1.893e-02 . 84 - ETS with avg smape 4.7: Model Number: 85 of 130 with model SeasonalNaive for Validation 1 85 - SeasonalNaive with avg smape 2.32: Model Number: 86 of 130 with model UnobservedComponents for Validation 1 86 - UnobservedComponents with avg smape 3.94: Model Number: 87 of 130 with model SeasonalNaive for Validation 1 87 - SeasonalNaive with avg smape 2.34: Model Number: 88 of 130 with model SeasonalNaive for Validation 1 88 - SeasonalNaive with avg smape 2.1: Model Number: 89 of 130 with model UnivariateMotif for Validation 1 89 - UnivariateMotif with avg smape 5.06: Model Number: 90 of 130 with model UnobservedComponents for Validation 1 90 - UnobservedComponents with avg smape 3.79: Model Number: 91 of 130 with model AverageValueNaive for Validation 1 91 - AverageValueNaive with avg smape 2.01: Model Number: 92 of 130 with model AverageValueNaive for Validation 1 92 - AverageValueNaive with avg smape 7.63: Model Number: 93 of 130 with model WindowRegression for Validation 1 93 - WindowRegression with avg smape 5.92: Model Number: 94 of 130 with model WindowRegression for Validation 1 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=-2)]: Done 194 tasks | elapsed: 0.7s [Parallel(n_jobs=-2)]: Done 444 tasks | elapsed: 1.6s [Parallel(n_jobs=-2)]: Done 794 tasks | elapsed: 3.0s [Parallel(n_jobs=-2)]: Done 1000 out of 1000 | elapsed: 3.7s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.3s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.4s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.4s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.4s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.3s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.4s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.3s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.4s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.3s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.4s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.4s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.5s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished . 94 - WindowRegression with avg smape 4.03: Model Number: 95 of 130 with model WindowRegression for Validation 1 95 - WindowRegression with avg smape 2.2: Model Number: 96 of 130 with model MultivariateRegression for Validation 1 96 - MultivariateRegression with avg smape 1.99: Model Number: 97 of 130 with model GLS for Validation 1 97 - GLS with avg smape 4.49: Model Number: 98 of 130 with model WindowRegression for Validation 1 98 - WindowRegression with avg smape 1.71: Model Number: 99 of 130 with model ETS for Validation 1 99 - ETS with avg smape 4.31: Model Number: 100 of 130 with model SectionalMotif for Validation 1 100 - SectionalMotif with avg smape 3.32: Model Number: 101 of 130 with model UnobservedComponents for Validation 1 101 - UnobservedComponents with avg smape 4.48: Model Number: 102 of 130 with model WindowRegression for Validation 1 102 - WindowRegression with avg smape 2.61: Model Number: 103 of 130 with model SeasonalNaive for Validation 1 103 - SeasonalNaive with avg smape 2.28: Model Number: 104 of 130 with model ETS for Validation 1 104 - ETS with avg smape 4.86: Model Number: 105 of 130 with model ETS for Validation 1 105 - ETS with avg smape 3.48: Model Number: 106 of 130 with model MultivariateMotif for Validation 1 106 - MultivariateMotif with avg smape 16.13: Model Number: 107 of 130 with model SectionalMotif for Validation 1 107 - SectionalMotif with avg smape 3.41: Model Number: 108 of 130 with model MultivariateMotif for Validation 1 108 - MultivariateMotif with avg smape 3.07: Model Number: 109 of 130 with model SeasonalNaive for Validation 1 109 - SeasonalNaive with avg smape 4.81: Model Number: 110 of 130 with model WindowRegression for Validation 1 110 - WindowRegression with avg smape 3.24: Model Number: 111 of 130 with model MultivariateMotif for Validation 1 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.379e+06, tolerance: 5.816e+05 . 111 - MultivariateMotif with avg smape 2.24: Model Number: 112 of 130 with model UnivariateMotif for Validation 1 112 - UnivariateMotif with avg smape 3.75: Model Number: 113 of 130 with model MultivariateRegression for Validation 1 113 - MultivariateRegression with avg smape 3.49: Model Number: 114 of 130 with model MultivariateMotif for Validation 1 114 - MultivariateMotif with avg smape 3.74: Model Number: 115 of 130 with model UnivariateMotif for Validation 1 115 - UnivariateMotif with avg smape 2.21: Model Number: 116 of 130 with model AverageValueNaive for Validation 1 116 - AverageValueNaive with avg smape 1.78: Model Number: 117 of 130 with model GLS for Validation 1 117 - GLS with avg smape 3.93: Model Number: 118 of 130 with model AverageValueNaive for Validation 1 118 - AverageValueNaive with avg smape 5.27: Model Number: 119 of 130 with model MultivariateRegression for Validation 1 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished . 119 - MultivariateRegression with avg smape 3.97: Model Number: 120 of 130 with model WindowRegression for Validation 1 Epoch 1/50 18/18 [==============================] - 10s 84ms/step - loss: 99.2440 - val_loss: 111.3699 Epoch 2/50 18/18 [==============================] - 0s 24ms/step - loss: 99.2834 - val_loss: 111.3751 Epoch 3/50 18/18 [==============================] - 0s 24ms/step - loss: 99.2982 - val_loss: 111.3863 Epoch 4/50 18/18 [==============================] - 0s 24ms/step - loss: 99.2836 - val_loss: 111.3936 Epoch 5/50 18/18 [==============================] - 0s 23ms/step - loss: 99.2469 - val_loss: 111.4016 Epoch 6/50 18/18 [==============================] - 0s 23ms/step - loss: 99.1917 - val_loss: 111.4130 Epoch 7/50 18/18 [==============================] - 0s 23ms/step - loss: 99.2039 - val_loss: 111.4200 Epoch 8/50 18/18 [==============================] - 0s 27ms/step - loss: 99.2091 - val_loss: 111.4346 Epoch 9/50 18/18 [==============================] - 1s 28ms/step - loss: 99.2968 - val_loss: 111.4445 Epoch 10/50 18/18 [==============================] - 0s 26ms/step - loss: 99.2864 - val_loss: 111.4534 Epoch 11/50 18/18 [==============================] - 1s 31ms/step - loss: 99.2964 - val_loss: 111.4684 120 - WindowRegression with avg smape 4.71: Model Number: 121 of 130 with model MultivariateMotif for Validation 1 121 - MultivariateMotif with avg smape 7.41: Model Number: 122 of 130 with model SectionalMotif for Validation 1 122 - SectionalMotif with avg smape 5.63: Model Number: 123 of 130 with model DatepartRegression for Validation 1 123 - DatepartRegression with avg smape 7.22: Model Number: 124 of 130 with model SeasonalNaive for Validation 1 124 - SeasonalNaive with avg smape 3.03: Model Number: 125 of 130 with model MultivariateMotif for Validation 1 125 - MultivariateMotif with avg smape 4.55: Model Number: 126 of 130 with model UnivariateMotif for Validation 1 126 - UnivariateMotif with avg smape 2.78: Model Number: 127 of 130 with model SectionalMotif for Validation 1 127 - SectionalMotif with avg smape 4.09: Model Number: 128 of 130 with model SectionalMotif for Validation 1 128 - SectionalMotif with avg smape 3.4: Model Number: 129 of 130 with model GLS for Validation 1 129 - GLS with avg smape 2.73: Model Number: 130 of 130 with model MultivariateMotif for Validation 1 130 - MultivariateMotif with avg smape 3.41: Validation Round: 2 Model Number: 1 of 130 with model Ensemble for Validation 2 📈 1 - Ensemble with avg smape 13.2: Model Number: 2 of 130 with model Ensemble for Validation 2 2 - Ensemble with avg smape 13.2: Model Number: 3 of 130 with model Ensemble for Validation 2 📈 3 - Ensemble with avg smape 12.29: Model Number: 4 of 130 with model Ensemble for Validation 2 📈 4 - Ensemble with avg smape 2.0: Model Number: 5 of 130 with model Ensemble for Validation 2 5 - Ensemble with avg smape 2.11: Model Number: 6 of 130 with model LastValueNaive for Validation 2 6 - LastValueNaive with avg smape 2.16: Model Number: 7 of 130 with model MultivariateRegression for Validation 2 7 - MultivariateRegression with avg smape 2.33: Model Number: 8 of 130 with model LastValueNaive for Validation 2 8 - LastValueNaive with avg smape 6.16: Model Number: 9 of 130 with model LastValueNaive for Validation 2 📈 9 - LastValueNaive with avg smape 1.86: Model Number: 10 of 130 with model LastValueNaive for Validation 2 10 - LastValueNaive with avg smape 1.86: Model Number: 11 of 130 with model Ensemble for Validation 2 11 - Ensemble with avg smape 1.86: Model Number: 12 of 130 with model LastValueNaive for Validation 2 12 - LastValueNaive with avg smape 1.86: Model Number: 13 of 130 with model LastValueNaive for Validation 2 13 - LastValueNaive with avg smape 1.87: Model Number: 14 of 130 with model LastValueNaive for Validation 2 14 - LastValueNaive with avg smape 1.9: Model Number: 15 of 130 with model LastValueNaive for Validation 2 15 - LastValueNaive with avg smape 2.17: Model Number: 16 of 130 with model MultivariateMotif for Validation 2 16 - MultivariateMotif with avg smape 47.41: Model Number: 17 of 130 with model ARDL for Validation 2 17 - ARDL with avg smape 2.91: Model Number: 18 of 130 with model NVAR for Validation 2 📈 18 - NVAR with avg smape 1.65: Model Number: 19 of 130 with model NVAR for Validation 2 19 - NVAR with avg smape 1.65: Model Number: 20 of 130 with model NVAR for Validation 2 20 - NVAR with avg smape 1.65: Model Number: 21 of 130 with model NVAR for Validation 2 21 - NVAR with avg smape 1.65: Model Number: 22 of 130 with model Ensemble for Validation 2 22 - Ensemble with avg smape 2.03: Model Number: 23 of 130 with model NVAR for Validation 2 23 - NVAR with avg smape 1.65: Model Number: 24 of 130 with model NVAR for Validation 2 24 - NVAR with avg smape 1.67: Model Number: 25 of 130 with model Theta for Validation 2 25 - Theta with avg smape 5.05: Model Number: 26 of 130 with model Theta for Validation 2 26 - Theta with avg smape 5.05: Model Number: 27 of 130 with model NVAR for Validation 2 27 - NVAR with avg smape 1.76: Model Number: 28 of 130 with model NVAR for Validation 2 28 - NVAR with avg smape 1.76: Model Number: 29 of 130 with model Theta for Validation 2 29 - Theta with avg smape 1.86: Model Number: 30 of 130 with model ARDL for Validation 2 30 - ARDL with avg smape 3.54: Model Number: 31 of 130 with model Theta for Validation 2 31 - Theta with avg smape 2.29: Model Number: 32 of 130 with model Theta for Validation 2 32 - Theta with avg smape 2.26: Model Number: 33 of 130 with model UnivariateMotif for Validation 2 33 - UnivariateMotif with avg smape 2.42: Model Number: 34 of 130 with model UnivariateMotif for Validation 2 34 - UnivariateMotif with avg smape 3.1: Model Number: 35 of 130 with model Theta for Validation 2 35 - Theta with avg smape 2.27: Model Number: 36 of 130 with model Theta for Validation 2 36 - Theta with avg smape 2.04: Model Number: 37 of 130 with model Theta for Validation 2 37 - Theta with avg smape 2.31: Model Number: 38 of 130 with model ARDL for Validation 2 38 - ARDL with avg smape 3.49: Model Number: 39 of 130 with model ARDL for Validation 2 39 - ARDL with avg smape 3.25: Model Number: 40 of 130 with model ConstantNaive for Validation 2 40 - ConstantNaive with avg smape 2.18: Model Number: 41 of 130 with model ConstantNaive for Validation 2 41 - ConstantNaive with avg smape 2.18: Model Number: 42 of 130 with model ARDL for Validation 2 42 - ARDL with avg smape 3.35: Model Number: 43 of 130 with model ARDL for Validation 2 43 - ARDL with avg smape 3.25: Model Number: 44 of 130 with model ARDL for Validation 2 44 - ARDL with avg smape 3.24: Model Number: 45 of 130 with model AverageValueNaive for Validation 2 45 - AverageValueNaive with avg smape 2.15: Model Number: 46 of 130 with model ARDL for Validation 2 46 - ARDL with avg smape 2.71: Model Number: 47 of 130 with model Ensemble for Validation 2 47 - Ensemble with avg smape 4.24: Model Number: 48 of 130 with model ConstantNaive for Validation 2 48 - ConstantNaive with avg smape 2.38: Model Number: 49 of 130 with model ConstantNaive for Validation 2 49 - ConstantNaive with avg smape 2.38: Model Number: 50 of 130 with model ConstantNaive for Validation 2 50 - ConstantNaive with avg smape 2.38: Model Number: 51 of 130 with model ConstantNaive for Validation 2 51 - ConstantNaive with avg smape 2.38: Model Number: 52 of 130 with model ConstantNaive for Validation 2 52 - ConstantNaive with avg smape 2.38: Model Number: 53 of 130 with model ConstantNaive for Validation 2 53 - ConstantNaive with avg smape 2.38: Model Number: 54 of 130 with model AverageValueNaive for Validation 2 54 - AverageValueNaive with avg smape 3.32: Model Number: 55 of 130 with model UnobservedComponents for Validation 2 55 - UnobservedComponents with avg smape 2.7: Model Number: 56 of 130 with model MultivariateRegression for Validation 2 56 - MultivariateRegression with avg smape 9.03: Model Number: 57 of 130 with model UnobservedComponents for Validation 2 57 - UnobservedComponents with avg smape 3.85: Model Number: 58 of 130 with model GLM for Validation 2 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . 58 - GLM with avg smape 3.77: Model Number: 59 of 130 with model GLM for Validation 2 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families links.py:187: RuntimeWarning: overflow encountered in exp . 59 - GLM with avg smape 3.77: Model Number: 60 of 130 with model GLM for Validation 2 Template Eval Error: ValueError(&#39;NaN, inf or invalid value detected in weights, estimation infeasible.&#39;) in model 60: GLM Model Number: 61 of 130 with model GLM for Validation 2 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families family.py:426: RuntimeWarning: divide by zero encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families family.py:132: RuntimeWarning: divide by zero encountered in true_divide . 61 - GLM with avg smape 3.77: Model Number: 62 of 130 with model GLM for Validation 2 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . 62 - GLM with avg smape 3.77: Model Number: 63 of 130 with model GLM for Validation 2 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . 63 - GLM with avg smape 3.77: Model Number: 64 of 130 with model GLM for Validation 2 64 - GLM with avg smape 3.77: Model Number: 65 of 130 with model GLM for Validation 2 65 - GLM with avg smape 3.77: Model Number: 66 of 130 with model MultivariateRegression for Validation 2 . C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:67: RuntimeWarning: divide by zero encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:68: RuntimeWarning: divide by zero encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:68: RuntimeWarning: invalid value encountered in true_divide . 66 - MultivariateRegression with avg smape 12.15: Model Number: 67 of 130 with model AverageValueNaive for Validation 2 67 - AverageValueNaive with avg smape 4.34: Model Number: 68 of 130 with model UnivariateMotif for Validation 2 68 - UnivariateMotif with avg smape 3.28: Model Number: 69 of 130 with model SeasonalNaive for Validation 2 69 - SeasonalNaive with avg smape 7.93: Model Number: 70 of 130 with model WindowRegression for Validation 2 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002236 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf 70 - WindowRegression with avg smape 2.85: Model Number: 71 of 130 with model UnobservedComponents for Validation 2 71 - UnobservedComponents with avg smape 2.32: Model Number: 72 of 130 with model GLS for Validation 2 72 - GLS with avg smape 3.22: Model Number: 73 of 130 with model UnobservedComponents for Validation 2 73 - UnobservedComponents with avg smape 3.37: Model Number: 74 of 130 with model UnivariateMotif for Validation 2 74 - UnivariateMotif with avg smape 5.14: Model Number: 75 of 130 with model ETS for Validation 2 75 - ETS with avg smape 3.04: Model Number: 76 of 130 with model ETS for Validation 2 76 - ETS with avg smape 3.04: Model Number: 77 of 130 with model UnobservedComponents for Validation 2 77 - UnobservedComponents with avg smape 3.93: Model Number: 78 of 130 with model MultivariateRegression for Validation 2 78 - MultivariateRegression with avg smape 2.16: Model Number: 79 of 130 with model ETS for Validation 2 79 - ETS with avg smape 3.17: Model Number: 80 of 130 with model GLS for Validation 2 80 - GLS with avg smape 3.06: Model Number: 81 of 130 with model SeasonalNaive for Validation 2 81 - SeasonalNaive with avg smape 1.84: Model Number: 82 of 130 with model ETS for Validation 2 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.312e+08, tolerance: 5.467e+05 . 82 - ETS with avg smape 3.47: Model Number: 83 of 130 with model MultivariateRegression for Validation 2 83 - MultivariateRegression with avg smape 2.21: Model Number: 84 of 130 with model ETS for Validation 2 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.015e+00, tolerance: 1.803e-02 . 84 - ETS with avg smape 3.84: Model Number: 85 of 130 with model SeasonalNaive for Validation 2 85 - SeasonalNaive with avg smape 5.52: Model Number: 86 of 130 with model UnobservedComponents for Validation 2 86 - UnobservedComponents with avg smape 3.19: Model Number: 87 of 130 with model SeasonalNaive for Validation 2 87 - SeasonalNaive with avg smape 5.5: Model Number: 88 of 130 with model SeasonalNaive for Validation 2 88 - SeasonalNaive with avg smape 3.67: Model Number: 89 of 130 with model UnivariateMotif for Validation 2 89 - UnivariateMotif with avg smape 3.52: Model Number: 90 of 130 with model UnobservedComponents for Validation 2 90 - UnobservedComponents with avg smape 3.21: Model Number: 91 of 130 with model AverageValueNaive for Validation 2 91 - AverageValueNaive with avg smape 1.96: Model Number: 92 of 130 with model AverageValueNaive for Validation 2 92 - AverageValueNaive with avg smape 12.27: Model Number: 93 of 130 with model WindowRegression for Validation 2 93 - WindowRegression with avg smape 3.37: Model Number: 94 of 130 with model WindowRegression for Validation 2 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=-2)]: Done 194 tasks | elapsed: 0.6s [Parallel(n_jobs=-2)]: Done 444 tasks | elapsed: 1.8s [Parallel(n_jobs=-2)]: Done 794 tasks | elapsed: 3.8s [Parallel(n_jobs=-2)]: Done 1000 out of 1000 | elapsed: 4.9s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.4s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.6s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.3s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.4s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.3s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.4s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.3s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.4s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.3s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.4s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.3s finished . 94 - WindowRegression with avg smape 3.2: Model Number: 95 of 130 with model WindowRegression for Validation 2 95 - WindowRegression with avg smape 1.77: Model Number: 96 of 130 with model MultivariateRegression for Validation 2 96 - MultivariateRegression with avg smape 6.62: Model Number: 97 of 130 with model GLS for Validation 2 97 - GLS with avg smape 3.74: Model Number: 98 of 130 with model WindowRegression for Validation 2 98 - WindowRegression with avg smape 1.89: Model Number: 99 of 130 with model ETS for Validation 2 99 - ETS with avg smape 2.87: Model Number: 100 of 130 with model SectionalMotif for Validation 2 100 - SectionalMotif with avg smape 5.26: Model Number: 101 of 130 with model UnobservedComponents for Validation 2 101 - UnobservedComponents with avg smape 3.73: Model Number: 102 of 130 with model WindowRegression for Validation 2 102 - WindowRegression with avg smape 10.88: Model Number: 103 of 130 with model SeasonalNaive for Validation 2 103 - SeasonalNaive with avg smape 1.93: Model Number: 104 of 130 with model ETS for Validation 2 104 - ETS with avg smape 4.14: Model Number: 105 of 130 with model ETS for Validation 2 105 - ETS with avg smape 3.36: Model Number: 106 of 130 with model MultivariateMotif for Validation 2 106 - MultivariateMotif with avg smape 17.36: Model Number: 107 of 130 with model SectionalMotif for Validation 2 107 - SectionalMotif with avg smape 8.29: Model Number: 108 of 130 with model MultivariateMotif for Validation 2 108 - MultivariateMotif with avg smape 2.49: Model Number: 109 of 130 with model SeasonalNaive for Validation 2 109 - SeasonalNaive with avg smape 5.45: Model Number: 110 of 130 with model WindowRegression for Validation 2 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.521e+06, tolerance: 5.406e+05 . 110 - WindowRegression with avg smape 2.81: Model Number: 111 of 130 with model MultivariateMotif for Validation 2 111 - MultivariateMotif with avg smape 7.09: Model Number: 112 of 130 with model UnivariateMotif for Validation 2 112 - UnivariateMotif with avg smape 1.8: Model Number: 113 of 130 with model MultivariateRegression for Validation 2 113 - MultivariateRegression with avg smape 2.17: Model Number: 114 of 130 with model MultivariateMotif for Validation 2 114 - MultivariateMotif with avg smape 8.75: Model Number: 115 of 130 with model UnivariateMotif for Validation 2 115 - UnivariateMotif with avg smape 7.46: Model Number: 116 of 130 with model AverageValueNaive for Validation 2 116 - AverageValueNaive with avg smape 4.61: Model Number: 117 of 130 with model GLS for Validation 2 117 - GLS with avg smape 3.48: Model Number: 118 of 130 with model AverageValueNaive for Validation 2 118 - AverageValueNaive with avg smape 3.74: Model Number: 119 of 130 with model MultivariateRegression for Validation 2 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed: 0.4s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.1s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.3s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished . 119 - MultivariateRegression with avg smape 2.28: Model Number: 120 of 130 with model WindowRegression for Validation 2 Epoch 1/50 18/18 [==============================] - 24s 310ms/step - loss: 99.5590 - val_loss: 103.4906 Epoch 2/50 18/18 [==============================] - 1s 44ms/step - loss: 98.5988 - val_loss: 103.4897 Epoch 3/50 18/18 [==============================] - 1s 42ms/step - loss: 98.0109 - val_loss: 103.4895 Epoch 4/50 18/18 [==============================] - 1s 46ms/step - loss: 97.7775 - val_loss: 103.4966 Epoch 5/50 18/18 [==============================] - 1s 44ms/step - loss: 98.3618 - val_loss: 103.5075 Epoch 6/50 18/18 [==============================] - 1s 58ms/step - loss: 97.5772 - val_loss: 103.5201 Epoch 7/50 18/18 [==============================] - 1s 48ms/step - loss: 99.9695 - val_loss: 103.5243 Epoch 8/50 18/18 [==============================] - 1s 43ms/step - loss: 99.5447 - val_loss: 103.5418 Epoch 9/50 18/18 [==============================] - 1s 61ms/step - loss: 99.2083 - val_loss: 103.5426 Epoch 10/50 18/18 [==============================] - 1s 52ms/step - loss: 100.1419 - val_loss: 103.5384 Epoch 11/50 18/18 [==============================] - 1s 43ms/step - loss: 98.6760 - val_loss: 103.5516 Epoch 12/50 18/18 [==============================] - 1s 39ms/step - loss: 99.9153 - val_loss: 103.5497 Epoch 13/50 18/18 [==============================] - 1s 43ms/step - loss: 98.8464 - val_loss: 103.5610 120 - WindowRegression with avg smape 3.88: Model Number: 121 of 130 with model MultivariateMotif for Validation 2 121 - MultivariateMotif with avg smape 3.07: Model Number: 122 of 130 with model SectionalMotif for Validation 2 122 - SectionalMotif with avg smape 4.27: Model Number: 123 of 130 with model DatepartRegression for Validation 2 123 - DatepartRegression with avg smape 3.08: Model Number: 124 of 130 with model SeasonalNaive for Validation 2 124 - SeasonalNaive with avg smape 7.14: Model Number: 125 of 130 with model MultivariateMotif for Validation 2 125 - MultivariateMotif with avg smape 4.06: Model Number: 126 of 130 with model UnivariateMotif for Validation 2 126 - UnivariateMotif with avg smape 1.84: Model Number: 127 of 130 with model SectionalMotif for Validation 2 127 - SectionalMotif with avg smape 3.28: Model Number: 128 of 130 with model SectionalMotif for Validation 2 128 - SectionalMotif with avg smape 7.18: Model Number: 129 of 130 with model GLS for Validation 2 129 - GLS with avg smape 6.82: Model Number: 130 of 130 with model MultivariateMotif for Validation 2 130 - MultivariateMotif with avg smape 8.57: Validation Round: 3 Model Number: 1 of 130 with model Ensemble for Validation 3 📈 1 - Ensemble with avg smape 1.42: Model Number: 2 of 130 with model Ensemble for Validation 3 2 - Ensemble with avg smape 1.42: Model Number: 3 of 130 with model Ensemble for Validation 3 3 - Ensemble with avg smape 1.42: Model Number: 4 of 130 with model Ensemble for Validation 3 📈 4 - Ensemble with avg smape 1.29: Model Number: 5 of 130 with model Ensemble for Validation 3 5 - Ensemble with avg smape 1.31: Model Number: 6 of 130 with model LastValueNaive for Validation 3 6 - LastValueNaive with avg smape 1.42: Model Number: 7 of 130 with model MultivariateRegression for Validation 3 7 - MultivariateRegression with avg smape 1.89: Model Number: 8 of 130 with model LastValueNaive for Validation 3 8 - LastValueNaive with avg smape 10.44: Model Number: 9 of 130 with model LastValueNaive for Validation 3 📈 9 - LastValueNaive with avg smape 1.1: Model Number: 10 of 130 with model LastValueNaive for Validation 3 10 - LastValueNaive with avg smape 1.1: Model Number: 11 of 130 with model Ensemble for Validation 3 11 - Ensemble with avg smape 1.1: Model Number: 12 of 130 with model LastValueNaive for Validation 3 12 - LastValueNaive with avg smape 1.1: Model Number: 13 of 130 with model LastValueNaive for Validation 3 13 - LastValueNaive with avg smape 1.1: Model Number: 14 of 130 with model LastValueNaive for Validation 3 📈 14 - LastValueNaive with avg smape 1.09: Model Number: 15 of 130 with model LastValueNaive for Validation 3 15 - LastValueNaive with avg smape 1.49: Model Number: 16 of 130 with model MultivariateMotif for Validation 3 16 - MultivariateMotif with avg smape 1.65: Model Number: 17 of 130 with model ARDL for Validation 3 17 - ARDL with avg smape 1.61: Model Number: 18 of 130 with model NVAR for Validation 3 18 - NVAR with avg smape 1.74: Model Number: 19 of 130 with model NVAR for Validation 3 19 - NVAR with avg smape 1.74: Model Number: 20 of 130 with model NVAR for Validation 3 20 - NVAR with avg smape 1.74: Model Number: 21 of 130 with model NVAR for Validation 3 21 - NVAR with avg smape 1.74: Model Number: 22 of 130 with model Ensemble for Validation 3 22 - Ensemble with avg smape 1.09: Model Number: 23 of 130 with model NVAR for Validation 3 23 - NVAR with avg smape 1.72: Model Number: 24 of 130 with model NVAR for Validation 3 24 - NVAR with avg smape 1.48: Model Number: 25 of 130 with model Theta for Validation 3 25 - Theta with avg smape 1.43: Model Number: 26 of 130 with model Theta for Validation 3 26 - Theta with avg smape 1.43: Model Number: 27 of 130 with model NVAR for Validation 3 27 - NVAR with avg smape 1.57: Model Number: 28 of 130 with model NVAR for Validation 3 28 - NVAR with avg smape 1.57: Model Number: 29 of 130 with model Theta for Validation 3 29 - Theta with avg smape 1.59: Model Number: 30 of 130 with model ARDL for Validation 3 30 - ARDL with avg smape 1.84: Model Number: 31 of 130 with model Theta for Validation 3 📈 31 - Theta with avg smape 0.99: Model Number: 32 of 130 with model Theta for Validation 3 32 - Theta with avg smape 0.99: Model Number: 33 of 130 with model UnivariateMotif for Validation 3 33 - UnivariateMotif with avg smape 1.46: Model Number: 34 of 130 with model UnivariateMotif for Validation 3 34 - UnivariateMotif with avg smape 1.14: Model Number: 35 of 130 with model Theta for Validation 3 35 - Theta with avg smape 1.0: Model Number: 36 of 130 with model Theta for Validation 3 36 - Theta with avg smape 2.08: Model Number: 37 of 130 with model Theta for Validation 3 37 - Theta with avg smape 0.99: Model Number: 38 of 130 with model ARDL for Validation 3 38 - ARDL with avg smape 1.38: Model Number: 39 of 130 with model ARDL for Validation 3 39 - ARDL with avg smape 1.54: Model Number: 40 of 130 with model ConstantNaive for Validation 3 40 - ConstantNaive with avg smape 2.12: Model Number: 41 of 130 with model ConstantNaive for Validation 3 41 - ConstantNaive with avg smape 2.12: Model Number: 42 of 130 with model ARDL for Validation 3 42 - ARDL with avg smape 1.41: Model Number: 43 of 130 with model ARDL for Validation 3 43 - ARDL with avg smape 1.51: Model Number: 44 of 130 with model ARDL for Validation 3 44 - ARDL with avg smape 1.52: Model Number: 45 of 130 with model AverageValueNaive for Validation 3 45 - AverageValueNaive with avg smape 2.62: Model Number: 46 of 130 with model ARDL for Validation 3 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3253: RuntimeWarning: divide by zero encountered in log C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3196: RuntimeWarning: divide by zero encountered in power C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3196: RuntimeWarning: divide by zero encountered in power C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3196: RuntimeWarning: divide by zero encountered in power . 📈 46 - ARDL with avg smape 0.0: Model Number: 47 of 130 with model Ensemble for Validation 3 47 - Ensemble with avg smape 2.7: Model Number: 48 of 130 with model ConstantNaive for Validation 3 48 - ConstantNaive with avg smape 1.83: Model Number: 49 of 130 with model ConstantNaive for Validation 3 49 - ConstantNaive with avg smape 1.83: Model Number: 50 of 130 with model ConstantNaive for Validation 3 50 - ConstantNaive with avg smape 1.83: Model Number: 51 of 130 with model ConstantNaive for Validation 3 51 - ConstantNaive with avg smape 1.83: Model Number: 52 of 130 with model ConstantNaive for Validation 3 52 - ConstantNaive with avg smape 0.0: Model Number: 53 of 130 with model ConstantNaive for Validation 3 53 - ConstantNaive with avg smape 0.0: Model Number: 54 of 130 with model AverageValueNaive for Validation 3 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3253: RuntimeWarning: divide by zero encountered in log C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3196: RuntimeWarning: divide by zero encountered in power C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3196: RuntimeWarning: divide by zero encountered in power C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3196: RuntimeWarning: divide by zero encountered in power C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3253: RuntimeWarning: divide by zero encountered in log C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3196: RuntimeWarning: divide by zero encountered in power C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3196: RuntimeWarning: divide by zero encountered in power C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3196: RuntimeWarning: divide by zero encountered in power . 54 - AverageValueNaive with avg smape 1.42: Model Number: 55 of 130 with model UnobservedComponents for Validation 3 55 - UnobservedComponents with avg smape 2.03: Model Number: 56 of 130 with model MultivariateRegression for Validation 3 56 - MultivariateRegression with avg smape 7.8: Model Number: 57 of 130 with model UnobservedComponents for Validation 3 57 - UnobservedComponents with avg smape 1.86: Model Number: 58 of 130 with model GLM for Validation 3 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . 58 - GLM with avg smape 3.84: Model Number: 59 of 130 with model GLM for Validation 3 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod families links.py:187: RuntimeWarning: overflow encountered in exp . 59 - GLM with avg smape 3.84: Model Number: 60 of 130 with model GLM for Validation 3 60 - GLM with avg smape 3.84: Model Number: 61 of 130 with model GLM for Validation 3 61 - GLM with avg smape 3.84: Model Number: 62 of 130 with model GLM for Validation 3 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . 62 - GLM with avg smape 3.84: Model Number: 63 of 130 with model GLM for Validation 3 . C: Users impep anaconda3 envs OpenCV lib site-packages statsmodels genmod generalized_linear_model.py:301: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family. . 63 - GLM with avg smape 3.84: Model Number: 64 of 130 with model GLM for Validation 3 64 - GLM with avg smape 3.84: Model Number: 65 of 130 with model GLM for Validation 3 65 - GLM with avg smape 3.84: Model Number: 66 of 130 with model MultivariateRegression for Validation 3 . C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:67: RuntimeWarning: divide by zero encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:68: RuntimeWarning: divide by zero encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages autots tools probabilistic.py:68: RuntimeWarning: invalid value encountered in true_divide C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3253: RuntimeWarning: divide by zero encountered in log . 66 - MultivariateRegression with avg smape 18.34: Model Number: 67 of 130 with model AverageValueNaive for Validation 3 67 - AverageValueNaive with avg smape 0.0: Model Number: 68 of 130 with model UnivariateMotif for Validation 3 68 - UnivariateMotif with avg smape 1.33: . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3196: RuntimeWarning: divide by zero encountered in power C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3196: RuntimeWarning: divide by zero encountered in power C: Users impep anaconda3 envs OpenCV lib site-packages sklearn preprocessing _data.py:3196: RuntimeWarning: divide by zero encountered in power . Model Number: 69 of 130 with model SeasonalNaive for Validation 3 69 - SeasonalNaive with avg smape 2.09: Model Number: 70 of 130 with model WindowRegression for Validation 3 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002369 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf 70 - WindowRegression with avg smape 3.88: Model Number: 71 of 130 with model UnobservedComponents for Validation 3 71 - UnobservedComponents with avg smape 2.11: Model Number: 72 of 130 with model GLS for Validation 3 72 - GLS with avg smape 1.31: Model Number: 73 of 130 with model UnobservedComponents for Validation 3 73 - UnobservedComponents with avg smape 1.64: Model Number: 74 of 130 with model UnivariateMotif for Validation 3 74 - UnivariateMotif with avg smape 1.27: Model Number: 75 of 130 with model ETS for Validation 3 75 - ETS with avg smape 1.72: Model Number: 76 of 130 with model ETS for Validation 3 76 - ETS with avg smape 1.72: Model Number: 77 of 130 with model UnobservedComponents for Validation 3 77 - UnobservedComponents with avg smape 1.78: Model Number: 78 of 130 with model MultivariateRegression for Validation 3 Template Eval Error: ValueError(&#34;Input contains NaN, infinity or a value too large for dtype(&#39;float64&#39;).&#34;) in model 78: MultivariateRegression Model Number: 79 of 130 with model ETS for Validation 3 79 - ETS with avg smape 1.72: Model Number: 80 of 130 with model GLS for Validation 3 80 - GLS with avg smape 1.32: Model Number: 81 of 130 with model SeasonalNaive for Validation 3 81 - SeasonalNaive with avg smape 1.69: Model Number: 82 of 130 with model ETS for Validation 3 82 - ETS with avg smape 2.75: Model Number: 83 of 130 with model MultivariateRegression for Validation 3 83 - MultivariateRegression with avg smape 1.13: Model Number: 84 of 130 with model ETS for Validation 3 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.184e+00, tolerance: 1.720e-02 . 84 - ETS with avg smape 2.07: Model Number: 85 of 130 with model SeasonalNaive for Validation 3 85 - SeasonalNaive with avg smape 10.06: Model Number: 86 of 130 with model UnobservedComponents for Validation 3 86 - UnobservedComponents with avg smape 1.45: Model Number: 87 of 130 with model SeasonalNaive for Validation 3 87 - SeasonalNaive with avg smape 10.03: Model Number: 88 of 130 with model SeasonalNaive for Validation 3 88 - SeasonalNaive with avg smape 1.96: Model Number: 89 of 130 with model UnivariateMotif for Validation 3 89 - UnivariateMotif with avg smape 1.97: Model Number: 90 of 130 with model UnobservedComponents for Validation 3 90 - UnobservedComponents with avg smape 1.42: Model Number: 91 of 130 with model AverageValueNaive for Validation 3 91 - AverageValueNaive with avg smape 4.35: Model Number: 92 of 130 with model AverageValueNaive for Validation 3 92 - AverageValueNaive with avg smape 16.88: Model Number: 93 of 130 with model WindowRegression for Validation 3 93 - WindowRegression with avg smape 1.02: Model Number: 94 of 130 with model WindowRegression for Validation 3 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=-2)]: Done 194 tasks | elapsed: 0.6s [Parallel(n_jobs=-2)]: Done 444 tasks | elapsed: 1.5s [Parallel(n_jobs=-2)]: Done 794 tasks | elapsed: 2.8s [Parallel(n_jobs=-2)]: Done 1000 out of 1000 | elapsed: 3.5s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.2s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 194 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 444 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 794 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 0.2s finished . 94 - WindowRegression with avg smape 1.52: Model Number: 95 of 130 with model WindowRegression for Validation 3 95 - WindowRegression with avg smape 2.21: Model Number: 96 of 130 with model MultivariateRegression for Validation 3 96 - MultivariateRegression with avg smape 10.31: Model Number: 97 of 130 with model GLS for Validation 3 97 - GLS with avg smape 1.47: Model Number: 98 of 130 with model WindowRegression for Validation 3 98 - WindowRegression with avg smape 1.98: Model Number: 99 of 130 with model ETS for Validation 3 99 - ETS with avg smape 1.68: Model Number: 100 of 130 with model SectionalMotif for Validation 3 100 - SectionalMotif with avg smape 2.71: Model Number: 101 of 130 with model UnobservedComponents for Validation 3 101 - UnobservedComponents with avg smape 1.46: Model Number: 102 of 130 with model WindowRegression for Validation 3 102 - WindowRegression with avg smape 1.81: Model Number: 103 of 130 with model SeasonalNaive for Validation 3 103 - SeasonalNaive with avg smape 2.08: Model Number: 104 of 130 with model ETS for Validation 3 104 - ETS with avg smape 1.74: Model Number: 105 of 130 with model ETS for Validation 3 105 - ETS with avg smape 1.86: Model Number: 106 of 130 with model MultivariateMotif for Validation 3 106 - MultivariateMotif with avg smape 18.06: Model Number: 107 of 130 with model SectionalMotif for Validation 3 107 - SectionalMotif with avg smape 17.13: Model Number: 108 of 130 with model MultivariateMotif for Validation 3 108 - MultivariateMotif with avg smape 1.21: Model Number: 109 of 130 with model SeasonalNaive for Validation 3 109 - SeasonalNaive with avg smape 3.06: Model Number: 110 of 130 with model WindowRegression for Validation 3 . C: Users impep anaconda3 envs OpenCV lib site-packages sklearn linear_model _coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.126e+06, tolerance: 4.854e+05 . 110 - WindowRegression with avg smape 1.3: Model Number: 111 of 130 with model MultivariateMotif for Validation 3 111 - MultivariateMotif with avg smape 12.3: Model Number: 112 of 130 with model UnivariateMotif for Validation 3 112 - UnivariateMotif with avg smape 1.51: Model Number: 113 of 130 with model MultivariateRegression for Validation 3 113 - MultivariateRegression with avg smape 1.11: Model Number: 114 of 130 with model MultivariateMotif for Validation 3 114 - MultivariateMotif with avg smape 13.83: Model Number: 115 of 130 with model UnivariateMotif for Validation 3 115 - UnivariateMotif with avg smape 12.28: Model Number: 116 of 130 with model AverageValueNaive for Validation 3 116 - AverageValueNaive with avg smape 8.7: Model Number: 117 of 130 with model GLS for Validation 3 117 - GLS with avg smape 1.4: Model Number: 118 of 130 with model AverageValueNaive for Validation 3 118 - AverageValueNaive with avg smape 1.57: Model Number: 119 of 130 with model MultivariateRegression for Validation 3 . [Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=-2)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed: 0.2s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.0s finished . 119 - MultivariateRegression with avg smape 1.76: Model Number: 120 of 130 with model WindowRegression for Validation 3 Epoch 1/50 17/17 [==============================] - 11s 116ms/step - loss: 105.7642 - val_loss: 108.1195 Epoch 2/50 17/17 [==============================] - 0s 25ms/step - loss: 101.8077 - val_loss: 108.1208 Epoch 3/50 17/17 [==============================] - 1s 32ms/step - loss: 111.3971 - val_loss: 108.1320 Epoch 4/50 17/17 [==============================] - 0s 26ms/step - loss: 102.1923 - val_loss: 108.1304 Epoch 5/50 17/17 [==============================] - 0s 27ms/step - loss: 105.6344 - val_loss: 108.1349 Epoch 6/50 17/17 [==============================] - 1s 41ms/step - loss: 103.9355 - val_loss: 108.1401 Epoch 7/50 17/17 [==============================] - 1s 51ms/step - loss: 98.0228 - val_loss: 108.1465 Epoch 8/50 17/17 [==============================] - 1s 63ms/step - loss: 115.9061 - val_loss: 108.1488 Epoch 9/50 17/17 [==============================] - 1s 44ms/step - loss: 101.9901 - val_loss: 108.1591 Epoch 10/50 17/17 [==============================] - 0s 27ms/step - loss: 114.2572 - val_loss: 108.1666 Epoch 11/50 17/17 [==============================] - 0s 27ms/step - loss: 99.9602 - val_loss: 108.1745 120 - WindowRegression with avg smape 1.53: Model Number: 121 of 130 with model MultivariateMotif for Validation 3 121 - MultivariateMotif with avg smape 2.17: Model Number: 122 of 130 with model SectionalMotif for Validation 3 122 - SectionalMotif with avg smape 2.09: Model Number: 123 of 130 with model DatepartRegression for Validation 3 123 - DatepartRegression with avg smape 4.41: Model Number: 124 of 130 with model SeasonalNaive for Validation 3 124 - SeasonalNaive with avg smape 12.08: Model Number: 125 of 130 with model MultivariateMotif for Validation 3 125 - MultivariateMotif with avg smape 1.79: Model Number: 126 of 130 with model UnivariateMotif for Validation 3 126 - UnivariateMotif with avg smape 1.23: Model Number: 127 of 130 with model SectionalMotif for Validation 3 127 - SectionalMotif with avg smape 1.24: Model Number: 128 of 130 with model SectionalMotif for Validation 3 128 - SectionalMotif with avg smape 5.13: Model Number: 129 of 130 with model GLS for Validation 3 129 - GLS with avg smape 11.68: Model Number: 130 of 130 with model MultivariateMotif for Validation 3 130 - MultivariateMotif with avg smape 13.47: . 在經過漫長（但自動化）的複雜運算之後，AutoTS會選擇最佳的模型預測結果 . print(forecast) . Close 2022-06-16 16021.916427 2022-06-17 16009.360169 2022-06-20 15971.442562 2022-06-21 15958.721237 2022-06-22 15945.959265 2022-06-23 15933.156894 2022-06-24 15920.314374 2022-06-27 15881.548438 2022-06-28 15868.547841 2022-06-29 15855.508357 2022-06-30 15842.430242 2022-07-01 15829.313751 2022-07-04 15789.736584 2022-07-05 15776.469157 2022-07-06 15763.164642 2022-07-07 15749.823298 2022-07-08 15736.445387 2022-07-11 15696.094866 2022-07-12 15682.573305 2022-07-13 15669.016490 2022-07-14 15655.424686 2022-07-15 15641.798158 2022-07-18 15600.712897 2022-07-19 15586.950142 2022-07-20 15573.154002 2022-07-21 15559.324744 2022-07-22 15545.462640 2022-07-25 15503.681958 2022-07-26 15489.691181 2022-07-27 15475.668918 . 從預測數據來看，在未來30天，台灣加權股價指數為向下趨勢。 . &#32317;&#32080; . 投資買賣一定會帶來風險。 使用機器學習進行加價格預測，是根據歷史價格進行推算所得到的結果，希望您在投資之前先做好所有的風險評估。 .",
            "url": "https://impepper.github.io/myPortfolio/python/prediction/jupyter/autots/stock%20price/2022/06/16/TW-weight-Index-Prediction-with-AutoTS.html",
            "relUrl": "/python/prediction/jupyter/autots/stock%20price/2022/06/16/TW-weight-Index-Prediction-with-AutoTS.html",
            "date": " • Jun 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Stock Proce Prediction 股價預測 II",
            "content": "2017年，Facebook 宣佈開源一款基於 Python 和 R 語言的數據預測工具——「Prophet」，即「先知」。取名倒是非常直白。 . Facebook 表示，Prophet 相比現有預測工具更加人性化，並且難得地提供 Python 支援。另外，它生成的預測結果足以和專業數據分析師媲美 。 . 今天，我們嘗試利用 Prophet 這個數據預測模型，來預測股價的未來走向 . Facebook Prophet Model . 從 Facebook 官方聲明來看， 推出 Prophet，是希望用它來替代 forecast，成為最受歡迎的預測工具 。相對於後者，Facebook 表示 Prophet 有兩大優點： . &#19968;&#12289;&#38283;&#30332;&#21512;&#29702;&#12289;&#28310;&#30906;&#30340;&#38928;&#28204;&#27169;&#22411;&#26356;&#21152;&#30452;&#25509;&#12290; . 之前許多預測工具 ARIMA 和 exponential smoothing 等等，每一項工具都有它的長處、短處和調節參數。但 Facebook 表示，選擇不恰當的模型或參數會造成讓人難以滿意的結果；而即便是有經驗的數據分析師在選擇模型、參數上也經常遇到困難。言下之意是，Prophet 將著重改善這方面的體驗，讓模型、參數的選擇更直觀。 . &#20108;&#12289;&#29992; Prophet &#20570;&#20986;&#30340;&#38928;&#28204;&#65292;&#33021;&#22816;&#20197;&#23565;&#26222;&#36890;&#20154;&#26356;&#21152;&#30452;&#35264;&#30340;&#26041;&#24335;&#36914;&#34892;&#23450;&#21046;&#12290; . Prophet 有針對週期性的平滑參數（smoothing parameters for seasonality），允許開發者調整與歷史週期的匹配程度。它還有針對趨勢的平滑參數，能夠調整對歷史趨勢變化的緊跟程度。對於成長曲線（growth curves），開發者能人工設置上限，即 capacities，把關於「該預測如何成長（或下降）」的先驗訊息注入進去。最後，開發者還能設置不規則日期，來對超級杯、感恩節、黑色星期五之類的特殊日子進行建模。 . Facebook 把預測任務的流程用下圖展示出來： . . 從圖中可以看出，預測共分四個流程：建模，預測評估，表面問題和人工檢查。 . &#32068;&#25104;&#37096;&#20998; . Prophet 的核心是一個可加性迴歸模型（additive regression model），它有四個組成部分： . 一個分段的線性或邏輯成長曲線趨勢。Prophet 通過提取數據中的轉變點，自動檢測趨勢變化。 | 一個按年的週期組件，使用傅立葉級數（Fourier series）建模而成。 | 一個按周的週期組件，使用虛擬變量（dummy variables）。 | 使用者設置的重要節日表。 | . &#36969;&#29992;&#31684;&#22285; . Prophet 針對的是商業預測任務，適用於具備以下特徵的場景： . 針對每小時、每天或每星期的觀察，有至少數月（理想情況的一年）的歷史記錄。 | 多重顯著的「人類層級」週期性：星期 X 以及年份。 | 日期間隔不規則的重要節日（比如超級杯），需要事先得知。 | 觀察缺失或是異常值在合理範圍內。 | 歷史趨勢變化，比如產品發佈或者改寫記錄（logging changes）。 | 符合非線性成長曲線的趨勢，有天然上、下限或者飽和點。 | . &#38283;&#22987;&#36914;&#34892;&#20729;&#26684;&#38928;&#28204; . &#36039;&#26009;&#20358;&#28304; . 在這次的練習中，我透過 yahoo finance 來取得特斯拉（代號： TSLA）的歷史股價並下載為CSV格式檔案： . . 載入相關的函式庫以歷史股價資料 . import numpy as np import pandas as pd import matplotlib.pyplot as plt from fbprophet import Prophet data = pd.read_csv(&quot;https://github.com/impepper/myPortfolio/raw/master/_notebooks/datasource/TSLA.csv&quot;) data.head() . Date Open High Low Close Adj Close Volume . 0 2021-06-14 | 612.229980 | 625.489990 | 609.179993 | 617.690002 | 617.690002 | 20424000 | . 1 2021-06-15 | 616.690002 | 616.789978 | 598.229980 | 599.359985 | 599.359985 | 17764100 | . 2 2021-06-16 | 597.539978 | 608.500000 | 593.500000 | 604.869995 | 604.869995 | 22144100 | . 3 2021-06-17 | 601.890015 | 621.469971 | 601.340027 | 616.599976 | 616.599976 | 22701400 | . 4 2021-06-18 | 613.369995 | 628.349976 | 611.799988 | 623.309998 | 623.309998 | 24560900 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 再進行後續步驟之前，我們先看一下特斯拉的歷史收盤價圖表： close = data[&#39;Close&#39;] ax = close.plot(title=&#39;Tesla&#39;) ax.set_xlabel(&#39;Date&#39;) ax.set_ylabel(&#39;Close&#39;) plt.show() . 由於Prophet數據預測僅需要時間序列（日期）以及價格資訊，我們僅留下日期以及收盤價兩個欄位資料，並且依照Prophet的欄位格式，將其更名為ds`（日期）及y（收盤價）： . data[&quot;Date&quot;] = pd.to_datetime(data[&quot;Date&quot;], infer_datetime_format=True) data = data[[&quot;Date&quot;, &quot;Close&quot;]] data = data.rename(columns={&quot;Date&quot; : &quot;ds&quot;, &quot;Close&quot; : &quot;y&quot; }) . 有了這兩個數據，我們就可以透過 Prophet 進行預測： . model = Prophet(daily_seasonality = True) model.fit(data) predict = model.make_future_dataframe(periods=365) # 我們透過 Prophet 來預測未來 365 天的股價 forcast = model.predict(predict) forcast[[&quot;ds&quot;, &quot;yhat&quot;, &quot;yhat_lower&quot;, &quot;yhat_upper&quot;]].tail() . INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. . ds yhat yhat_lower yhat_upper . 612 2023-06-06 | 123.105231 | -346.557206 | 577.605426 | . 613 2023-06-07 | 123.295644 | -321.961823 | 579.333774 | . 614 2023-06-08 | 114.798291 | -332.610815 | 584.606105 | . 615 2023-06-09 | 107.121738 | -367.662476 | 576.998907 | . 616 2023-06-10 | 119.452811 | -348.412252 | 564.446602 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 在預測的數據中，我們注意到預測價格包含了上限（預測的當日股價可能最大值）及下限（預測的當日股價可能最小值） 我們利用 matplot 函式庫，透過圖像的方式將其呈現出來： . graph = model.plot(forcast, xlabel=&quot;Date&quot;, ylabel=&quot;Price&quot;) plt.title(&quot;Stock Price Predition for Tesla&quot;) plt.xlabel(&quot;Date&quot;) plt.ylabel(&quot;Close Stock Price&quot;) plt.show() . model.plot_components(forcast) plt.show() . &#32080;&#35542; . 從 Prophet 的預測結果來看，特斯拉的股價在未來，應是呈現一個下跌趨勢。至於特斯拉的未來股價，就有待日後證明。 . 我希望你喜歡這篇文章。 關於 Prophet 的更多細節，你可以參考以下連結： . 使用指南：https://facebookincubator.github.io/prophet/docs/quick_start.html . 檔案：http://facebookincubator.github.io/prophet/ .",
            "url": "https://impepper.github.io/myPortfolio/python/prediction/jupyter/facebook%20prophet/stock%20price/2022/06/15/Stock-Price-Prediction-with-Facebook-Prophet-model.html",
            "relUrl": "/python/prediction/jupyter/facebook%20prophet/stock%20price/2022/06/15/Stock-Price-Prediction-with-Facebook-Prophet-model.html",
            "date": " • Jun 15, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "從銷售到體驗，零售與新零售",
            "content": "&#29978;&#40636;&#26159;&#38646;&#21806;&#65311; . 零售 (retail)，就是面對個別消費者所做的商品銷售，與提供消費者服務的商業行為。 . 自工業時代以來，產品的生產成為了大量製造模式，為了讓生產的產覑能夠更加快速的銷售給終端消費者，生產者將其重心放在「製造更好的產品」，銷售的任務，逐漸演變成由超市、便利商店、百貨公司、大賣場、購物中心這樣的銷售店家來面對消費者，而代理商、經銷商、批發商這些通路商角色的出現，也成為了串連起製造以及銷售兩端的重要一份子。 . 而生產者（製造商），為了增加自己的產品在市場中的辨識度，遂行透過品牌的建立，讓消費者能夠在諸多同類型商品中，快速的選擇他們所生產的產品。 . 從這樣的觀點來看，我們現在所熟悉的品牌（像是可口可樂），在零售產業中，更像是製造商，而不是實際上的銷售者。 . &#38646;&#21806;&#26989;&#30340;&#29256;&#22294; . 在這個由製造商、店家以及通路商（代理商、經銷商、批發商、連鎖銷售體系等）所堆砌出來的零售經濟王國裡，最大的板塊，已經從最早的這些生產者、製造商，轉變成了通路商。 . 網路世代的來臨，為零售業注入了一股活水 . 隨著工業技術的進步，以及所帶來的規模化與全球化市場，讓商店貨架上同質性的產品變多了：更多的品牌出現在市場上，競爭變激烈了，消費者也有了更多的選擇。 . 生產者們，這些品牌與製造商，也開始思考著如何拿回與消費者對話的主導權。 . 我認為，其中有兩個重要的因素： . &#19968;&#12289; &#36039;&#35338;&#29190;&#28856;&#32302;&#30701;&#20102;&#29986;&#21697;&#20729;&#26684;&#30340;&#21033;&#28516;&#31354;&#38291; . 身處在網路數位時代之中，讓資訊更加容易取得，也使得消費者的資訊落差變得越來越少，在零售市場上所反映出來的，就是價格資訊越來越透明。 而產品的利潤結構，包含了從製造商以下的層層通路，價格越透明，也反映出通路商..... . &#20108;&#12289; &#28858;&#20102;&#21109;&#36896;&#26356;&#22823;&#30340;&#29986;&#21697;&#24046;&#30064;&#24615;&#65292;&#29983;&#29986;&#32773;&#38656;&#35201;&#26356;&#21152;&#20102;&#35299;&#28040;&#36027;&#32773;&#30340;&#24847;&#21521; . 因為層層分工的體系，商品（品牌）的生產者，無法直接與消費者對話，反而離消費者越來越遠，越來越不認識自己的消費者。 而零售版塊中最重要通路商，因為每一天每一秒都在與消費者互動，甚至大規模的經營會員，深入掌握了消費者的行為，知道什麼東西會賣，知道消費者喜歡什麼，因此在商業合作上，掌握了很大的談判籌碼。 . 所以，現在的「生產者」們（或是品牌），都開始尋求直接面對消費者，經營自已的「會員」，藉此了解自己的會員。 . 我們所看的一個重要現象就是： . 近年來，像是Nike、Apple這些品牌，都開始成立自營官網、自營門市，透過這些官網、門市，經營起自有的消費族群（會員）。 各地的連鎖體系通路（如7-11、屈臣氏），也開始在網路建立自己的新通路（購物官網），並且嘗試創造話題、吸引人流前進這個新形態的通路。 . 數位時代的迅速發展，讓品牌有了更多豐富的工具來了解客戶。比起單純的電視廣告，在自媒體（或社群）可以更直接的面對目標族群，與消費者對話、溝通；比起路邊隨意發送的產品試用包，直接寄送給自家會員適用，更可以得到忠誠消費者的反饋。 . 當消費者對品牌建立起忠誠與信任，銷售，便不再是單純的銀貨兩訖，而是一個溫暖的、美好的生活體驗。 . 於是，我們迎來了「新零售」 . &#26032;&#38646;&#21806;&#26159;&#20160;&#40636; . 2016年阿里巴巴創辦人馬雲在一次大會上提及「在未來10年、20年將不再有電子商務，只有新零售。」這使得「新零售」一詞迅速竄紅。 . 新零售和傳統零售銷售最大的不同在於，新零售是以創造、滿足消費者的需求和體驗感受為中心，並整合包含數據分析、金流、物流以及全通路資訊流的模式。 . 根據阿里研究院提出的報告，新零售的價值主要有以下三點： . &#19968;&#12289; &#36861;&#27714;&#39015;&#23458;&#39636;&#39511; . 以滿足消費者的體驗和需求作為核心，新零售利用新科技整合產品、金／物流、以其客戶的數據回饋，以為顧客提供更優質的服務體驗 . &#20108;&#12289; &#20840;&#36890;&#36335;&#34395;&#23526;&#34701;&#21512; . 整合品牌全通路數據流，讓每個客戶在品牌的各個通路（包含線上以及線下），都可以獲得相對等的服務。 . &#19977;&#12289; &#38646;&#21806;&#21697;&#38917;&#22810;&#27171;&#21270; . 借助數據的資訊整合優勢，在新零售中企業可以更完整的了解消費者的需求和喜好，也因此可以更直接與快速地因應消費者的需求，開發出更多元的產品或是銷售策略的調整。 . &#26032;&#38646;&#21806;&#24118;&#20358;&#30340;&#37325;&#22823;&#35506;&#38988; . &#25976;&#20301;&#26178;&#20195;&#19979;&#30340;&#26032;&#31478;&#29229;&#32773;&#65306;&#31038;&#32676;&#23186;&#39636; . 既然新零售是以「追求顧客體驗」為中心，那麼，消費者的喜好，便是新零售中各項行銷策略所主要關注的事情。 . 更進一步，既然數位時代的消費者在進行消費時需要資料及意見蒐集，那麼，提供這些資料或是意見的平台，何不直接提供消費的渠道呢？ . 拜數位快速發展所致，數位社群媒體如Facebook、Instagram、Line等原先即以經營社群為主的平台，在與平台大量的會員互動數據之下，掌握了許多會員的喜好意向；加上消費者在資訊氾濫的環境下，透過認同的群體所得到的信息，更具有左右決定的相當分量。 . 也因此，這些數位時代的媒體平台，透過的數據演算，將內容社群化、個人化，讓每個人接收到他／她所喜歡的內容。並開始嘗試將社群平台的的人流與產品銷售進行連結，讓消費者可以破除實體消收鍾趖遭遇的時間以及空間（消費者必須親自到實體通路或是官網消費）上的限制，在最便利（最有效率）的情境下完成消費的行為。 . &#21697;&#29260;&#65288;&#35069;&#36896;&#21830;&#65289;&#33287;&#36890;&#36335;&#21830;&#30340;&#38364;&#20418;&#65292;&#19981;&#26159;&#31478;&#29229;&#65292;&#32780;&#26159;&#21512;&#20316;&#65292;&#32780;&#26368;&#22823;&#30340;&#24046;&#30064;&#65292;&#22312;&#26044;&#25152;&#25552;&#20379;&#30340;&#12300;&#39636;&#39511;&#12301; . 新零售是一種策略，不是一個產業，還是面對個別消費者進行商品銷售與提供服務。品牌與通路商之間，，自是需要協同合作，一起完成消費者從選擇到購買，進而到之後的售後及會員服務的整個銷售流程。銷售的商品雖然相同，這整個過程中所得到的體驗卻有所差異：有的消費者喜歡在購買前先試用產品，那麼實體通路就成為這類型顧客必到的地方；有的消費者希望趕快拿到購賣的產品，那麼提供24小時到貨服務的通路商便會成為首選。消費者期望的消費體驗類型太多，絕非單一品牌商或是通路商就可以滿足。 . &#26032;&#38646;&#21806;&#26399;&#35377;&#26997;&#22823;&#21270;&#22320;&#28415;&#36275;&#28040;&#36027;&#32773;&#30340;&#28040;&#36027;&#39636;&#39511;&#65292;&#32780;&#25976;&#20301;&#31185;&#25216;&#30340;&#30332;&#23637;&#21063;&#26159;&#25512;&#21205;&#26032;&#38646;&#21806;&#31574;&#30053;&#30340;&#37325;&#35201;&#38364;&#37749;&#12290; . &#25976;&#25818;&#25972;&#21512;&#26356;&#21152;&#20840;&#38754;&#65292;&#20197;&#22240;&#25033;&#28151;&#27788;&#22810;&#27171;&#30340;&#28040;&#36027;&#27511;&#31243; . 隨著資訊科技成長，消費者在購買前能獲取的資訊來源越來越多，導致在下單的資料蒐集與評估越來越複雜，像是比較網購與門市購買、各大論壇的心得分享、官網與通路商的促銷優惠等等。如此，消費者的購買路徑，將變得更為多樣以及複雜。 . 在新零售「追求顧客體驗」的重要課題下，透過大數據整合以及追蹤消費者行為，成為了許多品牌必須面對的課題。 . &#25976;&#20301;&#21270;&#24118;&#20358;&#30340;&#32068;&#32340;&#35722;&#38761; . 在擬定清晰的數位化策略與目標之後，業主和組織成員都必須跳脫舊有思維模式，包含教育員工使用新的數位工具、現有銷售／服務流程的重新調整與資源配置，方能共同迎接數位化變革，一同踏入新零售時代。 .",
            "url": "https://impepper.github.io/myPortfolio/%E6%96%B0%E9%9B%B6%E5%94%AE/%E5%95%86%E6%A5%AD%E7%B6%93%E7%87%9F/omo/2022/06/10/%E5%BE%9E%E9%8A%B7%E5%94%AE%E5%88%B0%E9%AB%94%E9%A9%97-%E9%9B%B6%E5%94%AE%E8%88%87%E6%96%B0%E9%9B%B6%E5%94%AE.html",
            "relUrl": "/%E6%96%B0%E9%9B%B6%E5%94%AE/%E5%95%86%E6%A5%AD%E7%B6%93%E7%87%9F/omo/2022/06/10/%E5%BE%9E%E9%8A%B7%E5%94%AE%E5%88%B0%E9%AB%94%E9%A9%97-%E9%9B%B6%E5%94%AE%E8%88%87%E6%96%B0%E9%9B%B6%E5%94%AE.html",
            "date": " • Jun 10, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Sales Prediction 透過預算分配預測業績",
            "content": "預測產品的未來銷售有助於企業更有效的管理產品的製造以及廣告成本。 因此，我希望透過這篇文章，來了解並學習使用機器學習模型預測產品的未來銷售量。 . 在這個Case Study中，所使用的數據資料包含了產品銷售的數據以及企業在各種廣告平台上所投放的媒體預算：以下是數據集中所有列的描述： . TV: 在電視上投放廣告所花費的廣告成本； | Radio: 在廣播上投放廣告的廣告費用（以美元計）； | Newspaper: 在報紙上投放廣告所花費的廣告成本； | Sales：售出的產品單位數量 | 因此，在上述數據資料中包含了產品的銷量以及產品的廣告成本。我希望了解這些數字之前是否有關連，並且利用 Python 進行機器學習的未來銷售預測。 . &#36039;&#26009;&#25972;&#29702; . 首先，我們導入此次專案所需要的 Python 函式庫以及數據資料： . import pandas as pd import numpy as np from sklearn import preprocessing from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression import plotly.express as px import plotly.graph_objects as go data = pd.read_csv(&quot;datasource/advertising.csv&quot;) print(data.head()) . TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 12.0 3 151.5 41.3 58.5 16.5 4 180.8 10.8 58.4 17.9 . 確認一下資料中有沒有無效的數據（例如空白欄位） . print(data.isnull().sum()) . TV 0 Radio 0 Newspaper 0 Sales 0 dtype: int64 . 看來資料品質不錯！ . 那，我們先來了解一下數據中各欄位與產品銷售數字之間的相關性： . 電視廣告成本與銷售數字的關聯性 . figure = px.scatter(data_frame = data, x=&quot;Sales&quot;, y=&quot;TV&quot;, size=&quot;TV&quot;, trendline=&quot;ols&quot;) #figure.show() . 透過 figure.show() 指令可以取得以下圖表: . 報紙廣告成本與銷售數字的關聯性 . figure = px.scatter(data_frame = data, x=&quot;Sales&quot;, y=&quot;Newspaper&quot;, size=&quot;Newspaper&quot;, trendline=&quot;ols&quot;) #figure.show() . . 電台廣告與銷售數字的關聯性 . figure = px.scatter(data_frame = data, x=&quot;Sales&quot;, y=&quot;Radio&quot;, size=&quot;Radio&quot;, trendline=&quot;ols&quot;) #figure.show() . . 在各種平台上的所有廣告費用中，可以看到在電視上為產品做廣告的費用會帶來更多的產品銷售。現在讓我們透過相關係數來了解所有欄位與銷售數字的相關性 . correlation = data.corr() print(correlation[&quot;Sales&quot;].sort_values(ascending=False)) . Sales 1.000000 TV 0.901208 Radio 0.349631 Newspaper 0.157960 Name: Sales, dtype: float64 . &#24314;&#31435;&#37559;&#21806;&#38928;&#28204;&#27169;&#22411; . 接下來，我們將數據分成訓練以及驗證兩部分，來進行線性回歸的機器學習模型訓練： . x = np.array(data.drop([&quot;Sales&quot;], 1)) y = np.array(data[&quot;Sales&quot;]) xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42) . C: Users impep anaconda3 envs OpenCV lib site-packages ipykernel_launcher.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument &#39;labels&#39; will be keyword-only . 開始訓練模型 . model = LinearRegression() model.fit(xtrain, ytrain) . LinearRegression() . 透過驗證資料比對來了解一下這個模型的準確程度 . print(model.score(xtest, ytest)) . 0.9059011844150826 . 現在讓我們根據我們訓練好的模型，透過輸入企業在各種平台上（電視|廣告|報紙）的廣告花費來預測可以銷售多少單位的產品： . features = np.array([[230.1, 37.8, 69.2]]) features_normalized = preprocessing.scale(features) print(model.predict(features)) . [21.37254028] . &#32317;&#32080; . 預測產品的未來銷售有助於企業管理產品的製造和廣告成本。我們在這次的案例中，透過線性回歸的機器學習訓練，來預測產品未來的銷售數字。 希望你喜歡這篇文章。 .",
            "url": "https://impepper.github.io/myPortfolio/python/prediction/jupyter/linear%20regression/sales/2022/04/20/Future-Sales-Prediction-with-Machine-Learning.html",
            "relUrl": "/python/prediction/jupyter/linear%20regression/sales/2022/04/20/Future-Sales-Prediction-with-Machine-Learning.html",
            "date": " • Apr 20, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Customer Lifetime Value (CLV)  用戶生命週期價值計算及消費預測",
            "content": "Customer Lifetime Value (CLV or CLTV) &#26159;&#20160;&#40636; . 顧名思義，Customer Lifetime Value就是用戶在其生命週期內所能帶來的商業價值。以商業市場來說，CLV 即是消費者從第一次購買你的產品，直到不再購買你的產品為止，在這段時間內對於你的公司所貢獻的商業價值。 . &#28858;&#20160;&#40636;&#38656;&#35201; CLV &#20316;&#28858;&#34913;&#37327;&#25351;&#27161; . &#36628;&#21161;&#21046;&#23450; CAC &#25351;&#27161; . CAC (Customer Acquisition Cost) 代表獲取一個顧客的成本，在商業經營中，唯有用戶在其生命週期內所帶來的商業價值（CLV） 大於 獲取這位顧客的成本(CAC)，這個 Business 才能持續地滾動下去。因此，透過CLV，商業經營者可以更加明確的規劃面相於故各地個性行銷活動，以確保商業模式能夠長久運轉。 . &#21487;&#25345;&#32396;&#24615;&#30340;&#22686;&#38263;&#25351;&#27161; . 隨著技術對用戶的生活產生改變，用戶的接觸點（Endpoints）越來越多多樣化，相對應的行銷策略與手段也因此而產生了重大變化，新零售（OMO）概念正充分說明，光是倚靠 Revenue 或是 CAC 來作為指標，無法有效的去衡量日常所做的每個項目投入；若是持續的僅使用這兩個指標來做衡量，甚至可能因此讓錯失了很多重要的項目無法被實施。 . CLV 和傳統常見的指標最大的不同在於：是將多個部門的「心血」所匯集在一起呈現的指標。 CLV的增長，反映了行銷、產品、業務跨三個部門協同運作的成果，將單一部門的活動轉化成了整體的經營成效增長。 . CLV一直是 Amazon 內部採用作為日常增長指標來判斷一個項目/業務是否成功的衡量方式，像是知名的 Kindle 業務即是在這樣的文化下所誕生並且成功。以一個近乎成本價的方式銷售 Kindle device，而購買 Kindle 後的用戶其設備上產生的消費金額，才是衡量該業務成功與否的核心指標。 . 在這個實作中，我將以消費紀錄為基礎，利用機器學習，來計算消費者 Lifetime Value，並推估該用戶未來六個月的消費數據。 . 數據資料 . 在本次使用的消費紀錄資料中，包含了以下欄位： . 消費日期 | Invoice Number | 客戶編號 | 消費金額 | . 首先，我們導入此次專案所需要的 Python 函式庫 . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set() import warnings warnings.filterwarnings(&#39;ignore&#39;) . 接下來，進行數據資料的讀取及匯入 . #data = pd.read_csv(&quot;datasource/data.csv&quot;, parse_dates=[&#39;invoice_date&#39;]) data_clv = pd.read_csv(&quot;datasource/CLV_data.csv&quot;, parse_dates=[&#39;invoice_date&#39;]) print(&quot;Total number of transactions happened in the given period: &quot;+ str(data_clv.shape[0])) data_clv.head(5) . Total number of transactions happened in the given period: 32030 . invoice_id invoice_date invoice_cus_id invoice_amount . 0 2021020600200001 | 2021-02-06 09:41:00 | 701000008.0 | 245 | . 1 2021020600200002 | 2021-02-06 09:43:00 | 701000001.0 | 240 | . 2 2021020600200003 | 2021-02-06 09:45:00 | 701000010.0 | 100 | . 3 2021020600200004 | 2021-02-06 09:46:00 | 701000010.0 | 515 | . 4 2021020600200005 | 2021-02-06 09:46:00 | 701000010.0 | 140 | . 因為退貨、折扣等因素，消費金額中可能會有負數出現。不過，在客戶生命週期價值計算中，我們只對客戶在其生命週期內可以為企業產生的總價值感興趣，我們可以簡單地將消費中的負值紀錄除掉，只使用正值。 . data_clv = data_clv[data_clv[&#39;invoice_amount&#39;] &gt; 0] data_clv.describe() #了解目前數列的各項統計數據 . invoice_cus_id invoice_amount . count 3.144700e+04 | 31447.000000 | . mean 7.211368e+08 | 354.360448 | . std 3.949744e+09 | 860.162176 | . min 2.010000e+08 | 1.000000 | . 25% 7.010000e+08 | 89.000000 | . 50% 7.010000e+08 | 150.000000 | . 75% 7.010002e+08 | 345.000000 | . max 7.010030e+11 | 67080.000000 | . 我們檢查一下現有資料中有沒有其他遺漏數據的資料列，像是欄位值為空值之類。 . 此外，因為接下來可能需要計算每位客戶的CLV，我們也要過濾掉客戶資料欄為空值的資料 . data_clv = data_clv[pd.notnull(data_clv[&#39;invoice_cus_id&#39;])] pd.DataFrame(zip(data_clv.isnull().sum(), data_clv.isnull().sum()/len(data_clv)), columns=[&#39;Count&#39;, &#39;Proportion&#39;], index=data_clv.columns) . Count Proportion . invoice_id 0 | 0.0 | . invoice_date 0 | 0.0 | . invoice_cus_id 0 | 0.0 | . invoice_amount 0 | 0.0 | . 至此，資料已經整理完畢，讓我們初步看一下目前數據的基本資料： . maxdate = data_clv[&#39;invoice_date&#39;].dt.date.max() mindate = data_clv[&#39;invoice_date&#39;].dt.date.min() unique_cust = data_clv[&#39;invoice_cus_id&#39;].nunique() tot_sales = data_clv[&#39;invoice_amount&#39;].sum() print(f&quot;The Time range of transactions is: {mindate} to {maxdate}&quot;) print(f&quot;Total number of unique customers: {unique_cust}&quot;) print(f&quot;Total Sales for the period: {tot_sales}&quot;) . The Time range of transactions is: 2021-02-06 to 2022-04-20 Total number of unique customers: 2615 Total Sales for the period: 11143573 . &#35336;&#31639; Customer Lifetime Value . Aggregate Model . 計算 CLV 的最簡單和最古老的方法是平均法。這假設所有客戶的平均支出和流失率保持不變。 . 此方法不區分單一客戶，只會整體地生成單一的CLV數值。雖然對於一般消費者還算堪用，不過如果一些客戶進行高價值和高交易量的交易，對於這類消費者的生命週期價值將出現不切實際的估計。 . 平均法中用以計算CLV的公式如下： . CLV = ((Average Sales X Purchase Frequency) / Churn) X Profit Margin . 其中， . Average Sales = TotalSales/Total no. of orders . Purchase Frequency = Total no. of orders/Total unique customers . Retention rate = Total no. of orders greater than 1/ Total unique customers . Churn = 1 - Retention rate . Profit Margin = Based on business context . customer = data_clv.groupby(&#39;invoice_cus_id&#39;).agg({&#39;invoice_date&#39;:lambda x: (x.max() - x.min()).days, &#39;invoice_id&#39;: lambda x: len(x), &#39;invoice_amount&#39;: lambda x: sum(x)}) customer.columns = [&#39;Age&#39;, &#39;Frequency&#39;, &#39;TotalSales&#39;] customer.head() . Age Frequency TotalSales . invoice_cus_id . 201000002.0 434 | 172 | 64997 | . 701000001.0 437 | 322 | 231844 | . 701000002.0 387 | 208 | 149509 | . 701000003.0 294 | 110 | 40052 | . 701000004.0 124 | 11 | 7749 | . Average_sales = round(np.mean(customer[&#39;TotalSales&#39;]),2) print(f&quot;Average sales: ${Average_sales}&quot;) Purchase_freq = round(np.mean(customer[&#39;Frequency&#39;]), 2) print(f&quot;Purchase Frequency: {Purchase_freq}&quot;) Retention_rate = customer[customer[&#39;Frequency&#39;]&gt;1].shape[0]/customer.shape[0] churn = round(1 - Retention_rate, 2) print(f&quot;Churn: {churn}%&quot;) . Average sales: $4261.4 Purchase Frequency: 12.03 Churn: 0.53% . 假設每筆交易所帶來的利潤空間(Profit margin)為5%進行計算 . Profit_margin = 0.05 CLV = round(((Average_sales * Purchase_freq/churn)) * Profit_margin, 2) print(f&quot;The Customer Lifetime Value (CLV) for each customer is: ${CLV}&quot;) . The Customer Lifetime Value (CLV) for each customer is: $4836.29 . 從這個方法中，我們得到了每個客戶的 CLV 值。 你覺得這個數字有意義嗎？ 原因是因為來自極少數客戶具有非常高的銷售金額，而這個CLV值實際上完全無法反應該類消費者的生命週期價值 . Cohort Model . 相比計算一個整體的CLV值，我們可以嘗試將他們分成多個組來計算每個組的 CLV，而不是簡單地假設所有客戶為一個組。這稱為隊列模型。 . 該模型的主要假設是，同組中的客戶花費相似。 . 這種模型克服了平均法模型的主要缺點 。 將客戶分組為同類群組的最常見方法是按客戶的開始日期，通常是按照月份。 . 在這種假設狀況下，我依照他們的消費開始月份將他們分組到不同的群組中。 . customer = data_clv.groupby(&#39;invoice_cus_id&#39;).agg({&#39;invoice_date&#39;:lambda x: x.min().month, &#39;invoice_id&#39;: lambda x: len(x), &#39;invoice_amount&#39;: lambda x: np.sum(x)}) customer.columns = [&#39;Start_Month&#39;, &#39;Frequency&#39;, &#39;TotalSales&#39;] customer.head() . Start_Month Frequency TotalSales . invoice_cus_id . 201000002.0 2 | 172 | 64997 | . 701000001.0 2 | 322 | 231844 | . 701000002.0 2 | 208 | 149509 | . 701000003.0 2 | 110 | 40052 | . 701000004.0 2 | 11 | 7749 | . months = [&#39;Jan&#39;, &#39;Feb&#39;, &#39;March&#39;, &#39;Apr&#39;, &#39;May&#39;, &#39;Jun&#39;, &#39;Jul&#39;, &#39;Aug&#39;, &#39;Sep&#39;, &#39;Oct&#39;, &#39;Nov&#39;, &#39;Dec&#39;] Monthly_CLV = [] for i in range(1, 13): customer_m = customer[customer[&#39;Start_Month&#39;]==i] Average_sales = round(np.mean(customer_m[&#39;TotalSales&#39;]),2) Purchase_freq = round(np.mean(customer_m[&#39;Frequency&#39;]), 2) Retention_rate = customer_m[customer_m[&#39;Frequency&#39;]&gt;1].shape[0]/customer_m.shape[0] churn = round(1 - Retention_rate, 2) CLV = round(((Average_sales * Purchase_freq/churn)) * Profit_margin, 2) Monthly_CLV.append(CLV) . monthly_clv = pd.DataFrame(zip(months, Monthly_CLV), columns=[&#39;Months&#39;, &#39;CLV&#39;]) display(monthly_clv.style.background_gradient()) . &nbsp; Months CLV . 0 Jan | 3673.600000 | . 1 Feb | 68379.320000 | . 2 March | 453.970000 | . 3 Apr | 386.970000 | . 4 May | 963.540000 | . 5 Jun | 1815.530000 | . 6 Jul | 1221.690000 | . 7 Aug | 927.490000 | . 8 Sep | 402.360000 | . 9 Oct | 305.040000 | . 10 Nov | 223.430000 | . 11 Dec | 92.510000 | . 現在，如果查看結果，從1月到12月，我們有 12 個不同的 CLV 值。 . 很明顯，在不同月份獲得的客戶具有不同的 CLV 值。 這是因為，他們可以通過不同的活動等方式獲得，因此他們的行為可能與其他人不同。 . BG/NBD Model (with Gamma-Gamma extension) . BG/NBD 指的是概率論中的 β 負二項分佈（離散隨機變量 X 的概率分佈） . 這是用於預測 CLV 的最常用的概率模型之一，也是 CLV 計算中最常用的方法之一。 . 在本例中，我們將只關注 BG/NBD 模型。 BG/NBD 模型實際上試圖預測每個客戶的未來交易。然後將其與 Gamma-Gamma 模型相結合，得到客戶生命週期價值 (CLV)。 . The BG/NBD 有一些假設條件： . When a user is active, number of transactions in a time t is described by Poisson distribution with rate lambda. | Heterogeneity in transaction across users (difference in purchasing behavior across users) has Gamma distribution with shape parameter r and scale parameter a. | Users may become inactive after any transaction with probability p and their dropout point is distributed between purchases with Geometric distribution. | Heterogeneity in dropout probability has Beta distribution with the two shape parameters alpha and beta. | Transaction rate and dropout probability vary independently across users. | 這些是該模型在預測客戶未來交易時考慮的一些假設。 . 我們不必擔心自己執行這個複雜的概率模型。有一個名為 Lifetimes 的 Python 函式庫，主要用於幫助計算客戶生命週期價值、預測客戶流失等。它具有 CLV 計算所需的所有主要模型和實用功能。 . 在這種情況下，我們將利用這個函式庫，快速進行CLV計算。 . import lifetimes . 首先，我們需要匯總我們的交易數據，使其成為一個客戶級別的 RFM 表格。 （RFM - Recency、Freguency &amp; Monetary） . 為此，我們可以在Lifetimes 函式庫中使用 summary_data_from_transactions_data 函數。 . 他所做的是將交易級別數據整合到客戶級別，並且計算每個客戶的Recency、Freguency、 T &amp; Monetary: . frequency - the number of repeat purchases (more than 1 purchases) | recency - the time between the first and the last transaction | T - the time between the first purchase and the end of the transaction period | monetary_value - it is the mean of a given customers sales value | . summary = lifetimes.utils.summary_data_from_transaction_data(data_clv, &#39;invoice_cus_id&#39;, &#39;invoice_date&#39;, &#39;invoice_amount&#39; ) summary = summary.reset_index() summary.head() . invoice_cus_id frequency recency T monetary_value . 0 201000002.0 | 122.0 | 434.0 | 438.0 | 527.270492 | . 1 701000001.0 | 181.0 | 437.0 | 438.0 | 1279.011050 | . 2 701000002.0 | 136.0 | 387.0 | 438.0 | 1092.125000 | . 3 701000003.0 | 81.0 | 294.0 | 427.0 | 480.765432 | . 4 701000004.0 | 9.0 | 125.0 | 435.0 | 853.222222 | . Here the value of 0 in frequency and recency means that, these are one time buyers. Let&#39;s check how many such one time buyers are there in our data. . summary[&#39;frequency&#39;].plot(kind=&#39;hist&#39;, bins=50) print(summary[&#39;frequency&#39;].describe()) print(&quot;&quot;) one_time_buyers = round(sum(summary[&#39;frequency&#39;] == 0)/float(len(summary))*(100),2) print(&quot;Percentage of customers purchase the item only once:&quot;, one_time_buyers ,&quot;%&quot;) . count 2615.000000 mean 2.221797 std 9.950323 min 0.000000 25% 0.000000 50% 0.000000 75% 2.000000 max 367.000000 Name: frequency, dtype: float64 Percentage of customers purchase the item only once: 55.72 % . Now, let&#39;s fit the BG/NBD model to our summary data. . BG/NBD model is available as BetaGeoFitter class in lifetimes package. . bgf = lifetimes.BetaGeoFitter(penalizer_coef=0.001) bgf.fit(summary[&#39;frequency&#39;], summary[&#39;recency&#39;], summary[&#39;T&#39;]) . &lt;lifetimes.BetaGeoFitter: fitted with 2615 subjects, a: 0.39, alpha: 16.78, b: 1.33, r: 0.22&gt; . bgf.summary . coef se(coef) lower 95% bound upper 95% bound . r 0.218279 | 0.008915 | 0.200805 | 0.235753 | . alpha 16.779933 | 1.179340 | 14.468426 | 19.091440 | . a 0.387987 | 0.042371 | 0.304941 | 0.471034 | . b 1.334940 | 0.184841 | 0.972652 | 1.697228 | . 上表顯示了從歷史數據中估計的分佈參數值。 該模型會使用它來預測未來的交易和客戶流失率。 . 因此，假設您想根據歷史數據了解客戶現在是否還活著（或預測客戶流失）。 在Lifetimes 函式庫中可以使用： . 1. model.conditional_probability_alive(): 此函數可以用來計算具有歷史記錄（frequency, recency, T）的客戶當前還活著的概率。 . 2. plot_probabilty_alive_matrix(model): 這個函數可以協助我們直觀地分析recency, frequency與客戶存活之間的關係。 . summary[&#39;probability_alive&#39;] = bgf.conditional_probability_alive(summary[&#39;frequency&#39;], summary[&#39;recency&#39;], summary[&#39;T&#39;]) summary.head(10) . invoice_cus_id frequency recency T monetary_value probability_alive . 0 201000002.0 | 122.0 | 434.0 | 438.0 | 527.270492 | 9.907499e-01 | . 1 701000001.0 | 181.0 | 437.0 | 438.0 | 1279.011050 | 9.968217e-01 | . 2 701000002.0 | 136.0 | 387.0 | 438.0 | 1092.125000 | 3.230038e-05 | . 3 701000003.0 | 81.0 | 294.0 | 427.0 | 480.765432 | 5.699000e-11 | . 4 701000004.0 | 9.0 | 125.0 | 435.0 | 853.222222 | 5.512930e-04 | . 5 701000006.0 | 4.0 | 302.0 | 401.0 | 165.000000 | 7.811945e-01 | . 6 701000008.0 | 0.0 | 0.0 | 438.0 | 0.000000 | 1.000000e+00 | . 7 701000009.0 | 4.0 | 305.0 | 425.0 | 972.500000 | 7.458372e-01 | . 8 701000010.0 | 367.0 | 438.0 | 438.0 | 13758.144414 | 9.989449e-01 | . 9 701000011.0 | 1.0 | 88.0 | 438.0 | 6350.000000 | 3.652369e-01 | . from lifetimes.plotting import plot_probability_alive_matrix fig = plt.figure(figsize=(12,8)) plot_probability_alive_matrix(bgf) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Probability Customer is Alive, nby Frequency and Recency of a Customer&#39;}, xlabel=&#34;Customer&#39;s Historical Frequency&#34;, ylabel=&#34;Customer&#39;s Recency&#34;&gt; . 稍微解釋一下： . 客戶存活機率，是依照客戶的Recency以及Frequency來計算出來的，所以： . 如果客戶購買了多次（Frequency）並且第一次和最後一次交易之間的時間很長（Recency），那麼他/她存續的可能性就很高。 | 同樣，如果客戶的頻率較低（只有購買一次或兩次）並且第一次和最後一次交易之間的時間很短（Recency），那麼他/她的存活概率就很高。 | . 接下來我們還可以使用這個經過訓練的模型來預測每個客戶未來可能發生的交易。 您可以使用： . model.conditional_expected_number_of_purchases_up_to_time() . 計算從人群（或整個人群）中隨機選擇的個體到時間 t 的預期重複購買次數 - 假設他們有購買記錄（frequency, recency, T）。 . t = 30 # 預測接下來的30天內，各個消費者的預期重複回購次數 summary[&#39;pred_num_txn&#39;] = round(bgf.conditional_expected_number_of_purchases_up_to_time(t, summary[&#39;frequency&#39;], summary[&#39;recency&#39;], summary[&#39;T&#39;]),2) summary.sort_values(by=&#39;pred_num_txn&#39;, ascending=False).head(10).reset_index() . index invoice_cus_id frequency recency T monetary_value probability_alive pred_num_txn . 0 8 | 701000010.0 | 367.0 | 438.0 | 438.0 | 13758.144414 | 0.998945 | 23.90 | . 1 2392 | 701003443.0 | 79.0 | 98.0 | 98.0 | 3692.278481 | 0.995133 | 19.67 | . 2 1 | 701000001.0 | 181.0 | 437.0 | 438.0 | 1279.011050 | 0.996822 | 11.77 | . 3 0 | 201000002.0 | 122.0 | 434.0 | 438.0 | 527.270492 | 0.990750 | 7.89 | . 4 1328 | 701001605.0 | 48.0 | 342.0 | 342.0 | 289.708333 | 0.992037 | 3.94 | . 5 1816 | 701002433.0 | 34.0 | 241.0 | 245.0 | 437.352941 | 0.981221 | 3.77 | . 6 1751 | 701002326.0 | 30.0 | 257.0 | 258.0 | 140.466667 | 0.985921 | 3.19 | . 7 2531 | 701003765.0 | 6.0 | 33.0 | 34.0 | 271.666667 | 0.935184 | 3.14 | . 8 335 | 701000367.0 | 47.0 | 413.0 | 421.0 | 1095.872340 | 0.980795 | 3.13 | . 9 1616 | 701002107.0 | 29.0 | 284.0 | 289.0 | 2824.275862 | 0.979038 | 2.76 | . 既然我們預測了預期的未來交易，我們現在需要預測每筆交易的未來貨幣價值。 . 就像我之前提到的，BG/NBD 模型只能預測客戶的未來交易和流失率。 為了增加問題的貨幣方面，我們必須使用 Gamma-Gamma 模型 對貨幣價值進行建模。 . Gamma-Gamma 模型的一些關鍵假設是： . 客戶給定交易的貨幣價值在其平均交易價值附近隨機變化。 | 平均交易價值因客戶而異，但對於任何給定客戶不會隨時間而變化。 | 客戶之間平均交易價值的分佈與交易過程無關。 | 作為將模型擬合到數據之前的第一步，我們必須檢查模型所做的假設是否適用於這些數據：只有滿足了，我們才繼續進行。 . 注意：我們只考慮重複購買的客戶，即頻率 &gt; 0。因為如果頻率為 0，則意味著他們是一次性客戶並且被認為已經流失。 . return_customers_summary = summary[summary[&#39;frequency&#39;]&gt;0] return_customers_summary[[&#39;frequency&#39;, &#39;monetary_value&#39;]].corr() . frequency monetary_value . frequency 1.000000 | 0.308369 | . monetary_value 0.308369 | 1.000000 | . 在這個CASE中，交易的頻率和貨幣價值之間沒有關係，我們可以透過相關性驗證其相關姓性並不顯著。 . 再來，我們利用 Gamma-Gamma 模型進行建模： . ggf = lifetimes.GammaGammaFitter(penalizer_coef=0.001) ggf.fit(return_customers_summary[&#39;frequency&#39;], return_customers_summary[&#39;monetary_value&#39;]) # Summary of the fitted parameters ggf.summary . coef se(coef) lower 95% bound upper 95% bound . p 10.770713 | 0.421400 | 9.944770 | 11.596656 | . q 0.665780 | 0.023646 | 0.619433 | 0.712127 | . v 10.636420 | 0.429762 | 9.794086 | 11.478754 | . 接下來，我們可以使用該模型預測每筆交易的預期平均利潤和客戶生命週期價值。 . 1. model.conditional_expected_average_profit(): 該函數計算一個或多個客戶的每筆交易的期望平均利潤。 . 2. model.customer_lifetime_value(): 該函數計算一組一個或多個客戶的平均生命週期價值。 該方法以 BG/NBD 模型和預測範圍作為參數來計算 CLV。 . summary = summary[summary[&#39;monetary_value&#39;] &gt;0] summary[&#39;exp_avg_sales&#39;] = ggf.conditional_expected_average_profit(summary[&#39;frequency&#39;], summary[&#39;monetary_value&#39;]) summary.head() . invoice_cus_id frequency recency T monetary_value probability_alive pred_num_txn exp_avg_sales . 0 201000002.0 | 122.0 | 434.0 | 438.0 | 527.270492 | 9.907499e-01 | 7.89 | 527.491842 | . 1 701000001.0 | 181.0 | 437.0 | 438.0 | 1279.011050 | 9.968217e-01 | 11.77 | 1279.289134 | . 2 701000002.0 | 136.0 | 387.0 | 438.0 | 1092.125000 | 3.230038e-05 | 0.00 | 1092.452468 | . 3 701000003.0 | 81.0 | 294.0 | 427.0 | 480.765432 | 5.699000e-11 | 0.00 | 481.081044 | . 4 701000004.0 | 9.0 | 125.0 | 435.0 | 853.222222 | 5.512930e-04 | 0.00 | 857.360073 | . 注意： 我們使用上述方法所得到的銷售額而不是利潤。我們可以另外將結果乘以我們的利潤率，得出實際利潤值。 . print(f&quot;Expected Average Sales: {summary[&#39;exp_avg_sales&#39;].mean()}&quot;) print(f&quot;Actual Average Sales: {summary[&#39;monetary_value&#39;].mean()}&quot;) . Expected Average Sales: 731.1509233516309 Actual Average Sales: 713.8926372998563 . 現在，讓我們直接使用Lifetime 函式庫中的函數來計算客戶生命週期價值。 . 有幾件事情要注意： . 1. time: customer_lifetime_value()函數中的這個參數以月為單位，t=1表示一個月，以此類推。 . 2. freq: 此參數是您將指定數據所在的時間單位的位置。如果您的數據是每日級別，那麼“D”，每月“M”等等。 . 3. discount_rate: 這個參數是基於DCF（discounted cash flow）的概念，你將未來的貨幣價值通過一個貼現率貼現得到該現金流的現值。 在文件中，給出的預設值是每月 0.01（每年 ~12.7%）。 . summary[&#39;predicted_clv&#39;] = ggf.customer_lifetime_value(bgf, summary[&#39;frequency&#39;], summary[&#39;recency&#39;], summary[&#39;T&#39;], summary[&#39;monetary_value&#39;], time=1, # lifetime in months freq=&#39;D&#39;, # frequency in which the data is present(T) discount_rate=0.01) # discount rate summary.head() . invoice_cus_id frequency recency T monetary_value probability_alive pred_num_txn exp_avg_sales predicted_clv . 0 201000002.0 | 122.0 | 434.0 | 438.0 | 527.270492 | 9.907499e-01 | 7.89 | 527.491842 | 4.120102e+03 | . 1 701000001.0 | 181.0 | 437.0 | 438.0 | 1279.011050 | 9.968217e-01 | 11.77 | 1279.289134 | 1.490641e+04 | . 2 701000002.0 | 136.0 | 387.0 | 438.0 | 1092.125000 | 3.230038e-05 | 0.00 | 1092.452468 | 3.100526e-01 | . 3 701000003.0 | 81.0 | 294.0 | 427.0 | 480.765432 | 5.699000e-11 | 0.00 | 481.081044 | 1.471555e-07 | . 4 701000004.0 | 9.0 | 125.0 | 435.0 | 853.222222 | 5.512930e-04 | 0.00 | 857.360073 | 2.830461e-01 | . 您還可以根據未來交易的預測數量 (pred_num_txn) 和每筆交易的預期平均銷售額 (exp_avg_sales) 手動計算 CLV。 . summary[&#39;manual_predicted_clv&#39;] = summary[&#39;pred_num_txn&#39;] * summary[&#39;exp_avg_sales&#39;] summary.head() . invoice_cus_id frequency recency T monetary_value probability_alive pred_num_txn exp_avg_sales predicted_clv manual_predicted_clv . 0 201000002.0 | 122.0 | 434.0 | 438.0 | 527.270492 | 9.907499e-01 | 7.89 | 527.491842 | 4.120102e+03 | 4161.910633 | . 1 701000001.0 | 181.0 | 437.0 | 438.0 | 1279.011050 | 9.968217e-01 | 11.77 | 1279.289134 | 1.490641e+04 | 15057.233110 | . 2 701000002.0 | 136.0 | 387.0 | 438.0 | 1092.125000 | 3.230038e-05 | 0.00 | 1092.452468 | 3.100526e-01 | 0.000000 | . 3 701000003.0 | 81.0 | 294.0 | 427.0 | 480.765432 | 5.699000e-11 | 0.00 | 481.081044 | 1.471555e-07 | 0.000000 | . 4 701000004.0 | 9.0 | 125.0 | 435.0 | 853.222222 | 5.512930e-04 | 0.00 | 857.360073 | 2.830461e-01 | 0.000000 | . 兩個 CLV 值非常接近，並且在接下來的 30 天內似乎是合理的。 . 這裡需要注意的一點是，我們為 CLV 計算的兩個值都是銷售價值，而不是實際利潤。 . Summary . 我們預測了每個客戶未來 30 天的 CLV。 . 行銷及業務團隊現在可以透過這些信息來定位客戶並尋求合適的方式來增加他們的銷售額。 . 希望你喜歡這篇文章。 . &#24310;&#20280;&#38321;&#35712;&#65306;&#36879;&#36942; K-means &#36914;&#34892;&#28040;&#36027;&#26063;&#32676;&#20998;&#32676; . 在我們取得消費者個交易紀錄後，我們可以嘗試透過機器學習的方式，來將這些消費這進行適當的分群 (Cluster) . 先載入相關的 Python 函式庫 . from sklearn.cluster import KMeans from scipy import stats from sklearn.preprocessing import StandardScaler from sklearn.manifold import TSNE . 我們需要建立一個副程式來檢查數據的偏移程度 . def check_skew(df_skew, column): skew = stats.skew(df_skew[column]) skewtest = stats.skewtest(df_skew[column]) plt.title(&#39;Distribution of &#39; + column) sns.distplot(df_skew[column]) print(&quot;{}&#39;s: Skew: {}, : {}&quot;.format(column, skew, skewtest)) return . 由於我們希望透過消費者交易的Recency/Frenquency/Monetary來進行分群，我們檢查一下相關的數據： . plt.figure(figsize=(9, 9)) plt.subplot(3, 1, 1) check_skew(summary,&#39;recency&#39;) plt.subplot(3, 1, 2) check_skew(summary,&#39;frequency&#39;) plt.subplot(3, 1, 3) check_skew(summary,&#39;monetary_value&#39;) plt.tight_layout() . recency&#39;s: Skew: 0.40947604860633036, : SkewtestResult(statistic=5.512357451345715, pvalue=3.54058924329199e-08) frequency&#39;s: Skew: 16.63523236612824, : SkewtestResult(statistic=42.62794152572261, pvalue=0.0) monetary_value&#39;s: Skew: 5.81288775759373, : SkewtestResult(statistic=30.559282782393712, pvalue=4.256870081427745e-205) . 我們注意到，相關數據的偏移度過大（具有長尾現象），我們需要將這些數據重新整理，進行特徵標準化(normalization)工作 . 我們首先將相關數據轉換為對數形式： . clv_rfm = summary[[&#39;invoice_cus_id&#39;,&#39;recency&#39;,&#39;frequency&#39;,&#39;T&#39;,&#39;monetary_value&#39;]] clv_rfm[&#39;invoice_cus_id&#39;].astype(&quot;string&quot;) . 0 201000002.0 1 701000001.0 2 701000002.0 3 701000003.0 4 701000004.0 ... 2560 701003812.0 2562 701003819.0 2563 701003820.0 2585 701003866.0 2613 7010015607.0 Name: invoice_cus_id, Length: 1158, dtype: string . clv_log = np.log(clv_rfm) plt.figure(figsize=(9, 9)) plt.subplot(3, 1, 1) check_skew(clv_log,&#39;recency&#39;) plt.subplot(3, 1, 2) check_skew(clv_log,&#39;frequency&#39;) plt.subplot(3, 1, 3) check_skew(clv_log,&#39;monetary_value&#39;) plt.tight_layout() . recency&#39;s: Skew: -1.3410639462558533, : SkewtestResult(statistic=-14.558048034387495, pvalue=5.1919215569780646e-48) frequency&#39;s: Skew: 0.9384463240043922, : SkewtestResult(statistic=11.248149137017123, pvalue=2.3650279001736935e-29) monetary_value&#39;s: Skew: 0.023739117952762616, : SkewtestResult(statistic=0.33185198477126177, pvalue=0.7400010282457292) . 可以看到當數據以對數形式呈現時，其偏移度相對改善許多，接下來，我們繼續處理特徵標準化(normalization)工作 . scaler = StandardScaler() RFM_Table_scaled_array = scaler.fit_transform(clv_log) . RFM_Table_scaled = pd.DataFrame(RFM_Table_scaled_array ,columns=clv_log.columns,index=clv_log.index,) RFM_Table_scaled.head() . invoice_cus_id recency frequency T monetary_value . 0 -16.240378 | 1.124740 | 4.005419 | 0.830115 | 0.220959 | . 1 -0.011846 | 1.129844 | 4.416920 | 0.830115 | 1.092030 | . 2 -0.011846 | 1.039817 | 4.118742 | 0.830115 | 0.936752 | . 3 -0.011846 | 0.836180 | 3.578171 | 0.780481 | 0.130194 | . 4 -0.011846 | 0.202501 | 1.286118 | 0.816703 | 0.694087 | . K-means&#30340;&#36939;&#20316; . K-means運作概念步驟: . 我們先設定好要分成多少(k)群。 | 然後在feature space(x軸身高和y軸體重組出來的2維空間，假設資料是d維，則會組出d維空間)隨機給k個群心。 | 每個資料都會所有k個群心算歐式距離(歐基李德距離Euclidean distance，其實就是直線距離公式，從小學到大的那個距離公式，這邊距離當然也可以換成別種距離公式，但基本上都還是以歐式距離為主)。 | 將每筆資料分類判給距離最近的那個群心。 | 每個群心內都會有被分類過來的資料，用這些資料更新一次新的群心。 | 一直重複3–5，直到所有群心不在有太大的變動(收斂)，結束 | K-means clustering 就是希望可以最小化群內的資料和群心的誤差平方和越小越好。我們根據這個概念，嘗試透過Elbow Method，推論處最適合的K值 . Elbow Method是在 K-means 算法中將模型與 K 值的範圍擬合來選擇最佳數最流行的方法之一。它需要在 SSE（平方誤差和）與簇數之間繪製一條線圖，並找到代表“Elbow&quot;的點（在該點之後，SSE 或慣性開始以線性方式減小）。 . from scipy.spatial.distance import cdist distortions = [] inertias = [] mapping1 = {} mapping2 = {} K = range(1,10) for k in K: #Building and fitting the model kmeanModel = KMeans(n_clusters=k).fit(RFM_Table_scaled) kmeanModel.fit(RFM_Table_scaled) distortions.append(sum(np.min(cdist(RFM_Table_scaled, kmeanModel.cluster_centers_, &#39;euclidean&#39;),axis=1)) / RFM_Table_scaled.shape[0]) inertias.append(kmeanModel.inertia_) mapping1[k] = sum(np.min(cdist(RFM_Table_scaled, kmeanModel.cluster_centers_, &#39;euclidean&#39;),axis=1)) / RFM_Table_scaled.shape[0] mapping2[k] = kmeanModel.inertia_ plt.plot(K,inertias,&#39;bx-&#39;) plt.xlabel(&#39;Values of K&#39;) plt.ylabel(&#39;Inertia&#39;) plt.title(&#39;The Elbow Method using Inertia&#39;) plt.show() . 我們在Elbow Method中所得到的數值（圖表） ，並沒有明倩的Elobow點，我們需要做更多嘗試 . 我們再建立一個副程式，將不同K值的特徵圖表描繪出來： . def kmeans(normalised_df_rfm, clusters_number, original_df_rfm): kmeans = KMeans(n_clusters = clusters_number, random_state = 1) kmeans.fit(normalised_df_rfm) # Extract cluster labels cluster_labels = kmeans.labels_ # Create a cluster label column in original dataset df_new = original_df_rfm.assign(Cluster = cluster_labels) # Initialise TSNE model = TSNE(random_state=1) transformed = model.fit_transform(df_new) # Plot t-SNE plt.title(&#39;Flattened Graph of {} Clusters&#39;.format(clusters_number)) sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=cluster_labels, style=cluster_labels, palette=&quot;Set1&quot;) return df_new . 我們嘗試以K值為 4/5/6 分別進行圖表繪製： . plt.figure(figsize=(10, 10)) plt.subplot(3, 1, 1) df_rfm_k4 = kmeans(RFM_Table_scaled, 4, clv_rfm) plt.subplot(3, 1, 2) df_rfm_k5 = kmeans(RFM_Table_scaled, 5, clv_rfm) plt.subplot(3, 1, 3) df_rfm_k6 = kmeans(RFM_Table_scaled, 6, clv_rfm) plt.tight_layout() . 我們再撰寫一個副程式，繪製分群的Snake Plot： . def snake_plot(normalised_df_rfm, df_rfm_kmeans, df_rfm_original): normalised_df_rfm = pd.DataFrame(normalised_df_rfm, index=df_rfm_original.index, columns=df_rfm_original.columns) normalised_df_rfm[&#39;Cluster&#39;] = df_rfm_kmeans[&#39;Cluster&#39;] # Melt data into long format df_melt = pd.melt(normalised_df_rfm.reset_index(), id_vars=[&#39;invoice_cus_id&#39;, &#39;Cluster&#39;], value_vars=[&#39;recency&#39;, &#39;frequency&#39;, &#39;monetary_value&#39;], var_name=&#39;Metric&#39;, value_name=&#39;Value&#39;) plt.xlabel(&#39;Metric&#39;) plt.ylabel(&#39;Value&#39;) sns.pointplot(data=df_melt, x=&#39;Metric&#39;, y=&#39;Value&#39;, hue=&#39;Cluster&#39;) return . 以K值為 4/5/6 分別進行圖表繪製： . snake_plot(RFM_Table_scaled_array,df_rfm_k4,clv_log) . snake_plot(RFM_Table_scaled_array,df_rfm_k5,clv_log) . snake_plot(RFM_Table_scaled_array,df_rfm_k6,clv_log) . 根據上面圖表中的特徵差異，選擇合適的Cluster數量 . 目前，應以5為合適值 . df_cluster_selected = df_rfm_k5.copy() df_cluster_selected.head() df_cluster_selected.groupby(&#39;Cluster&#39;).describe() . invoice_cus_id recency ... T monetary_value . count mean std min 25% 50% 75% max count mean ... 75% max count mean std min 25% 50% 75% max . Cluster . 0 104.0 | 7.010032e+08 | 6.576772e+02 | 7.010007e+08 | 7.010032e+08 | 7.010034e+08 | 7.010036e+08 | 7.010039e+08 | 104.0 | 42.509615 | ... | 131.5 | 307.0 | 104.0 | 1303.274279 | 1750.402065 | 35.000000 | 288.333333 | 687.791667 | 1383.250 | 9615.000000 | . 1 486.0 | 7.010016e+08 | 9.175926e+02 | 7.010000e+08 | 7.010008e+08 | 7.010016e+08 | 7.010024e+08 | 7.010032e+08 | 486.0 | 157.273663 | ... | 404.0 | 438.0 | 486.0 | 708.833813 | 768.709031 | 45.000000 | 265.416667 | 450.000000 | 843.875 | 6350.000000 | . 2 360.0 | 6.996122e+08 | 2.635237e+07 | 2.010000e+08 | 7.010003e+08 | 7.010010e+08 | 7.010016e+08 | 7.010034e+08 | 360.0 | 285.916667 | ... | 420.0 | 438.0 | 360.0 | 829.070090 | 1160.899275 | 51.333333 | 298.553571 | 532.666667 | 973.815 | 13758.144414 | . 3 1.0 | 7.010016e+09 | NaN | 7.010016e+09 | 7.010016e+09 | 7.010016e+09 | 7.010016e+09 | 7.010016e+09 | 1.0 | 158.000000 | ... | 344.0 | 344.0 | 1.0 | 400.000000 | NaN | 400.000000 | 400.000000 | 400.000000 | 400.000 | 400.000000 | . 4 207.0 | 7.010015e+08 | 9.677252e+02 | 7.010000e+08 | 7.010008e+08 | 7.010014e+08 | 7.010024e+08 | 7.010034e+08 | 207.0 | 27.946860 | ... | 404.5 | 438.0 | 207.0 | 230.863205 | 281.807964 | 10.000000 | 87.500000 | 150.000000 | 251.500 | 2100.000000 | . 5 rows × 40 columns . 在分群的數據描述中，我們發現有一個分群 (Cluster=3)，其資料筆數僅有 1 筆 . df_cluster_selected[df_cluster_selected[&#39;Cluster&#39;]==3] . invoice_cus_id recency frequency T monetary_value Cluster . 2613 7.010016e+09 | 158.0 | 2.0 | 344.0 | 400.0 | 3 | . 在這裡，我們先不對這筆資料進行排除或清理，僅先將分群後的數據與之前計算CLV的數據進行整併： . clv_merge = summary.copy() clv_merge = clv_merge.rename_axis(&#39;cus_id&#39;).reset_index() . df_cluster_selected_merge = df_cluster_selected.copy() df_cluster_selected_merge = df_cluster_selected.rename_axis(&#39;cus_id&#39;).reset_index() df_cluster_selected_merge.drop(columns=[&quot;frequency&quot;, &quot;recency&quot;,&quot;monetary_value&quot;,&quot;T&quot;],inplace=True) . 注意：因為進行分群計算的基礎數據已經經過特徵標準化處理，所以我們不需要將這些資料整併回 CLV 數據中 . clv_final = pd.merge(clv_merge,df_cluster_selected_merge,on=&#39;cus_id&#39;) clv_final.head(10) . cus_id invoice_cus_id_x frequency recency T monetary_value probability_alive pred_num_txn exp_avg_sales predicted_clv manual_predicted_clv invoice_cus_id_y Cluster . 0 0 | 201000002.0 | 122.0 | 434.0 | 438.0 | 527.270492 | 9.907499e-01 | 7.89 | 527.491842 | 4.120102e+03 | 4161.910633 | 201000002.0 | 2 | . 1 1 | 701000001.0 | 181.0 | 437.0 | 438.0 | 1279.011050 | 9.968217e-01 | 11.77 | 1279.289134 | 1.490641e+04 | 15057.233110 | 701000001.0 | 2 | . 2 2 | 701000002.0 | 136.0 | 387.0 | 438.0 | 1092.125000 | 3.230038e-05 | 0.00 | 1092.452468 | 3.100526e-01 | 0.000000 | 701000002.0 | 2 | . 3 3 | 701000003.0 | 81.0 | 294.0 | 427.0 | 480.765432 | 5.699000e-11 | 0.00 | 481.081044 | 1.471555e-07 | 0.000000 | 701000003.0 | 2 | . 4 4 | 701000004.0 | 9.0 | 125.0 | 435.0 | 853.222222 | 5.512930e-04 | 0.00 | 857.360073 | 2.830461e-01 | 0.000000 | 701000004.0 | 2 | . 5 5 | 701000006.0 | 4.0 | 302.0 | 401.0 | 165.000000 | 7.811945e-01 | 0.23 | 168.969907 | 3.909935e+01 | 38.863079 | 701000006.0 | 2 | . 6 7 | 701000009.0 | 4.0 | 305.0 | 425.0 | 972.500000 | 7.458372e-01 | 0.21 | 982.783147 | 2.054615e+02 | 206.384461 | 701000009.0 | 2 | . 7 8 | 701000010.0 | 367.0 | 438.0 | 438.0 | 13758.144414 | 9.989449e-01 | 23.90 | 13759.336769 | 3.255674e+05 | 328848.148788 | 701000010.0 | 2 | . 8 9 | 701000011.0 | 1.0 | 88.0 | 438.0 | 6350.000000 | 3.652369e-01 | 0.03 | 6564.330322 | 1.888321e+02 | 196.929910 | 701000011.0 | 1 | . 9 10 | 701000012.0 | 9.0 | 341.0 | 438.0 | 275.555556 | 7.249446e-01 | 0.44 | 277.694823 | 1.197687e+02 | 122.185722 | 701000012.0 | 2 | .",
            "url": "https://impepper.github.io/myPortfolio/python/prediction/jupyter/customer%20lifetime%20value/sales/clustering/k-means/2022/04/11/Customer-Lifetime-Value-(CLV).html",
            "relUrl": "/python/prediction/jupyter/customer%20lifetime%20value/sales/clustering/k-means/2022/04/11/Customer-Lifetime-Value-(CLV).html",
            "date": " • Apr 11, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "股價預測",
            "content": "&#32929;&#20729;&#38928;&#28204;&#26159;&#27231;&#22120;&#23416;&#32722;&#22312;&#37329;&#34701;&#38936;&#22495;&#26368;&#37325;&#35201;&#30340;&#25033;&#29992;&#20043;&#19968;&#12290;&#22312;&#36889;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#23559;&#22039;&#35430;&#36879;&#36942;&#20351;&#29992; Python &#30340;&#32218;&#24615;&#22238;&#27512;&#27169;&#22411;&#20358;&#36914;&#34892;&#32929;&#31080;&#20729;&#26684;&#30340;&#38928;&#28204;&#12290; . Stock Price Prediction . 預測股市的價格，一直是投資者的終極目標。 在每天數以億計的交易之中，每筆交易，都代表者投資者對於該股票的價格預期，並且期望透過交易獲利。 . 也因此，股票漲跌，便取決於投資者在交易市場中的投資行為。 如果投資者能夠準確預測市場動向，便有機會創造誘人的財富。 . 如果您具有股票市場的投資經驗以及機器學習的量化數據分析技能，對於您進行股票價格預測將會有明顯的助益。 . 我們來看看如何使用 python 來預測股票價格。 . 首先，我們先導入此項專案所需的所有必要 python 函式庫： . import numpy as np import pandas as pd from sklearn import preprocessing from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression . &#36039;&#26009;&#28310;&#20633; Data Preparation . 在這裡，我們先建立一個子程式，協助我們將等會讀取進來的資料進行切割： . def prepare_data(df,forecast_col,forecast_out,test_size): label = df[forecast_col].shift(-forecast_out) #建立一個新的序列，裡面的是5 {%forecast_out%}天後的股價（收盤價） X = np.array(df[[forecast_col]]) #建立序列 X = preprocessing.scale(X) #將訓練資料進行特徵標準化(normalization)工作 X_lately = X[-forecast_out:] #建立等會用來進行預測的資料(資料集中最後5 {%forecast_out%}天的股價（收盤價）) X = X[:-forecast_out] # 將資料集中最後的5{%forecast_out%}筆資料移除掉，亦即移除掉等下要作為預測用的資料 label.dropna(inplace=True) #通過**dropna()**過濾掉缺失的數據（亦即移除掉最後5筆數列） y = np.array(label) X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=test_size, random_state=0) #切割數據集 response = [X_train,X_test , Y_train, Y_test , X_lately] return response . 為什麼要將資料進行切割呢？這是因為在機器學習的領域之中，資料將會分成以下4類資料來進行資料分析以及驗證： . 訓練資料 &lt;- 在訓練模型時所依據的基礎資料 | 訓練資料所對應的歷史結果 &lt;- 在訓練模型時，透過基礎資料與歷史結果的關係，來建立數據模型 | 測試資料 &lt;- 在數據模型建立之後，作為模型驗證實的基礎資料 | 測試資料所對應的歷史結果 &lt;- 在訓練資料透過數據模模演算後的結過（預測），與相對應的歷史結果進行比較，以了解數據麼型的準確程度 | (上述的4類資料，都是來自於歷史資料) . 接下來，我們開始讀取要進行分析的股價資料（以TESLA股價為例） . df = pd.read_csv(&quot;datasource/TSLA.csv&quot;) . 現在，我準備建立三個輸入變量供上面創建的子程式使用。包含了我們想要預測的資料欄位、訓練及驗證資料的分割比例、以及我們預計預測的天數。 . forecast_col = &#39;Close&#39; # 在這個練習中，我選擇收盤價欄位作為訓練模型以及預測的欄位 forecast_out = 5 # 在這個練習中，我規劃預測 5 天後的TESLA的收盤價 test_size = 0.2 # 在這個練習中，我預計用來進行模型驗證的資料比例（在大多數的案例中，會採用20%。也就是0.2作為模型驗證之用） . &#24314;&#31435;&#20006;&#25033;&#29992;&#32929;&#20729;&#38928;&#28204;&#27169;&#22411; . 現在，我們開始將匯入的資料進行切割，並據此開始訓練線性回歸模型： . X_train, X_test, Y_train, Y_test , X_lately =prepare_data(df,forecast_col,forecast_out,test_size); learner = LinearRegression() #定義以及初始化線性回歸模型 learner.fit(X_train,Y_train) #訓練線性回歸模型 . LinearRegression() . 到這裡，模型已經訓練並且建立完畢，我們可以快速地了解模型準確度並開始進行預測！ . score=learner.score(X_test,Y_test) # 了解一下這個模型的準確程度（透過驗證資料比對） forecast= learner.predict(X_lately) # 提供基礎資料進行模型演算餅取得預測結果 （5 天後的TESLA的收盤價） response={} # creting json object response[&#39;模型分數&#39;]=score response[&#39;預測價格&#39;]=forecast print(response) . {&#39;模型分數&#39;: 0.7901258449593493, &#39;預測價格&#39;: array([738.10424235, 739.67504036, 747.39116981, 741.79828889, 722.43892968])} . &#32317;&#32080; . 這就是我們如何學習透過 Python 和機器學習中的線性回歸模型來預測股票價格。 希望你喜歡這篇文章。 .",
            "url": "https://impepper.github.io/myPortfolio/python/prediction/jupyter/linear%20regression/stock%20price/2022/02/12/Stock-Price-Prediction.html",
            "relUrl": "/python/prediction/jupyter/linear%20regression/stock%20price/2022/02/12/Stock-Price-Prediction.html",
            "date": " • Feb 12, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "CV",
          "content": "EDUCATION . Gemological Institute of America | 東海大學 (Sep.1990 ~ Jun.1994) | . WORK EXPERIENCE . Chanel Inc. (Taiwan) . Jun. 2021 - Jan. 2022 . IT Manager - Client Solution &amp; Digital . One to support frontline the suitable ecosystems and find potential digital activaions accordingly. . To design and build the digital ecosystem architect on LINE/SFMC for CRM purpose and potential business activation ideas | To cooperate with regional colleagues launching newly activations | To utilize functional roles on each sub-systems | . Loreal Taiwan . May. 2017 - Jun. 2021 . Ditital Product Solution Manager . One to build OMNI ecosystems as turnkey and find potential buiness activaions accordingly. . To manage and build the digital ecosystem architect for potential business activation ideas | To define, build, and utilize functional roles on each sub-system | To find business opportunities though existed systems or implementing new technologies/ideas | Country key contact with Global/Region on GA/GTM Tagging | . Project Manager . To build/upgrading the online eCommerce website solution as the brand/local integrations needs | To monitor and enhance the online sales activity and performance | To ensure user experience and all eCommerce activities are consistent with the brand sense of purpose | Collect and connect working knowledge on the e-retailers | Optimizing SEO/Data Tracking Schema collecting owned data for further analyzing | Create synergies between inside and outside websites in data synchronizations | . Freelace . Jun. 2011 - May. 2017 . One to build and integrate digital systems for business innovations. . To build eCommerce website and desktop tools managing product data (external data sources) | To build mobile app and managing web consols | To build and maintain customer/product/sales systems for small business | . TechArt Center, Taipei National University of the Arts . Jan. 2007 - Jun. 2011 . Engineer . To support exhibition planning | Code rewriting | Exhibition space planning | To support annual projects and general affairs | .",
          "url": "https://impepper.github.io/myPortfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "Cutomer Lifetime Vlue calculation . Data analysis from pharmacies | Customer Lifetime Value calculation using Beta-Geometric / Negative Binomial Distribution (BG/NBD) model | Segmentation using Machine Learning | . OMO Ecosystem Architect . Define and build the digital ecosystem architect for the OMO market strategies . Consolidate existing sub-systems | Standardize data schema &amp; tracking tags | Define functional roles to meeting possible business models such as affiliation &amp; conversational / Social commerce | . RPA - Daily Sale Report . RPA to generate daily sale reports . Tools used: Katalon Studio, Selenium, Microsoft Power Automate | To scrape sales numbers from various endpotints and consolidate into existed excel file | Endpoints including: website backend consoles / FTP / Intranet Files | . Brand eCommerce websites . Brand e-com site buildings . Lead directly on 4 SFCC site buildings in LUXE, Lroeal | Utilized GA/GTM configurations on 4 brand websites per global guideline | Monitoring and improving site performance &amp; SEO | Setup 20+ A/B Tests | . QR APP . Content Management Solution for marketers delivering Campaign through QRCode . App alike UI/UX for QR Code viewers | Including APP and Website (Front-end) and Back-end developing | . Mobile APP - ProcessingLab . A simple simulator for artists using Processing language on iOS devices . Processing is a flexible software sketchbook and a language for learning how to code within the context of the visual arts. .",
          "url": "https://impepper.github.io/myPortfolio/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  
      ,"page7": {
          "title": "Skills",
          "content": "Certifications . Critical Thinking for More Effective Communication (2021) . | fierce CONVERSATIONS (2021) . | Python for Machine Learning and Data Science Bootcamp (2021) . | Social Media Marketing &amp; Digital Marketing Course (2020) . | . Skills . Software Deveopment . Desktop/WEB applications, Integrations (e.g. LINE Messaging Bot), API . Programing Language . Javascript, Python, VBA | . Web/Data analytics . Web Analytics . Google Analytics | Google Tag Manager | . Website Optimizing . SEO | AB Test (ABTasty / Google Optimize) | . Data Management . Data Managing/Engineering . SQL | Data processing | . Data Visualization . Google Data Studio | Microsoft Power BI | . Project Management . Robotic Process Automation (RPA) . Selenium | Microsoft Power Automate | Katalon Studio | AutoIt | UIPath | . Software Project Management . JIRA | Confluence | Airtable | .",
          "url": "https://impepper.github.io/myPortfolio/skill/",
          "relUrl": "/skill/",
          "date": ""
      }
      
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://impepper.github.io/myPortfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}