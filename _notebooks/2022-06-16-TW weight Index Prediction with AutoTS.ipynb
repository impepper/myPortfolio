{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2ca2e1",
   "metadata": {},
   "source": [
    "# 台灣加權股價指數預測 - 使用 AutoTS\n",
    "> 利用 Python 強大的時間序列分析的 AutoTS 函式庫進行台灣加權股價指數預測\n",
    "\n",
    "- toc: false\n",
    "- badges: false\n",
    "- comments: false\n",
    "- sticky_rank: 1\n",
    "- author: ChiHong Lin\n",
    "- categories: [python, prediction, jupyter, AutoTS, Stock Price]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e328e721",
   "metadata": {},
   "source": [
    "#### 股價預測是機器學習在金融領域最重要的應用之一。在這篇文章中，我將嘗試透過使用 Python 的線性回歸模型來進行股票價格的預測。\n",
    "\n",
    "預測股市的價格，一直是投資者的終極目標。 在每天數以億計的交易之中，每筆交易，都代表者投資者對於該股票的價格預期，並且期望透過交易獲利。\n",
    "\n",
    "也因此，股票漲跌，便取決於投資者在交易市場中的投資行為。 如果投資者能夠準確預測市場動向，便有機會創造誘人的財富。\n",
    "\n",
    "如果您具有股票市場的投資經驗以及機器學習的量化數據分析技能，對於您預測價格趨勢將會有明顯的助益。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5153eb7",
   "metadata": {},
   "source": [
    "## Python 函式庫 AutoTS\n",
    "\n",
    "AutoTS 是一個用於自動時間序列分析的 Python 函式庫。它是 autoML 的一部分，其目標是為初學者提供自動化的時間序列模型工具。\n",
    "\n",
    "時間序列問題無論是在銷量預測，天氣預測還是在股票預測等問題中都至關重要，而如今隨著機器學習等快速發展，已經出現了非常多時間序列建模相關的函式庫，AutoTS 融合了自動化機器學習技術，會先對數據進行預處理，從數據中刪除異常值，再通過學習尋找最佳值。Auto 只需使用一行代碼，就可以訓練多個時間序列模型，包括ARIMA、SARIMAX、FB Prophet、VAR，並得出效果最佳的模型。它具有以下幾個特性：\n",
    "\n",
    "1. 它可用於查找最佳時間序列預測模型，具體取決於您使用的數據類型。\n",
    "2. 它可以處理單變量和多變量時間序列。\n",
    "3. 它還可以通過刪除和填充 NaN 值來處理混亂的數據，它還可以處理異常值。\n",
    "\n",
    "您可以將此庫用於時間序列預測的任何任務，例如預測未來 n 天的股票價格。\n",
    "\n",
    "AutoTS是一個非常不錯的時間序列函式庫，在碰到時間序列問題時，可以考慮使用AutoTS來進行訓練和預測，作為一個非常不錯的Baseline。\n",
    "\n",
    "### 參考連結\n",
    "\n",
    "- https://pypi.org/project/AutoTS/\n",
    "- https://github.com/winedarksea/AutoTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58044c97",
   "metadata": {},
   "source": [
    "## 台灣加權股價指數預測\n",
    "\n",
    "我將透過 [yfinance API](https://pypi.org/project/yfinance/) 從 [yfinance API](https://pypi.org/project/yfinance/) 取得台灣加權指數（代碼 **^TWII**）近3年的歷史價格數據，並藉由這些數據來進行分析及預測。\n",
    "\n",
    "我們首先先將需要的函式庫載入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf35e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81d420c",
   "metadata": {},
   "source": [
    "透過 [yfinance API](https://pypi.org/project/yfinance/) 從 [yfinance API](https://pypi.org/project/yfinance/) 取得台灣加權指數（代碼 **^TWII**）近3年的歷史價格數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32de3db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "\n",
    "d1 = today.strftime(\"%Y-%m-%d\")\n",
    "end_date = d1\n",
    "d2 = date.today() - timedelta(days=1095)\n",
    "d2 = d2.strftime(\"%Y-%m-%d\")\n",
    "start_date = d2\n",
    "\n",
    "data = yf.download('^TWII',\n",
    "                   start=start_date,\n",
    "                   end=end_date,\n",
    "                   progress=False)\n",
    "data[\"Date\"] = data.index\n",
    "data = data[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]]\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78091e59",
   "metadata": {},
   "source": [
    "初步了解取得的數據內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c270138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(730, 7)\n",
      "        Date          Open          High           Low         Close  \\\n",
      "0 2019-06-17  10488.700195  10562.969727  10474.190430  10530.540039   \n",
      "1 2019-06-18  10547.150391  10573.769531  10521.700195  10566.740234   \n",
      "2 2019-06-19  10650.480469  10778.639648  10650.480469  10775.339844   \n",
      "3 2019-06-20  10749.410156  10799.139648  10745.250000  10785.009766   \n",
      "4 2019-06-21  10817.709961  10840.290039  10773.469727  10803.769531   \n",
      "\n",
      "      Adj Close   Volume  \n",
      "0  10530.540039  1444800  \n",
      "1  10566.740234  1524200  \n",
      "2  10775.339844  2370300  \n",
      "3  10785.009766  2079800  \n",
      "4  10803.769531  2780700  \n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7f27d",
   "metadata": {},
   "source": [
    "我們繪製一下指數的走勢（以K線圖表示）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac36da5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "close": [
          10530.5400390625,
          10566.740234375,
          10775.33984375,
          10785.009765625,
          10803.76953125,
          10779.4501953125,
          10706.7197265625,
          10652.5498046875,
          10773.900390625,
          10730.830078125,
          10895.4599609375,
          10865.1201171875,
          10743.76953125,
          10775.900390625,
          10785.73046875,
          10751.2197265625,
          10702.7802734375,
          10798.48046875,
          10843.419921875,
          10824.349609375,
          10876.4296875,
          10886.0498046875,
          10828.48046875,
          10799.2802734375,
          10873.1904296875,
          10944.5302734375,
          10947.259765625,
          10935.759765625,
          10941.41015625,
          10891.98046875,
          10885.73046875,
          10830.900390625,
          10823.8095703125,
          10731.75,
          10549.0400390625,
          10423.41015625,
          10394.75,
          10386.1796875,
          10494.490234375,
          10472.3603515625,
          10362.66015625,
          10427.73046875,
          10327.1298828125,
          10420.8896484375,
          10488.75,
          10522.5,
          10525.7998046875,
          10529.7802734375,
          10538.1103515625,
          10354.5703125,
          10387.23046875,
          10434.2900390625,
          10462.4296875,
          10618.0498046875,
          10634.849609375,
          10558.2099609375,
          10657.3095703125,
          10756.9296875,
          10780.6396484375,
          10753.580078125,
          10790.349609375,
          10827.5498046875,
          10898.1298828125,
          10874.5,
          10929.4501953125,
          10894.7001953125,
          10929.6904296875,
          10919.01953125,
          10918.009765625,
          10873.6904296875,
          10871.990234375,
          10829.6796875,
          10967.650390625,
          10947.8798828125,
          10875.91015625,
          10894.48046875,
          10935.0595703125,
          11017.3095703125,
          10889.9599609375,
          11066.9501953125,
          11111.7998046875,
          11162.830078125,
          11186.8798828125,
          11180.2197265625,
          11184.150390625,
          11271.25,
          11239.669921875,
          11320.1396484375,
          11296.1201171875,
          11315.01953125,
          11333.8701171875,
          11380.2802734375,
          11358.7099609375,
          11399.5302734375,
          11556.849609375,
          11644.0302734375,
          11653.0703125,
          11606.5595703125,
          11579.5400390625,
          11427.2802734375,
          11520.3701171875,
          11467.830078125,
          11450.419921875,
          11525.599609375,
          11599.7802734375,
          11656.400390625,
          11631.2001953125,
          11558.26953125,
          11566.7998046875,
          11561.580078125,
          11576.8203125,
          11647.4599609375,
          11617.080078125,
          11489.5703125,
          11502.830078125,
          11531.580078125,
          11510.4697265625,
          11594.650390625,
          11609.6396484375,
          11660.76953125,
          11627.83984375,
          11700.76953125,
          11836.419921875,
          11927.73046875,
          11939.76953125,
          12097.009765625,
          12122.4501953125,
          12018.900390625,
          11959.080078125,
          12022.23046875,
          11976.3798828125,
          12008.1298828125,
          12001.009765625,
          12091.58984375,
          12053.3701171875,
          11997.1396484375,
          12100.48046875,
          12110.4296875,
          11953.3603515625,
          11880.3203125,
          11817.099609375,
          11970.6298828125,
          12024.650390625,
          12113.419921875,
          12179.8095703125,
          12091.8798828125,
          12066.9296875,
          12090.2900390625,
          12118.7099609375,
          11421.740234375,
          11495.099609375,
          11354.919921875,
          11555.919921875,
          11573.6201171875,
          11749.6796875,
          11612.8095703125,
          11574.0703125,
          11664.0400390625,
          11774.1904296875,
          11791.7802734375,
          11815.7001953125,
          11763.509765625,
          11648.98046875,
          11758.83984375,
          11725.08984375,
          11686.349609375,
          11534.8701171875,
          11540.23046875,
          11433.6201171875,
          11292.169921875,
          11170.4599609375,
          11327.7197265625,
          11392.349609375,
          11514.8203125,
          11321.8095703125,
          10977.6396484375,
          11003.5400390625,
          10893.75,
          10422.3203125,
          10128.8701171875,
          9717.76953125,
          9439.6298828125,
          9218.669921875,
          8681.33984375,
          9234.08984375,
          8890.0302734375,
          9285.6201171875,
          9644.75,
          9736.3603515625,
          9698.919921875,
          9629.4296875,
          9708.0595703125,
          9663.6298828125,
          9818.740234375,
          9996.3896484375,
          10137.4697265625,
          10119.4296875,
          10157.6103515625,
          10099.2197265625,
          10332.9404296875,
          10447.2099609375,
          10375.48046875,
          10597.0400390625,
          10586.7099609375,
          10288.419921875,
          10307.740234375,
          10366.509765625,
          10347.3603515625,
          10567.26953125,
          10616.0595703125,
          10772.2197265625,
          10992.1396484375,
          10720.48046875,
          10774.6103515625,
          10774.98046875,
          10842.919921875,
          10901.419921875,
          11013.259765625,
          10879.4697265625,
          10938.26953125,
          10780.8798828125,
          10814.919921875,
          10740.5498046875,
          10860.4404296875,
          10907.7998046875,
          11008.3095703125,
          10811.150390625,
          10871.1796875,
          10997.2099609375,
          11014.66015625,
          10944.1904296875,
          10942.16015625,
          11079.01953125,
          11127.9296875,
          11320.16015625,
          11393.23046875,
          11479.400390625,
          11610.3203125,
          11637.1103515625,
          11720.16015625,
          11535.76953125,
          11429.9404296875,
          11306.259765625,
          11511.6396484375,
          11534.58984375,
          11548.330078125,
          11549.8603515625,
          11572.9296875,
          11612.3603515625,
          11660.669921875,
          11542.6201171875,
          11621.240234375,
          11703.419921875,
          11805.1396484375,
          11909.16015625,
          12116.7001953125,
          12092.9697265625,
          12170.1904296875,
          12192.6904296875,
          12073.6796875,
          12211.5595703125,
          12209.009765625,
          12202.849609375,
          12157.740234375,
          12181.5595703125,
          12174.5400390625,
          12397.5498046875,
          12473.26953125,
          12413.0400390625,
          12304.0400390625,
          12588.2998046875,
          12586.73046875,
          12540.9697265625,
          12722.919921875,
          12664.7998046875,
          12513.0302734375,
          12709.919921875,
          12802.2998046875,
          12913.5,
          12828.8701171875,
          12894,
          12780.1904296875,
          12670.349609375,
          12763.1298828125,
          12795.4599609375,
          12956.1103515625,
          12872.1396484375,
          12778.6396484375,
          12362.6396484375,
          12607.83984375,
          12647.1298828125,
          12758.25,
          12833.2900390625,
          12797.3095703125,
          12728.849609375,
          12591.4501953125,
          12703.2802734375,
          12699.5,
          12757.9697265625,
          12637.9501953125,
          12601.400390625,
          12663.5595703125,
          12608.580078125,
          12691.75,
          12675.9501953125,
          12787.8203125,
          12845.650390625,
          12976.759765625,
          12872.740234375,
          12875.6201171875,
          12795.1201171875,
          12645.509765625,
          12583.8798828125,
          12264.3798828125,
          12232.91015625,
          12462.759765625,
          12467.73046875,
          12515.6103515625,
          12548.2802734375,
          12704.23046875,
          12746.3701171875,
          12887.1904296875,
          12955.91015625,
          12947.1298828125,
          12919.3095703125,
          12827.8203125,
          12750.3701171875,
          12908.33984375,
          12862.3701171875,
          12877.25,
          12917.0302734375,
          12898.8203125,
          12909.0302734375,
          12875.009765625,
          12793.75,
          12662.91015625,
          12546.33984375,
          12591.3095703125,
          12736.009765625,
          12867.900390625,
          12918.7998046875,
          12973.5302734375,
          13127.4697265625,
          13081.7197265625,
          13262.1904296875,
          13221.7802734375,
          13273.330078125,
          13551.830078125,
          13593.009765625,
          13773.2900390625,
          13722.4296875,
          13716.4404296875,
          13878.009765625,
          13807.1298828125,
          13738.830078125,
          13845.66015625,
          13867.08984375,
          13722.8896484375,
          13885.669921875,
          13989.1396484375,
          13977.08984375,
          14132.4404296875,
          14256.599609375,
          14360.400390625,
          14390.1396484375,
          14249.490234375,
          14261.6904296875,
          14211.0498046875,
          14068.51953125,
          14304.4599609375,
          14258.9296875,
          14249.9599609375,
          14384.9599609375,
          14177.4599609375,
          14223.08984375,
          14280.2802734375,
          14331.419921875,
          14483.0703125,
          14472.0498046875,
          14687.7001953125,
          14732.5302734375,
          14902.0302734375,
          15000.0302734375,
          14983.1298828125,
          15214,
          15463.9501953125,
          15557.2998046875,
          15500.7001953125,
          15769.98046875,
          15707.1904296875,
          15616.3896484375,
          15612,
          15877.3701171875,
          15806.1796875,
          16153.76953125,
          16019.0302734375,
          15946.5400390625,
          15658.849609375,
          15701.4501953125,
          15415.8798828125,
          15138.3095703125,
          15410.08984375,
          15760.0498046875,
          15771.3203125,
          15706.2197265625,
          15802.400390625,
          16362.2900390625,
          16424.509765625,
          16341.3798828125,
          16410.16015625,
          16443.400390625,
          16212.5302734375,
          16452.1796875,
          15953.7998046875,
          15946.8798828125,
          16211.73046875,
          15906.41015625,
          15855.23046875,
          15820.1103515625,
          15853.08984375,
          15911.669921875,
          16179.5595703125,
          16255.1796875,
          16249.330078125,
          16313.16015625,
          16215.8203125,
          16287.83984375,
          16070.240234375,
          16189.2197265625,
          16177.58984375,
          16032.1201171875,
          16060.1396484375,
          16305.8798828125,
          16475.970703125,
          16554.900390625,
          16431.130859375,
          16571.279296875,
          16815.359375,
          16926.439453125,
          16854.099609375,
          16859.69921875,
          16824.91015625,
          16865.970703125,
          17076.73046875,
          17158.810546875,
          17263.279296875,
          17323.869140625,
          17202.109375,
          17096.970703125,
          17300.26953125,
          17572.2890625,
          17595.900390625,
          17567.529296875,
          17566.66015625,
          17222.349609375,
          16933.779296875,
          16843.439453125,
          16994.359375,
          17285,
          17235.609375,
          16583.130859375,
          15902.3701171875,
          15670.099609375,
          15827.08984375,
          15353.8896484375,
          16145.98046875,
          16132.66015625,
          16042.3603515625,
          16302.0595703125,
          16338.2900390625,
          16595.669921875,
          16643.689453125,
          16601.609375,
          16870.859375,
          17068.4296875,
          17162.380859375,
          17165.0390625,
          17246.16015625,
          17147.41015625,
          17083.91015625,
          17076.2109375,
          16966.220703125,
          17159.220703125,
          17213.51953125,
          17371.2890625,
          17307.859375,
          17390.609375,
          17318.5390625,
          17062.98046875,
          17075.55078125,
          17336.7109375,
          17407.9609375,
          17502.990234375,
          17590.970703125,
          17598.189453125,
          17755.4609375,
          17713.939453125,
          17710.150390625,
          17919.330078125,
          17913.0703125,
          17850.689453125,
          17866.08984375,
          17661.48046875,
          17814.330078125,
          17847.51953125,
          17845.75,
          18034.189453125,
          17895.25,
          17789.25,
          17528.740234375,
          17458.7890625,
          17572.330078125,
          17572.919921875,
          17403.560546875,
          17269.869140625,
          17135.220703125,
          17402.810546875,
          17247.41015625,
          17503.279296875,
          17553.759765625,
          17623.890625,
          17603.119140625,
          17526.279296875,
          17485.150390625,
          17323.640625,
          17227.1796875,
          17219.939453125,
          16982.109375,
          16858.76953125,
          16661.359375,
          16826.26953125,
          16375.400390625,
          16341.9404296875,
          16741.83984375,
          16818.73046875,
          17045.859375,
          17066.9609375,
          17209.9296875,
          17396.51953125,
          17490.2890625,
          17473.990234375,
          17319.759765625,
          17516.919921875,
          17495.30078125,
          17428.869140625,
          17270.490234375,
          17304.330078125,
          17474.5703125,
          17446.310546875,
          17434.900390625,
          17354,
          17278.69921875,
          17276.7890625,
          16925.8203125,
          17078.220703125,
          17260.189453125,
          17313.76953125,
          17181.439453125,
          16855.4609375,
          16934.76953125,
          16570.890625,
          16408.349609375,
          16460.75,
          16393.16015625,
          16713.859375,
          16640.4296875,
          16462.83984375,
          16347.990234375,
          16387.279296875,
          16781.189453125,
          16705.4609375,
          16900.669921875,
          16887.8203125,
          16889.509765625,
          16888.740234375,
          16894.240234375,
          17034.33984375,
          17074.55078125,
          17041.630859375,
          16987.41015625,
          17068.240234375,
          17065.970703125,
          17122.16015625,
          17078.859375,
          17296.900390625,
          17415.30078125,
          17541.359375,
          17559.650390625,
          17452.51953125,
          17518.130859375,
          17634.470703125,
          17693.130859375,
          17764.0390625,
          17841.369140625,
          17818.310546875,
          17803.5390625,
          17666.119140625,
          17642.51953125,
          17654.189453125,
          17369.390625,
          17328.08984375,
          17427.759765625,
          17585.990234375,
          17724.880859375,
          17697.140625,
          17688.2109375,
          17796.919921875,
          17832.419921875,
          17914.119140625,
          17826.259765625,
          17767.599609375,
          17599.369140625,
          17660.099609375,
          17785.740234375,
          17812.58984375,
          17669.109375,
          17789.26953125,
          17826.830078125,
          17946.66015625,
          17961.640625,
          18048.939453125,
          18196.810546875,
          18248.279296875,
          18218.83984375,
          18270.509765625,
          18526.349609375,
          18499.9609375,
          18367.919921875,
          18169.759765625,
          18239.380859375,
          18288.2109375,
          18375.400390625,
          18436.9296875,
          18403.330078125,
          18525.439453125,
          18378.640625,
          18227.4609375,
          18218.279296875,
          17899.30078125,
          17989.0390625,
          17701.119140625,
          17674.400390625,
          17900.30078125,
          17966.560546875,
          18151.759765625,
          18338.05078125,
          18310.939453125,
          17997.669921875,
          17951.810546875,
          18231.470703125,
          18268.5703125,
          18232.349609375,
          18221.490234375,
          17969.2890625,
          18055.73046875,
          17594.55078125,
          17652.1796875,
          17898.25,
          17867.599609375,
          17934.400390625,
          17736.51953125,
          17178.689453125,
          16825.25,
          17015.359375,
          17433.19921875,
          17264.740234375,
          17263.0390625,
          16926.060546875,
          16940.830078125,
          17448.220703125,
          17456.51953125,
          17560.359375,
          17559.7109375,
          17731.369140625,
          17699.060546875,
          17676.94921875,
          17520.009765625,
          17548.66015625,
          17740.560546875,
          17693.470703125,
          17625.58984375,
          17522.5,
          17178.630859375,
          17284.5390625,
          17048.369140625,
          16990.91015625,
          17301.650390625,
          17245.650390625,
          17004.1796875,
          16898.869140625,
          16993.400390625,
          17148.880859375,
          17127.94921875,
          17025.08984375,
          16620.900390625,
          16644.7890625,
          16303.349609375,
          16419.380859375,
          16592.1796875,
          16498.900390625,
          16565.830078125,
          16696.119140625,
          16408.19921875,
          16048.919921875,
          16061.7001953125,
          16006.25,
          15616.6796875,
          15832.5400390625,
          15901.0400390625,
          16056.08984375,
          16296.8603515625,
          16020.3203125,
          16144.849609375,
          16156.41015625,
          15963.6298828125,
          16104.0302734375,
          15968.830078125,
          16266.2197265625,
          16610.619140625,
          16807.76953125,
          16675.08984375,
          16552.5703125,
          16605.9609375,
          16512.880859375,
          16670.509765625,
          16621.33984375,
          16460.119140625,
          16070.98046875,
          16047.3701171875,
          15999.25
         ],
         "high": [
          10562.9697265625,
          10573.76953125,
          10778.6396484375,
          10799.1396484375,
          10840.2900390625,
          10786.330078125,
          10787.6201171875,
          10690.98046875,
          10800.2900390625,
          10788.080078125,
          10914.849609375,
          10888.2099609375,
          10812.650390625,
          10791.2802734375,
          10812.3798828125,
          10781.1103515625,
          10733.5703125,
          10810.16015625,
          10856.509765625,
          10867.58984375,
          10880.5595703125,
          10899.83984375,
          10861.1103515625,
          10835.4296875,
          10919.9599609375,
          10949.759765625,
          10994.4404296875,
          10976.4599609375,
          10945.919921875,
          10907.4404296875,
          10905.740234375,
          10927.16015625,
          10835.7998046875,
          10773.25,
          10641.2802734375,
          10542.490234375,
          10425.599609375,
          10461.3701171875,
          10500.76953125,
          10514.0498046875,
          10436.6201171875,
          10518.01953125,
          10374.75,
          10456.3203125,
          10516.0498046875,
          10538.1201171875,
          10545.25,
          10596.41015625,
          10541.73046875,
          10453.400390625,
          10431.7802734375,
          10456.2998046875,
          10466.990234375,
          10618.0498046875,
          10634.849609375,
          10650.150390625,
          10670.4296875,
          10766.599609375,
          10803.0703125,
          10810.0302734375,
          10815.58984375,
          10862.6796875,
          10898.1298828125,
          10912.23046875,
          10971.5400390625,
          10954.1796875,
          10931.759765625,
          10948.1904296875,
          10947.5302734375,
          10885.669921875,
          10934.91015625,
          10921.169921875,
          10967.650390625,
          10965.2001953125,
          10888.400390625,
          10941.490234375,
          10954.4501953125,
          11020.509765625,
          10989.3896484375,
          11085.490234375,
          11126.4501953125,
          11180.1103515625,
          11186.8798828125,
          11237.3798828125,
          11188.1298828125,
          11271.25,
          11281.1201171875,
          11320.1396484375,
          11347.1796875,
          11341.0302734375,
          11373.9501953125,
          11380.2802734375,
          11428.419921875,
          11399.5302734375,
          11559.0703125,
          11644.0302734375,
          11668.2001953125,
          11647.4697265625,
          11642.41015625,
          11570.330078125,
          11520.3701171875,
          11509.3701171875,
          11498.25,
          11552.25,
          11599.7802734375,
          11656.400390625,
          11649.0703125,
          11573.83984375,
          11577.01953125,
          11604.75,
          11642.9599609375,
          11651.73046875,
          11654.0595703125,
          11623.58984375,
          11524.849609375,
          11531.580078125,
          11513.830078125,
          11604.3896484375,
          11657.650390625,
          11678.3603515625,
          11649.73046875,
          11700.76953125,
          11875.98046875,
          11990.7802734375,
          11975.73046875,
          12097.009765625,
          12125.900390625,
          12111.76953125,
          12028.2197265625,
          12022.23046875,
          12027.599609375,
          12032.3701171875,
          12030.66015625,
          12093.01953125,
          12114.8896484375,
          12047.75,
          12110.740234375,
          12197.6396484375,
          12040.080078125,
          11986.0302734375,
          11899.669921875,
          11992.5498046875,
          12038.2099609375,
          12113.419921875,
          12186.6201171875,
          12169.919921875,
          12075.5400390625,
          12117.5498046875,
          12151.419921875,
          11933.23046875,
          11594.2099609375,
          11365.900390625,
          11581.3603515625,
          11620.419921875,
          11749.6796875,
          11712.7099609375,
          11598.0302734375,
          11678.580078125,
          11801.51953125,
          11854.98046875,
          11840.7900390625,
          11775.5595703125,
          11717.099609375,
          11783.4296875,
          11827.830078125,
          11755.169921875,
          11615.2802734375,
          11567.08984375,
          11494.2197265625,
          11470.23046875,
          11282.0498046875,
          11390.240234375,
          11392.349609375,
          11525.3203125,
          11471.3603515625,
          11221.759765625,
          11032.4697265625,
          11088.23046875,
          10845.3896484375,
          10171.2001953125,
          10130.7998046875,
          9677.0703125,
          9509.6103515625,
          9085.2802734375,
          9264.419921875,
          9029.51953125,
          9415.6396484375,
          9722.3701171875,
          9739.73046875,
          9954.8603515625,
          9655.7001953125,
          9789.8095703125,
          9736,
          9818.740234375,
          10039.150390625,
          10149.759765625,
          10246.83984375,
          10165.1201171875,
          10179.400390625,
          10344.1298828125,
          10461.83984375,
          10425.2900390625,
          10710.150390625,
          10637.6396484375,
          10544.7998046875,
          10310.849609375,
          10462.7001953125,
          10386.9296875,
          10578.9599609375,
          10620.51953125,
          10794.830078125,
          11012.7802734375,
          10781.509765625,
          10833.5302734375,
          10828.73046875,
          10890.8701171875,
          10970.7001953125,
          11039.1103515625,
          10974.509765625,
          10938.26953125,
          10894.169921875,
          10904.009765625,
          10812.8701171875,
          10924.7900390625,
          10933.98046875,
          11021.66015625,
          10977.5498046875,
          10874.5400390625,
          11046.2197265625,
          11069.7197265625,
          11087.5302734375,
          10965.0302734375,
          11109.2900390625,
          11170.490234375,
          11330.51953125,
          11425.419921875,
          11482.3603515625,
          11631.8896484375,
          11640.4501953125,
          11740.900390625,
          11771.1201171875,
          11442.2998046875,
          11469.83984375,
          11542.25,
          11550.3203125,
          11567.419921875,
          11584.8798828125,
          11632.8798828125,
          11679.400390625,
          11701.23046875,
          11608.7998046875,
          11638,
          11736.3896484375,
          11805.2099609375,
          11933.58984375,
          12116.7001953125,
          12249.9501953125,
          12190.26953125,
          12273.4296875,
          12253.580078125,
          12216.240234375,
          12228.3095703125,
          12320.48046875,
          12220.9296875,
          12266.9296875,
          12221.330078125,
          12450.16015625,
          12486.9501953125,
          12429.759765625,
          12502.83984375,
          12686.3603515625,
          13031.7001953125,
          12660.8701171875,
          12769.4697265625,
          12733.48046875,
          12673.01953125,
          12709.919921875,
          12816.3896484375,
          12971.8701171875,
          12912.6298828125,
          12933.16015625,
          12906.009765625,
          12730.9599609375,
          12812.7998046875,
          12801.3095703125,
          12956.1103515625,
          12981.580078125,
          12950.1103515625,
          12764.75,
          12638.650390625,
          12702.9404296875,
          12814.5595703125,
          12833.2900390625,
          12960.6796875,
          12799.16015625,
          12831.3798828125,
          12703.2802734375,
          12802.6103515625,
          12857.7900390625,
          12655.669921875,
          12725.0498046875,
          12702.5498046875,
          12616.5703125,
          12708.6396484375,
          12701.7900390625,
          12793.0703125,
          12857.73046875,
          13021.6796875,
          12972.51953125,
          12927.8095703125,
          12952.3095703125,
          12786.91015625,
          12700.8896484375,
          12487.48046875,
          12385.8095703125,
          12462.759765625,
          12571.33984375,
          12568.6796875,
          12637.6201171875,
          12706.3896484375,
          12774.3603515625,
          12887.1904296875,
          12997.7900390625,
          12994.66015625,
          12960,
          12909.1103515625,
          12896.2998046875,
          12911.41015625,
          12917.75,
          12942.1201171875,
          12917.0302734375,
          12963.259765625,
          12971.580078125,
          12884.3203125,
          12879.98046875,
          12699.8203125,
          12656.2900390625,
          12594.33984375,
          12760.1396484375,
          12885.2802734375,
          12918.7998046875,
          12999.16015625,
          13149.900390625,
          13142.2197265625,
          13262.1904296875,
          13324.8603515625,
          13273.330078125,
          13551.830078125,
          13780.1201171875,
          13773.2900390625,
          13785.919921875,
          13726.2197265625,
          13921.16015625,
          13951.169921875,
          13893.2001953125,
          13856.759765625,
          13885.009765625,
          13969.3896484375,
          13885.669921875,
          13995.7001953125,
          14049.580078125,
          14149.5595703125,
          14306.8701171875,
          14367,
          14427.41015625,
          14319.3798828125,
          14353.169921875,
          14270.33984375,
          14256.5498046875,
          14339.5,
          14338.400390625,
          14329.1201171875,
          14384.9599609375,
          14411.9296875,
          14247.7900390625,
          14324.419921875,
          14400.830078125,
          14483.0703125,
          14547.0703125,
          14695.4404296875,
          14760.0595703125,
          14937.1298828125,
          15000.0302734375,
          15197.6796875,
          15270.400390625,
          15463.9501953125,
          15557.2998046875,
          15642.0302734375,
          15778.51953125,
          15760.150390625,
          16041.58984375,
          15676.4501953125,
          15928.1396484375,
          16004.3203125,
          16238.4599609375,
          16138.0400390625,
          16014.9599609375,
          16023.8701171875,
          15837.4697265625,
          15557.01953125,
          15603.41015625,
          15429.98046875,
          15838.150390625,
          15896.400390625,
          15801.3701171875,
          15938.08984375,
          16406.779296875,
          16517.73046875,
          16382.7001953125,
          16579.169921875,
          16467.75,
          16456.939453125,
          16474.05078125,
          16190.0400390625,
          16262.91015625,
          16211.73046875,
          16091.75,
          15934.4296875,
          16074.9697265625,
          15864.259765625,
          15986.0703125,
          16216.2197265625,
          16298.0302734375,
          16281.91015625,
          16340.66015625,
          16349.2099609375,
          16410.01953125,
          16186.4599609375,
          16235.6298828125,
          16351.3798828125,
          16125.5400390625,
          16146.580078125,
          16325.6201171875,
          16520.890625,
          16556.189453125,
          16550.19921875,
          16602.470703125,
          16816.33984375,
          16926.720703125,
          17016.130859375,
          16979.349609375,
          17041.369140625,
          16935.390625,
          17076.73046875,
          17158.810546875,
          17294.150390625,
          17323.869140625,
          17282.759765625,
          17428.150390625,
          17300.26953125,
          17572.2890625,
          17630.189453125,
          17628.810546875,
          17709.23046875,
          17546.939453125,
          17328.55078125,
          17052.19921875,
          17088.73046875,
          17285,
          17304.470703125,
          17137.189453125,
          16552.630859375,
          16031.9296875,
          16075.48046875,
          15719.740234375,
          16153.76953125,
          16273.98046875,
          16154.080078125,
          16431.140625,
          16397.58984375,
          16657.599609375,
          16706.2890625,
          16601.609375,
          16889.009765625,
          17113.330078125,
          17184.30078125,
          17274.66015625,
          17311.279296875,
          17225.060546875,
          17189.23046875,
          17181.51953125,
          17082.51953125,
          17159.220703125,
          17278.55078125,
          17371.2890625,
          17398.220703125,
          17390.609375,
          17417.5,
          17305.490234375,
          17270.470703125,
          17375.7890625,
          17439.630859375,
          17597.330078125,
          17595.509765625,
          17713.240234375,
          17797.26953125,
          17863.900390625,
          17795.880859375,
          17945.51953125,
          18008.369140625,
          17933.619140625,
          17935.109375,
          17778.119140625,
          17947.900390625,
          18018.0390625,
          17940.859375,
          18034.189453125,
          17926.380859375,
          17854.349609375,
          17724.919921875,
          17707.560546875,
          17667.380859375,
          17672.599609375,
          17637.359375,
          17459.349609375,
          17252.869140625,
          17402.810546875,
          17429.009765625,
          17503.279296875,
          17553.759765625,
          17636.109375,
          17643.970703125,
          17593.849609375,
          17510.08984375,
          17524.91015625,
          17327.19921875,
          17233.109375,
          17216.33984375,
          16983.58984375,
          16870.55078125,
          16826.26953125,
          16777.779296875,
          16507.109375,
          16785.2890625,
          16904.30078125,
          17045.859375,
          17201.419921875,
          17229.890625,
          17396.51953125,
          17490.2890625,
          17503.9296875,
          17523.16015625,
          17540.779296875,
          17633.669921875,
          17559.2109375,
          17447.580078125,
          17319.08984375,
          17474.5703125,
          17482.5703125,
          17529.470703125,
          17439.75,
          17411.369140625,
          17408.7109375,
          17196.7890625,
          17145.25,
          17273.58984375,
          17335.990234375,
          17286.890625,
          17127.859375,
          16994.2109375,
          16883,
          16680.2890625,
          16460.75,
          16568.2109375,
          16731.580078125,
          16771.7109375,
          16589.720703125,
          16543.330078125,
          16519.9609375,
          16781.189453125,
          16862.029296875,
          16916.720703125,
          16973.859375,
          17026.890625,
          16932.890625,
          16930.98046875,
          17067.150390625,
          17079.599609375,
          17104.859375,
          17073.970703125,
          17119.4296875,
          17237.08984375,
          17153.759765625,
          17219.650390625,
          17296.900390625,
          17415.30078125,
          17581.51953125,
          17575.439453125,
          17527.1796875,
          17602.05078125,
          17683.189453125,
          17708.099609375,
          17764.0390625,
          17841.369140625,
          17986.1796875,
          17857.73046875,
          17797.900390625,
          17722.890625,
          17724.5,
          17641.7890625,
          17415.630859375,
          17535.23046875,
          17626.2109375,
          17741.55078125,
          17781.140625,
          17763.419921875,
          17796.919921875,
          17988.880859375,
          17925.119140625,
          17893.2890625,
          17919.349609375,
          17754.380859375,
          17678.779296875,
          17814.3203125,
          17822.740234375,
          17812.810546875,
          17825.890625,
          17870.16015625,
          17960.990234375,
          18039.849609375,
          18099.779296875,
          18197.359375,
          18283.25,
          18291.25,
          18379.689453125,
          18526.349609375,
          18619.609375,
          18427.150390625,
          18444.119140625,
          18243.259765625,
          18293.529296875,
          18394.69921875,
          18459.73046875,
          18509,
          18535.419921875,
          18575.41015625,
          18359.140625,
          18292.140625,
          18113.939453125,
          18004.44921875,
          17890.7109375,
          17776.419921875,
          17900.30078125,
          18063.55078125,
          18168.599609375,
          18338.05078125,
          18310.939453125,
          18182.73046875,
          18085.91015625,
          18233.7890625,
          18330.630859375,
          18262.9609375,
          18253.099609375,
          18159.509765625,
          18109.279296875,
          17939.529296875,
          17737.3203125,
          17944.5,
          17918.98046875,
          18026.029296875,
          17845.009765625,
          17581.5703125,
          17113.669921875,
          17072.740234375,
          17478.4296875,
          17373.900390625,
          17363.0390625,
          17177.26953125,
          17085.76953125,
          17472.94921875,
          17472.380859375,
          17603.990234375,
          17573.2890625,
          17738.689453125,
          17711.169921875,
          17747.130859375,
          17527.150390625,
          17585.91015625,
          17770.0703125,
          17767.810546875,
          17657.759765625,
          17528.990234375,
          17438.599609375,
          17316.689453125,
          17316.349609375,
          17100.849609375,
          17333.2109375,
          17374.140625,
          17141.330078125,
          16999.359375,
          17106.259765625,
          17148.880859375,
          17213.66015625,
          17088.759765625,
          16912.30078125,
          16729.470703125,
          16427.3203125,
          16455.5703125,
          16663.66015625,
          16604.869140625,
          16617.060546875,
          16783.779296875,
          16491.109375,
          16345.83984375,
          16071.5,
          16081.150390625,
          15943.6201171875,
          15860.6904296875,
          16032.849609375,
          16085.849609375,
          16316.580078125,
          16111.490234375,
          16181.66015625,
          16249.4501953125,
          16219.7998046875,
          16169.1796875,
          16179.009765625,
          16266.2197265625,
          16610.619140625,
          16807.76953125,
          16811.0390625,
          16617.259765625,
          16654.119140625,
          16593.75,
          16702.990234375,
          16643.94921875,
          16581.810546875,
          16295.0703125,
          16067.7998046875,
          16106.75
         ],
         "low": [
          10474.1904296875,
          10521.7001953125,
          10650.48046875,
          10745.25,
          10773.4697265625,
          10695.490234375,
          10693.0400390625,
          10645.3701171875,
          10674.2001953125,
          10719.080078125,
          10821.2998046875,
          10843.6396484375,
          10720.66015625,
          10750.900390625,
          10756.669921875,
          10708.919921875,
          10680.849609375,
          10723.23046875,
          10799.650390625,
          10823.8896484375,
          10769.5703125,
          10865,
          10804.33984375,
          10792.26953125,
          10861.990234375,
          10909.150390625,
          10932.5302734375,
          10912.919921875,
          10879.400390625,
          10879.240234375,
          10863.330078125,
          10827.740234375,
          10754.0703125,
          10714.73046875,
          10524.6796875,
          10418.25,
          10180.0400390625,
          10366.5400390625,
          10383.6201171875,
          10440.76953125,
          10361.8896484375,
          10427.73046875,
          10287.759765625,
          10318.3701171875,
          10449.3896484375,
          10497.1298828125,
          10504.490234375,
          10515.7197265625,
          10503.0703125,
          10340.150390625,
          10368.9599609375,
          10397.599609375,
          10406.490234375,
          10504.2802734375,
          10589.419921875,
          10557.0302734375,
          10566.3203125,
          10714.26953125,
          10753.4501953125,
          10732.7802734375,
          10746.8701171875,
          10804.01953125,
          10822.6904296875,
          10866.4296875,
          10878.33984375,
          10874.66015625,
          10903.4501953125,
          10905.919921875,
          10886.6201171875,
          10825.849609375,
          10856.9599609375,
          10827.6103515625,
          10855.599609375,
          10927.3896484375,
          10809.2802734375,
          10854.1201171875,
          10919.4296875,
          10986.009765625,
          10889.9599609375,
          11004.740234375,
          11091.2900390625,
          11113.1796875,
          11131.4501953125,
          11154.8603515625,
          11147.8603515625,
          11218.25,
          11209.099609375,
          11254.58984375,
          11281.3701171875,
          11295.4404296875,
          11292.830078125,
          11306.9501953125,
          11358.7099609375,
          11335.509765625,
          11451.7197265625,
          11576.1796875,
          11601.7001953125,
          11552.080078125,
          11561.3095703125,
          11409.830078125,
          11459.6796875,
          11439.08984375,
          11424.76953125,
          11485.4599609375,
          11532.9697265625,
          11576.7900390625,
          11591.73046875,
          11478.4296875,
          11534.150390625,
          11559.330078125,
          11576.8203125,
          11590.8095703125,
          11604.6796875,
          11485.16015625,
          11454.3798828125,
          11460.0595703125,
          11457.4296875,
          11546.75,
          11577.830078125,
          11631.0302734375,
          11607.259765625,
          11622.580078125,
          11766.98046875,
          11913.01953125,
          11915.6103515625,
          11939.6201171875,
          12059.919921875,
          12008.16015625,
          11937.4501953125,
          11960.2099609375,
          11976.3798828125,
          11973.9501953125,
          11982.6396484375,
          12032.7197265625,
          12046.2197265625,
          11997.1396484375,
          12026.23046875,
          12023.599609375,
          11953.3603515625,
          11822.400390625,
          11777.4501953125,
          11889.4599609375,
          11959.0595703125,
          12037.240234375,
          12140.26953125,
          12048.009765625,
          12006.080078125,
          12055.91015625,
          12101.5498046875,
          11418.2197265625,
          11436.9501953125,
          11138.0302734375,
          11393.0302734375,
          11512.7099609375,
          11605.1103515625,
          11592.08984375,
          11423.33984375,
          11614.25,
          11693.400390625,
          11784.9404296875,
          11788.8701171875,
          11724.8203125,
          11642.9501953125,
          11654.16015625,
          11714.150390625,
          11661.6298828125,
          11512.080078125,
          11415.4697265625,
          11408.0595703125,
          11274.51953125,
          11049.849609375,
          11279.8603515625,
          11297.8095703125,
          11454.650390625,
          11310.3203125,
          10977.6396484375,
          10830.2197265625,
          10885.91015625,
          10359.669921875,
          9636.150390625,
          9717.76953125,
          9371.349609375,
          9218.669921875,
          8523.6298828125,
          8816.8603515625,
          8750.1396484375,
          9083.7802734375,
          9426.4296875,
          9565.01953125,
          9691.1396484375,
          9415.51953125,
          9630.2998046875,
          9663.6298828125,
          9651.51953125,
          9928.16015625,
          9984.66015625,
          10092.3603515625,
          10103.76953125,
          10080.650390625,
          10130.650390625,
          10366.76953125,
          10317.1298828125,
          10552.580078125,
          10542.9599609375,
          10278.9501953125,
          10140.08984375,
          10294.8798828125,
          10324.2001953125,
          10407.490234375,
          10537.7099609375,
          10656.400390625,
          10826.259765625,
          10658.5,
          10735.009765625,
          10708.099609375,
          10775.16015625,
          10883.2197265625,
          10942.1796875,
          10854.509765625,
          10828.9296875,
          10777.4599609375,
          10730.5498046875,
          10730.7001953125,
          10812.7900390625,
          10860.150390625,
          10933.2099609375,
          10804.5595703125,
          10719.25,
          10903.3203125,
          10972.25,
          10915.2998046875,
          10861.900390625,
          10971.400390625,
          11102.9697265625,
          11182.51953125,
          11341.580078125,
          11411.9404296875,
          11528.4404296875,
          11537.3798828125,
          11621.01953125,
          11516.58984375,
          11244.6396484375,
          11299.4501953125,
          11380.669921875,
          11482.3203125,
          11485.3896484375,
          11542.3701171875,
          11553.91015625,
          11530.849609375,
          11637.7900390625,
          11500.650390625,
          11563.599609375,
          11622.6396484375,
          11690.9501953125,
          11857.900390625,
          11941.83984375,
          12028.5703125,
          12083.83984375,
          12181.33984375,
          12030.2001953125,
          12109.6103515625,
          12143.91015625,
          12162.51953125,
          12107.0400390625,
          12144.6201171875,
          12065.7998046875,
          12242.3203125,
          12389.400390625,
          12347.8603515625,
          12266.5498046875,
          12586.91015625,
          12533.1904296875,
          12488.009765625,
          12616.830078125,
          12635.7099609375,
          12506.58984375,
          12577.8798828125,
          12739.990234375,
          12851.2001953125,
          12791.1796875,
          12786.7998046875,
          12780.1904296875,
          12625.5400390625,
          12717.1298828125,
          12679.1904296875,
          12800.6904296875,
          12840.25,
          12778.6396484375,
          12144.759765625,
          12462.51953125,
          12567.9501953125,
          12698.849609375,
          12734.6904296875,
          12786.4501953125,
          12674.5703125,
          12591.4501953125,
          12565.830078125,
          12646.4697265625,
          12732.3203125,
          12559.7802734375,
          12575.3603515625,
          12614.7197265625,
          12480.5,
          12650.400390625,
          12616.6796875,
          12680.41015625,
          12773.23046875,
          12948.7197265625,
          12851.51953125,
          12841.08984375,
          12795.08984375,
          12632.740234375,
          12548.0703125,
          12264.3798828125,
          12149.8095703125,
          12282.3603515625,
          12429.7197265625,
          12466.580078125,
          12519.1201171875,
          12644.08984375,
          12619.8095703125,
          12818.009765625,
          12898.8896484375,
          12857.099609375,
          12895.7900390625,
          12786.259765625,
          12750.3701171875,
          12803.3203125,
          12862.3701171875,
          12846.419921875,
          12827.400390625,
          12875.23046875,
          12894.1796875,
          12821.0703125,
          12761.330078125,
          12583.3701171875,
          12546.33984375,
          12480.66015625,
          12641.2802734375,
          12736.01953125,
          12840.400390625,
          12927.2998046875,
          13048.669921875,
          13022.919921875,
          13067.0400390625,
          13193.740234375,
          13170.1201171875,
          13356.740234375,
          13593.009765625,
          13608.7197265625,
          13700.0400390625,
          13666.0703125,
          13793.5498046875,
          13798.3203125,
          13731.8095703125,
          13763.7900390625,
          13811.8095703125,
          13722.8896484375,
          13749.7099609375,
          13894.7001953125,
          13940.0703125,
          14010.169921875,
          14142.01953125,
          14184.580078125,
          14336.6904296875,
          14191.849609375,
          14081.419921875,
          14182.25,
          14053.349609375,
          14145.240234375,
          14213.490234375,
          14245.599609375,
          14166.8896484375,
          14175.6904296875,
          14134.8701171875,
          14256.2900390625,
          14296.9697265625,
          14363.4501953125,
          14435.0302734375,
          14476.6904296875,
          14646.330078125,
          14720.25,
          14861.990234375,
          14837,
          15049.8603515625,
          15275.3798828125,
          15395.73046875,
          15421.23046875,
          15550.16015625,
          15620.9599609375,
          15615.1103515625,
          15320.9697265625,
          15716.6396484375,
          15745.48046875,
          15775.73046875,
          15973.01953125,
          15772.6298828125,
          15589.2099609375,
          15642.1201171875,
          15367.4501953125,
          15138.3095703125,
          15089.9599609375,
          15546.6904296875,
          15741.16015625,
          15606.740234375,
          15774.33984375,
          16197.4501953125,
          16323.2197265625,
          16211.25,
          16410.16015625,
          16211.75,
          16212.5302734375,
          16322.26953125,
          15953.7998046875,
          15946.8798828125,
          15884.5498046875,
          15840.990234375,
          15636.4296875,
          15816.5,
          15657.919921875,
          15857.3203125,
          15947.2099609375,
          16166.349609375,
          16194.91015625,
          16244.990234375,
          16166.349609375,
          16264.6103515625,
          16022.169921875,
          15983.76953125,
          16166.2900390625,
          15967.9501953125,
          15944.9599609375,
          16140.3701171875,
          16411.669921875,
          16438.400390625,
          16427.19921875,
          16496.16015625,
          16715.3203125,
          16735.779296875,
          16815.58984375,
          16793.5390625,
          16802.220703125,
          16559.5703125,
          16851.060546875,
          16998.91015625,
          17135.51953125,
          17175.01953125,
          17167.390625,
          17066.169921875,
          17055.48046875,
          17378.359375,
          17489.7109375,
          17497.740234375,
          17548.30078125,
          17222.349609375,
          16647.609375,
          16843.439453125,
          16764.7109375,
          17032.23046875,
          17188.150390625,
          16460.869140625,
          15165.26953125,
          15368.5400390625,
          15702.099609375,
          15159.8603515625,
          15564.9599609375,
          16009.759765625,
          15943.4501953125,
          16136.8603515625,
          16136.9697265625,
          16444.75,
          16523.23046875,
          16419.419921875,
          16690.0390625,
          16939.91015625,
          17060.44921875,
          17056.419921875,
          17198.890625,
          17084.490234375,
          16775.849609375,
          17056.529296875,
          16907.4296875,
          16978.009765625,
          17193.609375,
          17279.69921875,
          17275.01953125,
          17150.51953125,
          17318.5390625,
          17023.310546875,
          17075.55078125,
          17127.58984375,
          17319.890625,
          17471.0703125,
          17481.23046875,
          17541.140625,
          17648.25,
          17644.359375,
          17676.98046875,
          17783.80078125,
          17895.779296875,
          17756.8203125,
          17742.080078125,
          17597.4609375,
          17759.900390625,
          17786.759765625,
          17716.439453125,
          17878.779296875,
          17779.609375,
          17708.150390625,
          17500.689453125,
          17352.240234375,
          17482.810546875,
          17511.859375,
          17403.560546875,
          17264.5,
          16893.69921875,
          17190.48046875,
          17237.669921875,
          17231.220703125,
          17456.19921875,
          17557.1796875,
          17566.720703125,
          17469.6796875,
          17350.150390625,
          17305.060546875,
          17090.25,
          17139.2109375,
          16978.109375,
          16773.5703125,
          16657.630859375,
          16418.5390625,
          16375.400390625,
          16248.080078125,
          16459.130859375,
          16779.900390625,
          16821.080078125,
          16984.689453125,
          17000.259765625,
          17244.619140625,
          17207.5703125,
          17415.51953125,
          17319.759765625,
          17380.51953125,
          17461.0703125,
          17388.369140625,
          17167.080078125,
          17122.94921875,
          17270.279296875,
          17387.5703125,
          17424.5390625,
          17316.51953125,
          17254.099609375,
          17235.44921875,
          16838.580078125,
          16998.0703125,
          17130.740234375,
          17235.6796875,
          17113.470703125,
          16801.779296875,
          16767.19921875,
          16503.740234375,
          16380.0595703125,
          16162.169921875,
          16303.6298828125,
          16465.5703125,
          16605.490234375,
          16349.490234375,
          16328.4697265625,
          16347.8798828125,
          16426.759765625,
          16695.890625,
          16772.150390625,
          16855.810546875,
          16873.990234375,
          16800.490234375,
          16784.109375,
          16909.3203125,
          16973.16015625,
          16994.369140625,
          16920.6796875,
          17021.76953125,
          17026.630859375,
          17080.509765625,
          17061.4609375,
          17097.16015625,
          17279.4609375,
          17433.099609375,
          17489.869140625,
          17403.48046875,
          17479.55078125,
          17560.51953125,
          17629.80078125,
          17669.580078125,
          17748.2109375,
          17786.05078125,
          17790.640625,
          17650.0703125,
          17588.779296875,
          17609.509765625,
          17330.439453125,
          17167.240234375,
          17369.7890625,
          17374.58984375,
          17559.439453125,
          17670.3203125,
          17585.94921875,
          17642.3203125,
          17832.419921875,
          17807.119140625,
          17767.80078125,
          17767.599609375,
          17566.990234375,
          17556.869140625,
          17719.060546875,
          17718.26953125,
          17646.390625,
          17652.3203125,
          17799.720703125,
          17855.779296875,
          17953.5703125,
          17975.41015625,
          18099.7109375,
          18192.859375,
          18216.44921875,
          18238.470703125,
          18395.140625,
          18446.51953125,
          18253.8203125,
          18134.41015625,
          18043.970703125,
          18135.4296875,
          18255.380859375,
          18346.810546875,
          18213.439453125,
          18435.01953125,
          18378.640625,
          18199.349609375,
          18125.19921875,
          17851.390625,
          17682.330078125,
          17645.66015625,
          17633.029296875,
          17712.349609375,
          17955.94921875,
          18039.23046875,
          18145.0390625,
          18191.75,
          17965.220703125,
          17942.6796875,
          18109.169921875,
          18190.240234375,
          18098.51953125,
          18129.650390625,
          17840.380859375,
          17954.75,
          17561.0703125,
          17554.970703125,
          17657.5,
          17784.640625,
          17906.240234375,
          17710.5390625,
          17135.6796875,
          16764.779296875,
          16944.080078125,
          17224.73046875,
          17239.900390625,
          17217.580078125,
          16911.94921875,
          16808.4296875,
          17172.810546875,
          17359.5,
          17503.94921875,
          17468.55078125,
          17581.619140625,
          17603.220703125,
          17633.859375,
          17368.66015625,
          17493.009765625,
          17572.900390625,
          17682.529296875,
          17465.609375,
          17381.0703125,
          17178.630859375,
          17210.400390625,
          17046.669921875,
          16905.130859375,
          17080.400390625,
          17245.650390625,
          17004.1796875,
          16845.509765625,
          16926.33984375,
          16983.55078125,
          17106.369140625,
          16923.869140625,
          16579.890625,
          16582.109375,
          16219.41015625,
          16256.8798828125,
          16521.939453125,
          16465.990234375,
          16514.30078125,
          16650.810546875,
          16312.169921875,
          16048.919921875,
          15734.4404296875,
          15953.26953125,
          15616.6796875,
          15687.150390625,
          15847.4599609375,
          15915.9296875,
          16172.7998046875,
          15892.73046875,
          16058.2001953125,
          16125.4501953125,
          15963.6298828125,
          15980.900390625,
          15949.6103515625,
          16075.91015625,
          16368.9296875,
          16493.0703125,
          16649.91015625,
          16540.55078125,
          16509.009765625,
          16465.869140625,
          16538.669921875,
          16557.5390625,
          16403.330078125,
          16055.8896484375,
          15869.0595703125,
          15981.5595703125
         ],
         "open": [
          10488.7001953125,
          10547.150390625,
          10650.48046875,
          10749.41015625,
          10817.7099609375,
          10734.25,
          10777.1796875,
          10661.3896484375,
          10674.2001953125,
          10786.66015625,
          10821.2998046875,
          10878.009765625,
          10793.099609375,
          10755.8701171875,
          10785.849609375,
          10742.8095703125,
          10729.830078125,
          10723.23046875,
          10817.599609375,
          10855.16015625,
          10819.919921875,
          10865,
          10861.1103515625,
          10821.919921875,
          10861.990234375,
          10910.5,
          10963.8603515625,
          10969.740234375,
          10892.25,
          10898.25,
          10872.6298828125,
          10909.98046875,
          10824.150390625,
          10773.0595703125,
          10641.2802734375,
          10528.8701171875,
          10304.8798828125,
          10422.8896484375,
          10383.6201171875,
          10491.7900390625,
          10436.6201171875,
          10444.5,
          10374.75,
          10345.419921875,
          10449.3896484375,
          10508.419921875,
          10526.8701171875,
          10560.009765625,
          10526.150390625,
          10453.400390625,
          10386.2099609375,
          10397.599609375,
          10440.150390625,
          10504.2802734375,
          10613.1904296875,
          10629.2197265625,
          10566.3203125,
          10718.400390625,
          10783.4599609375,
          10791.9501953125,
          10787.650390625,
          10830.25,
          10822.6904296875,
          10908.849609375,
          10887.419921875,
          10947.6201171875,
          10908.099609375,
          10924.8095703125,
          10917.4404296875,
          10885.669921875,
          10909.5703125,
          10904,
          10855.599609375,
          10945.919921875,
          10866.5,
          10907.6904296875,
          10932.240234375,
          10986.009765625,
          10976.9296875,
          11004.740234375,
          11103.5,
          11166.169921875,
          11156.1396484375,
          11190.2099609375,
          11166.240234375,
          11221.1904296875,
          11266.1796875,
          11266.83984375,
          11336.6396484375,
          11321.990234375,
          11347.3896484375,
          11336.9697265625,
          11393.6796875,
          11373.7001953125,
          11451.7197265625,
          11576.1796875,
          11645.080078125,
          11642.91015625,
          11620.8896484375,
          11570.330078125,
          11463.330078125,
          11483.240234375,
          11488.740234375,
          11485.4599609375,
          11543.7998046875,
          11608.48046875,
          11645.73046875,
          11573.83984375,
          11556.75,
          11590.8095703125,
          11608.3701171875,
          11590.8095703125,
          11641.7001953125,
          11616.8798828125,
          11509.9404296875,
          11473.3203125,
          11511.8203125,
          11546.75,
          11639.7998046875,
          11635.4697265625,
          11647.7802734375,
          11635.0703125,
          11766.98046875,
          11937.900390625,
          11915.6103515625,
          11939.6201171875,
          12061.099609375,
          12082.98046875,
          12001.5595703125,
          11969.1396484375,
          12023.7900390625,
          11978.9404296875,
          12018.3798828125,
          12032.7197265625,
          12094.8095703125,
          12019.1904296875,
          12026.5,
          12167.4404296875,
          12035.7099609375,
          11961.9697265625,
          11818.759765625,
          11889.4599609375,
          12009.26953125,
          12069.6103515625,
          12161.73046875,
          12169.919921875,
          12006.080078125,
          12080.7197265625,
          12107.5595703125,
          11933.23046875,
          11494.0302734375,
          11365.900390625,
          11399.4599609375,
          11601.5498046875,
          11605.1103515625,
          11712.7099609375,
          11514.7197265625,
          11614.25,
          11693.400390625,
          11813.5,
          11806.51953125,
          11770.2998046875,
          11700.91015625,
          11655.3701171875,
          11785.7802734375,
          11721.16015625,
          11615.2802734375,
          11506.740234375,
          11468.599609375,
          11436.9599609375,
          11184.66015625,
          11279.8603515625,
          11368.25,
          11454.650390625,
          11471.3603515625,
          11221.759765625,
          10907.6201171875,
          11022.8203125,
          10845.3896484375,
          10091.0302734375,
          10069.419921875,
          9538.6396484375,
          9453.98046875,
          9085.2802734375,
          8816.8603515625,
          9025.5498046875,
          9083.7802734375,
          9426.4296875,
          9667.1396484375,
          9807.900390625,
          9571.2197265625,
          9689.6201171875,
          9726.2001953125,
          9707.75,
          9928.16015625,
          10010.6396484375,
          10173.259765625,
          10105.4296875,
          10147.5595703125,
          10130.650390625,
          10366.76953125,
          10385.7802734375,
          10554.5498046875,
          10612.4501953125,
          10544.7998046875,
          10256.1103515625,
          10370.1298828125,
          10367.9697265625,
          10407.490234375,
          10580.25,
          10656.400390625,
          10826.259765625,
          10781.509765625,
          10756.9501953125,
          10771.08984375,
          10778.5302734375,
          10883.2197265625,
          10942.1796875,
          10974.509765625,
          10861.009765625,
          10894.169921875,
          10833.16015625,
          10753.2099609375,
          10812.7900390625,
          10892.759765625,
          10933.2099609375,
          10977.5498046875,
          10812.9404296875,
          10903.3203125,
          11023.9296875,
          11040.6201171875,
          10921.16015625,
          10971.400390625,
          11109.26953125,
          11182.51953125,
          11373.9404296875,
          11418.009765625,
          11539,
          11600.6103515625,
          11635.990234375,
          11738.490234375,
          11406.5400390625,
          11436.2802734375,
          11380.669921875,
          11525.5,
          11534.2001953125,
          11557.26953125,
          11553.91015625,
          11618.419921875,
          11647.8603515625,
          11608.7998046875,
          11563.599609375,
          11622.6396484375,
          11694.099609375,
          11857.900390625,
          11941.83984375,
          12176.7900390625,
          12086.1298828125,
          12222.900390625,
          12250.4404296875,
          12109.6103515625,
          12202.8896484375,
          12233.990234375,
          12173.0400390625,
          12195.7197265625,
          12205.25,
          12242.3203125,
          12389.759765625,
          12423.01953125,
          12467.4296875,
          12618.6904296875,
          12951.7197265625,
          12530.740234375,
          12691.8603515625,
          12653.4599609375,
          12642.73046875,
          12577.8798828125,
          12762.33984375,
          12894.240234375,
          12901.4296875,
          12786.7998046875,
          12856.4599609375,
          12713.6298828125,
          12758.8701171875,
          12709.9697265625,
          12813.9404296875,
          12974.9599609375,
          12904.83984375,
          12764.75,
          12462.51953125,
          12629.7802734375,
          12706.400390625,
          12768.9599609375,
          12888.650390625,
          12759.2998046875,
          12763.4404296875,
          12617.3896484375,
          12772.4296875,
          12774.9697265625,
          12645.919921875,
          12645.9296875,
          12637.419921875,
          12592.1796875,
          12665.740234375,
          12697.4599609375,
          12680.41015625,
          12787.419921875,
          12989.8095703125,
          12920.150390625,
          12858.599609375,
          12874.6103515625,
          12786.91015625,
          12656.2998046875,
          12487.48046875,
          12312.9404296875,
          12282.3603515625,
          12488.099609375,
          12483.650390625,
          12567.8798828125,
          12644.08984375,
          12667.509765625,
          12832.5400390625,
          12959.150390625,
          12988.4404296875,
          12927.08984375,
          12892.2900390625,
          12814.16015625,
          12803.3203125,
          12889.990234375,
          12889.8798828125,
          12854.9697265625,
          12945.1201171875,
          12923.5498046875,
          12867.76953125,
          12853.759765625,
          12687.6396484375,
          12651.349609375,
          12565.3203125,
          12641.2802734375,
          12766.8603515625,
          12878.009765625,
          12952.490234375,
          13053.98046875,
          13090.650390625,
          13067.0400390625,
          13324.8603515625,
          13221.7998046875,
          13356.740234375,
          13723.76953125,
          13628.419921875,
          13775.259765625,
          13702.58984375,
          13793.5498046875,
          13918.83984375,
          13860.9501953125,
          13763.7900390625,
          13833.2099609375,
          13917.650390625,
          13810.5498046875,
          13970.509765625,
          13988.490234375,
          14010.169921875,
          14230,
          14251.9404296875,
          14336.6904296875,
          14295.150390625,
          14298.58984375,
          14238.919921875,
          14206.330078125,
          14145.240234375,
          14326.16015625,
          14268.1298828125,
          14273.7900390625,
          14348.1103515625,
          14180.7001953125,
          14256.2900390625,
          14306.98046875,
          14363.4501953125,
          14500.91015625,
          14485.9697265625,
          14704.5302734375,
          14720.25,
          14913.6396484375,
          15145.849609375,
          15059.51953125,
          15365.1298828125,
          15425.580078125,
          15549.5302734375,
          15550.16015625,
          15651.2001953125,
          15987.16015625,
          15676.4501953125,
          15716.6396484375,
          15934.849609375,
          15775.73046875,
          15984.8896484375,
          16006.2099609375,
          15955.16015625,
          15711.759765625,
          15519.849609375,
          15544.3603515625,
          15176.5595703125,
          15546.6904296875,
          15828.6396484375,
          15697.75,
          15805.759765625,
          16197.4501953125,
          16366.23046875,
          16376.91015625,
          16445.869140625,
          16320.8798828125,
          16329.5400390625,
          16376.7998046875,
          16190.0400390625,
          16127.8701171875,
          15992.3203125,
          16091.75,
          15759.580078125,
          15943.3701171875,
          15715.0703125,
          15921.4501953125,
          15947.2099609375,
          16241.5498046875,
          16256.580078125,
          16253.9296875,
          16312.98046875,
          16264.6103515625,
          16186.4599609375,
          16065.5,
          16250.2001953125,
          15994.9599609375,
          16010.5400390625,
          16140.3701171875,
          16411.669921875,
          16490.310546875,
          16529.23046875,
          16538.279296875,
          16771.349609375,
          16786.560546875,
          16974.279296875,
          16908.55078125,
          16872.400390625,
          16869.779296875,
          16851.060546875,
          17028.349609375,
          17141.25,
          17231.580078125,
          17282.759765625,
          17302.73046875,
          17117.490234375,
          17378.359375,
          17586.01953125,
          17610.720703125,
          17646.9296875,
          17505.51953125,
          17249.560546875,
          16968.75,
          16925.51953125,
          17032.23046875,
          17274.23046875,
          17137.189453125,
          16515.880859375,
          15668.330078125,
          15819.240234375,
          15554.990234375,
          15564.9599609375,
          16112.2099609375,
          16122.51953125,
          16136.8603515625,
          16246.51953125,
          16444.75,
          16645.169921875,
          16591.69921875,
          16690.0390625,
          16948.470703125,
          17098.490234375,
          17192.91015625,
          17201.30078125,
          17225.060546875,
          17176.08984375,
          17094.33984375,
          17082.51953125,
          17024.5390625,
          17200.44921875,
          17279.69921875,
          17357.369140625,
          17254.83984375,
          17384.740234375,
          17305.490234375,
          17162.7109375,
          17127.58984375,
          17358.5703125,
          17471.0703125,
          17531.8203125,
          17621.150390625,
          17648.25,
          17801.189453125,
          17745.689453125,
          17783.80078125,
          17929.220703125,
          17901.25,
          17880.900390625,
          17778.119140625,
          17843.30078125,
          17931.470703125,
          17892.509765625,
          17878.779296875,
          17839.580078125,
          17854.349609375,
          17724.919921875,
          17593.599609375,
          17482.810546875,
          17603.94921875,
          17554.9296875,
          17394.76953125,
          17252.869140625,
          17216.580078125,
          17414.76953125,
          17256.609375,
          17489.119140625,
          17588.640625,
          17624.8203125,
          17593.849609375,
          17505.009765625,
          17492.44921875,
          17308.619140625,
          17221.349609375,
          17216.33984375,
          16941.330078125,
          16841.619140625,
          16630.990234375,
          16777.779296875,
          16426.98046875,
          16459.130859375,
          16830.630859375,
          16821.080078125,
          17177.390625,
          17061.099609375,
          17244.619140625,
          17384.609375,
          17463.80078125,
          17455.919921875,
          17380.51953125,
          17534.05078125,
          17534.380859375,
          17411.529296875,
          17175.0390625,
          17270.279296875,
          17452.16015625,
          17463.779296875,
          17434.01953125,
          17332.33984375,
          17279.2890625,
          17196.7890625,
          16998.0703125,
          17130.740234375,
          17278.109375,
          17286.890625,
          17127.859375,
          16886.5703125,
          16883,
          16600.51953125,
          16362.26953125,
          16488.2109375,
          16465.5703125,
          16742.349609375,
          16589.720703125,
          16480.98046875,
          16392.509765625,
          16426.759765625,
          16816.939453125,
          16791.119140625,
          16936.19921875,
          16904.4609375,
          16900.3203125,
          16879.33984375,
          16909.3203125,
          17037.369140625,
          17081.169921875,
          17058.0390625,
          17021.76953125,
          17094.419921875,
          17087.890625,
          17158.279296875,
          17097.16015625,
          17323.390625,
          17433.099609375,
          17549.08984375,
          17527.1796875,
          17479.55078125,
          17560.51953125,
          17639.9609375,
          17705.2890625,
          17767.060546875,
          17864.189453125,
          17828.529296875,
          17797.900390625,
          17680.16015625,
          17686.119140625,
          17641.7890625,
          17320.009765625,
          17369.7890625,
          17428.609375,
          17575.91015625,
          17720.9296875,
          17650.669921875,
          17734.189453125,
          17845.060546875,
          17880.41015625,
          17893.2890625,
          17841.029296875,
          17754.380859375,
          17592.869140625,
          17719.060546875,
          17744.5390625,
          17812.810546875,
          17686.83984375,
          17805.740234375,
          17855.779296875,
          17966.349609375,
          17975.41015625,
          18099.7109375,
          18209.140625,
          18270,
          18260.23046875,
          18395.140625,
          18598.130859375,
          18395.720703125,
          18388.16015625,
          18095.390625,
          18266.5390625,
          18348.390625,
          18356.2109375,
          18509,
          18512.150390625,
          18492.810546875,
          18275.1796875,
          18212.259765625,
          18113.939453125,
          17843.330078125,
          17890.7109375,
          17657.970703125,
          17750.69921875,
          17955.94921875,
          18060.330078125,
          18217.2890625,
          18258.810546875,
          18182.73046875,
          17978.439453125,
          18109.169921875,
          18213.30078125,
          18250.369140625,
          18196.400390625,
          18159.509765625,
          17954.75,
          17939.529296875,
          17617.359375,
          17657.5,
          17864.109375,
          17932.150390625,
          17845.009765625,
          17581.5703125,
          17000,
          16944.080078125,
          17224.73046875,
          17373.8203125,
          17287.900390625,
          17177.26953125,
          17007.580078125,
          17172.810546875,
          17439.91015625,
          17503.94921875,
          17512.41015625,
          17581.619140625,
          17711.169921875,
          17695.80078125,
          17510.150390625,
          17568.3203125,
          17572.900390625,
          17754.099609375,
          17657.759765625,
          17484.189453125,
          17430.130859375,
          17210.400390625,
          17273.05078125,
          17005.83984375,
          17080.400390625,
          17353.25,
          17139.33984375,
          16958.66015625,
          16926.33984375,
          17057.130859375,
          17159.880859375,
          17088.759765625,
          16912.30078125,
          16677.83984375,
          16427.3203125,
          16350.7197265625,
          16599.83984375,
          16593.2109375,
          16531.369140625,
          16689.98046875,
          16491.109375,
          16345.83984375,
          15891.400390625,
          16053.75,
          15943.6201171875,
          15687.150390625,
          15943.6904296875,
          15915.9296875,
          16174.7802734375,
          16111.490234375,
          16061.919921875,
          16193.919921875,
          16166.5703125,
          15987.740234375,
          16128.2099609375,
          16075.91015625,
          16368.9296875,
          16561.6796875,
          16718.91015625,
          16605.4609375,
          16570.890625,
          16593.75,
          16538.669921875,
          16643.94921875,
          16581.810546875,
          16295.0703125,
          15975.169921875,
          16025.83984375
         ],
         "type": "candlestick",
         "x": [
          "2019-06-17T00:00:00",
          "2019-06-18T00:00:00",
          "2019-06-19T00:00:00",
          "2019-06-20T00:00:00",
          "2019-06-21T00:00:00",
          "2019-06-24T00:00:00",
          "2019-06-25T00:00:00",
          "2019-06-26T00:00:00",
          "2019-06-27T00:00:00",
          "2019-06-28T00:00:00",
          "2019-07-01T00:00:00",
          "2019-07-02T00:00:00",
          "2019-07-03T00:00:00",
          "2019-07-04T00:00:00",
          "2019-07-05T00:00:00",
          "2019-07-08T00:00:00",
          "2019-07-09T00:00:00",
          "2019-07-10T00:00:00",
          "2019-07-11T00:00:00",
          "2019-07-12T00:00:00",
          "2019-07-15T00:00:00",
          "2019-07-16T00:00:00",
          "2019-07-17T00:00:00",
          "2019-07-18T00:00:00",
          "2019-07-19T00:00:00",
          "2019-07-22T00:00:00",
          "2019-07-23T00:00:00",
          "2019-07-24T00:00:00",
          "2019-07-25T00:00:00",
          "2019-07-26T00:00:00",
          "2019-07-29T00:00:00",
          "2019-07-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-01T00:00:00",
          "2019-08-02T00:00:00",
          "2019-08-05T00:00:00",
          "2019-08-06T00:00:00",
          "2019-08-07T00:00:00",
          "2019-08-08T00:00:00",
          "2019-08-12T00:00:00",
          "2019-08-13T00:00:00",
          "2019-08-14T00:00:00",
          "2019-08-15T00:00:00",
          "2019-08-16T00:00:00",
          "2019-08-19T00:00:00",
          "2019-08-20T00:00:00",
          "2019-08-21T00:00:00",
          "2019-08-22T00:00:00",
          "2019-08-23T00:00:00",
          "2019-08-26T00:00:00",
          "2019-08-27T00:00:00",
          "2019-08-28T00:00:00",
          "2019-08-29T00:00:00",
          "2019-08-30T00:00:00",
          "2019-09-02T00:00:00",
          "2019-09-03T00:00:00",
          "2019-09-04T00:00:00",
          "2019-09-05T00:00:00",
          "2019-09-06T00:00:00",
          "2019-09-10T00:00:00",
          "2019-09-11T00:00:00",
          "2019-09-12T00:00:00",
          "2019-09-16T00:00:00",
          "2019-09-17T00:00:00",
          "2019-09-18T00:00:00",
          "2019-09-19T00:00:00",
          "2019-09-20T00:00:00",
          "2019-09-23T00:00:00",
          "2019-09-24T00:00:00",
          "2019-09-25T00:00:00",
          "2019-09-26T00:00:00",
          "2019-09-27T00:00:00",
          "2019-10-01T00:00:00",
          "2019-10-02T00:00:00",
          "2019-10-03T00:00:00",
          "2019-10-04T00:00:00",
          "2019-10-07T00:00:00",
          "2019-10-08T00:00:00",
          "2019-10-09T00:00:00",
          "2019-10-14T00:00:00",
          "2019-10-15T00:00:00",
          "2019-10-16T00:00:00",
          "2019-10-17T00:00:00",
          "2019-10-18T00:00:00",
          "2019-10-21T00:00:00",
          "2019-10-22T00:00:00",
          "2019-10-23T00:00:00",
          "2019-10-24T00:00:00",
          "2019-10-25T00:00:00",
          "2019-10-28T00:00:00",
          "2019-10-29T00:00:00",
          "2019-10-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-01T00:00:00",
          "2019-11-04T00:00:00",
          "2019-11-05T00:00:00",
          "2019-11-06T00:00:00",
          "2019-11-07T00:00:00",
          "2019-11-08T00:00:00",
          "2019-11-11T00:00:00",
          "2019-11-12T00:00:00",
          "2019-11-13T00:00:00",
          "2019-11-14T00:00:00",
          "2019-11-15T00:00:00",
          "2019-11-18T00:00:00",
          "2019-11-19T00:00:00",
          "2019-11-20T00:00:00",
          "2019-11-21T00:00:00",
          "2019-11-22T00:00:00",
          "2019-11-25T00:00:00",
          "2019-11-26T00:00:00",
          "2019-11-27T00:00:00",
          "2019-11-28T00:00:00",
          "2019-11-29T00:00:00",
          "2019-12-02T00:00:00",
          "2019-12-03T00:00:00",
          "2019-12-04T00:00:00",
          "2019-12-05T00:00:00",
          "2019-12-06T00:00:00",
          "2019-12-09T00:00:00",
          "2019-12-10T00:00:00",
          "2019-12-11T00:00:00",
          "2019-12-12T00:00:00",
          "2019-12-13T00:00:00",
          "2019-12-16T00:00:00",
          "2019-12-17T00:00:00",
          "2019-12-18T00:00:00",
          "2019-12-19T00:00:00",
          "2019-12-20T00:00:00",
          "2019-12-23T00:00:00",
          "2019-12-24T00:00:00",
          "2019-12-25T00:00:00",
          "2019-12-26T00:00:00",
          "2019-12-27T00:00:00",
          "2019-12-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-02T00:00:00",
          "2020-01-03T00:00:00",
          "2020-01-06T00:00:00",
          "2020-01-07T00:00:00",
          "2020-01-08T00:00:00",
          "2020-01-09T00:00:00",
          "2020-01-10T00:00:00",
          "2020-01-13T00:00:00",
          "2020-01-14T00:00:00",
          "2020-01-15T00:00:00",
          "2020-01-16T00:00:00",
          "2020-01-17T00:00:00",
          "2020-01-20T00:00:00",
          "2020-01-30T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-03T00:00:00",
          "2020-02-04T00:00:00",
          "2020-02-05T00:00:00",
          "2020-02-06T00:00:00",
          "2020-02-07T00:00:00",
          "2020-02-10T00:00:00",
          "2020-02-11T00:00:00",
          "2020-02-12T00:00:00",
          "2020-02-13T00:00:00",
          "2020-02-14T00:00:00",
          "2020-02-17T00:00:00",
          "2020-02-18T00:00:00",
          "2020-02-19T00:00:00",
          "2020-02-20T00:00:00",
          "2020-02-21T00:00:00",
          "2020-02-24T00:00:00",
          "2020-02-25T00:00:00",
          "2020-02-26T00:00:00",
          "2020-02-27T00:00:00",
          "2020-03-02T00:00:00",
          "2020-03-03T00:00:00",
          "2020-03-04T00:00:00",
          "2020-03-05T00:00:00",
          "2020-03-06T00:00:00",
          "2020-03-09T00:00:00",
          "2020-03-10T00:00:00",
          "2020-03-11T00:00:00",
          "2020-03-12T00:00:00",
          "2020-03-13T00:00:00",
          "2020-03-16T00:00:00",
          "2020-03-17T00:00:00",
          "2020-03-18T00:00:00",
          "2020-03-19T00:00:00",
          "2020-03-20T00:00:00",
          "2020-03-23T00:00:00",
          "2020-03-24T00:00:00",
          "2020-03-25T00:00:00",
          "2020-03-26T00:00:00",
          "2020-03-27T00:00:00",
          "2020-03-30T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-01T00:00:00",
          "2020-04-06T00:00:00",
          "2020-04-07T00:00:00",
          "2020-04-08T00:00:00",
          "2020-04-09T00:00:00",
          "2020-04-10T00:00:00",
          "2020-04-13T00:00:00",
          "2020-04-14T00:00:00",
          "2020-04-15T00:00:00",
          "2020-04-16T00:00:00",
          "2020-04-17T00:00:00",
          "2020-04-20T00:00:00",
          "2020-04-21T00:00:00",
          "2020-04-22T00:00:00",
          "2020-04-23T00:00:00",
          "2020-04-24T00:00:00",
          "2020-04-27T00:00:00",
          "2020-04-28T00:00:00",
          "2020-04-29T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-04T00:00:00",
          "2020-05-05T00:00:00",
          "2020-05-06T00:00:00",
          "2020-05-07T00:00:00",
          "2020-05-08T00:00:00",
          "2020-05-11T00:00:00",
          "2020-05-12T00:00:00",
          "2020-05-13T00:00:00",
          "2020-05-14T00:00:00",
          "2020-05-15T00:00:00",
          "2020-05-18T00:00:00",
          "2020-05-19T00:00:00",
          "2020-05-20T00:00:00",
          "2020-05-21T00:00:00",
          "2020-05-22T00:00:00",
          "2020-05-25T00:00:00",
          "2020-05-26T00:00:00",
          "2020-05-27T00:00:00",
          "2020-05-28T00:00:00",
          "2020-05-29T00:00:00",
          "2020-06-01T00:00:00",
          "2020-06-02T00:00:00",
          "2020-06-03T00:00:00",
          "2020-06-04T00:00:00",
          "2020-06-05T00:00:00",
          "2020-06-08T00:00:00",
          "2020-06-09T00:00:00",
          "2020-06-10T00:00:00",
          "2020-06-11T00:00:00",
          "2020-06-12T00:00:00",
          "2020-06-15T00:00:00",
          "2020-06-16T00:00:00",
          "2020-06-17T00:00:00",
          "2020-06-18T00:00:00",
          "2020-06-19T00:00:00",
          "2020-06-22T00:00:00",
          "2020-06-23T00:00:00",
          "2020-06-24T00:00:00",
          "2020-06-29T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-01T00:00:00",
          "2020-07-02T00:00:00",
          "2020-07-03T00:00:00",
          "2020-07-06T00:00:00",
          "2020-07-07T00:00:00",
          "2020-07-08T00:00:00",
          "2020-07-09T00:00:00",
          "2020-07-10T00:00:00",
          "2020-07-13T00:00:00",
          "2020-07-14T00:00:00",
          "2020-07-15T00:00:00",
          "2020-07-16T00:00:00",
          "2020-07-17T00:00:00",
          "2020-07-20T00:00:00",
          "2020-07-21T00:00:00",
          "2020-07-22T00:00:00",
          "2020-07-23T00:00:00",
          "2020-07-24T00:00:00",
          "2020-07-27T00:00:00",
          "2020-07-28T00:00:00",
          "2020-07-29T00:00:00",
          "2020-07-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-03T00:00:00",
          "2020-08-04T00:00:00",
          "2020-08-05T00:00:00",
          "2020-08-06T00:00:00",
          "2020-08-07T00:00:00",
          "2020-08-10T00:00:00",
          "2020-08-11T00:00:00",
          "2020-08-12T00:00:00",
          "2020-08-13T00:00:00",
          "2020-08-14T00:00:00",
          "2020-08-17T00:00:00",
          "2020-08-18T00:00:00",
          "2020-08-19T00:00:00",
          "2020-08-20T00:00:00",
          "2020-08-21T00:00:00",
          "2020-08-24T00:00:00",
          "2020-08-25T00:00:00",
          "2020-08-26T00:00:00",
          "2020-08-27T00:00:00",
          "2020-08-28T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-01T00:00:00",
          "2020-09-02T00:00:00",
          "2020-09-03T00:00:00",
          "2020-09-04T00:00:00",
          "2020-09-07T00:00:00",
          "2020-09-08T00:00:00",
          "2020-09-09T00:00:00",
          "2020-09-10T00:00:00",
          "2020-09-11T00:00:00",
          "2020-09-14T00:00:00",
          "2020-09-15T00:00:00",
          "2020-09-16T00:00:00",
          "2020-09-17T00:00:00",
          "2020-09-18T00:00:00",
          "2020-09-21T00:00:00",
          "2020-09-22T00:00:00",
          "2020-09-23T00:00:00",
          "2020-09-24T00:00:00",
          "2020-09-25T00:00:00",
          "2020-09-28T00:00:00",
          "2020-09-29T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-05T00:00:00",
          "2020-10-06T00:00:00",
          "2020-10-07T00:00:00",
          "2020-10-08T00:00:00",
          "2020-10-12T00:00:00",
          "2020-10-13T00:00:00",
          "2020-10-14T00:00:00",
          "2020-10-15T00:00:00",
          "2020-10-16T00:00:00",
          "2020-10-19T00:00:00",
          "2020-10-20T00:00:00",
          "2020-10-21T00:00:00",
          "2020-10-22T00:00:00",
          "2020-10-23T00:00:00",
          "2020-10-26T00:00:00",
          "2020-10-27T00:00:00",
          "2020-10-28T00:00:00",
          "2020-10-29T00:00:00",
          "2020-10-30T00:00:00",
          "2020-11-02T00:00:00",
          "2020-11-03T00:00:00",
          "2020-11-04T00:00:00",
          "2020-11-05T00:00:00",
          "2020-11-06T00:00:00",
          "2020-11-09T00:00:00",
          "2020-11-10T00:00:00",
          "2020-11-11T00:00:00",
          "2020-11-12T00:00:00",
          "2020-11-13T00:00:00",
          "2020-11-16T00:00:00",
          "2020-11-17T00:00:00",
          "2020-11-18T00:00:00",
          "2020-11-19T00:00:00",
          "2020-11-20T00:00:00",
          "2020-11-23T00:00:00",
          "2020-11-24T00:00:00",
          "2020-11-25T00:00:00",
          "2020-11-26T00:00:00",
          "2020-11-27T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-01T00:00:00",
          "2020-12-02T00:00:00",
          "2020-12-03T00:00:00",
          "2020-12-04T00:00:00",
          "2020-12-07T00:00:00",
          "2020-12-08T00:00:00",
          "2020-12-09T00:00:00",
          "2020-12-10T00:00:00",
          "2020-12-11T00:00:00",
          "2020-12-14T00:00:00",
          "2020-12-15T00:00:00",
          "2020-12-16T00:00:00",
          "2020-12-17T00:00:00",
          "2020-12-18T00:00:00",
          "2020-12-21T00:00:00",
          "2020-12-22T00:00:00",
          "2020-12-23T00:00:00",
          "2020-12-24T00:00:00",
          "2020-12-25T00:00:00",
          "2020-12-28T00:00:00",
          "2020-12-29T00:00:00",
          "2020-12-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-04T00:00:00",
          "2021-01-05T00:00:00",
          "2021-01-06T00:00:00",
          "2021-01-07T00:00:00",
          "2021-01-08T00:00:00",
          "2021-01-11T00:00:00",
          "2021-01-12T00:00:00",
          "2021-01-13T00:00:00",
          "2021-01-14T00:00:00",
          "2021-01-15T00:00:00",
          "2021-01-18T00:00:00",
          "2021-01-19T00:00:00",
          "2021-01-20T00:00:00",
          "2021-01-21T00:00:00",
          "2021-01-22T00:00:00",
          "2021-01-25T00:00:00",
          "2021-01-26T00:00:00",
          "2021-01-27T00:00:00",
          "2021-01-28T00:00:00",
          "2021-01-29T00:00:00",
          "2021-02-01T00:00:00",
          "2021-02-02T00:00:00",
          "2021-02-03T00:00:00",
          "2021-02-04T00:00:00",
          "2021-02-05T00:00:00",
          "2021-02-17T00:00:00",
          "2021-02-18T00:00:00",
          "2021-02-19T00:00:00",
          "2021-02-22T00:00:00",
          "2021-02-23T00:00:00",
          "2021-02-24T00:00:00",
          "2021-02-25T00:00:00",
          "2021-02-26T00:00:00",
          "2021-03-02T00:00:00",
          "2021-03-03T00:00:00",
          "2021-03-04T00:00:00",
          "2021-03-05T00:00:00",
          "2021-03-08T00:00:00",
          "2021-03-09T00:00:00",
          "2021-03-10T00:00:00",
          "2021-03-11T00:00:00",
          "2021-03-12T00:00:00",
          "2021-03-15T00:00:00",
          "2021-03-16T00:00:00",
          "2021-03-17T00:00:00",
          "2021-03-18T00:00:00",
          "2021-03-19T00:00:00",
          "2021-03-22T00:00:00",
          "2021-03-23T00:00:00",
          "2021-03-24T00:00:00",
          "2021-03-25T00:00:00",
          "2021-03-26T00:00:00",
          "2021-03-29T00:00:00",
          "2021-03-30T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-01T00:00:00",
          "2021-04-07T00:00:00",
          "2021-04-08T00:00:00",
          "2021-04-09T00:00:00",
          "2021-04-12T00:00:00",
          "2021-04-13T00:00:00",
          "2021-04-14T00:00:00",
          "2021-04-15T00:00:00",
          "2021-04-16T00:00:00",
          "2021-04-19T00:00:00",
          "2021-04-20T00:00:00",
          "2021-04-21T00:00:00",
          "2021-04-22T00:00:00",
          "2021-04-23T00:00:00",
          "2021-04-26T00:00:00",
          "2021-04-27T00:00:00",
          "2021-04-28T00:00:00",
          "2021-04-29T00:00:00",
          "2021-05-03T00:00:00",
          "2021-05-04T00:00:00",
          "2021-05-05T00:00:00",
          "2021-05-06T00:00:00",
          "2021-05-07T00:00:00",
          "2021-05-10T00:00:00",
          "2021-05-11T00:00:00",
          "2021-05-12T00:00:00",
          "2021-05-13T00:00:00",
          "2021-05-14T00:00:00",
          "2021-05-17T00:00:00",
          "2021-05-18T00:00:00",
          "2021-05-19T00:00:00",
          "2021-05-20T00:00:00",
          "2021-05-21T00:00:00",
          "2021-05-24T00:00:00",
          "2021-05-25T00:00:00",
          "2021-05-26T00:00:00",
          "2021-05-27T00:00:00",
          "2021-05-28T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-01T00:00:00",
          "2021-06-02T00:00:00",
          "2021-06-03T00:00:00",
          "2021-06-04T00:00:00",
          "2021-06-07T00:00:00",
          "2021-06-08T00:00:00",
          "2021-06-09T00:00:00",
          "2021-06-10T00:00:00",
          "2021-06-11T00:00:00",
          "2021-06-15T00:00:00",
          "2021-06-16T00:00:00",
          "2021-06-17T00:00:00",
          "2021-06-18T00:00:00",
          "2021-06-21T00:00:00",
          "2021-06-22T00:00:00",
          "2021-06-23T00:00:00",
          "2021-06-24T00:00:00",
          "2021-06-25T00:00:00",
          "2021-06-28T00:00:00",
          "2021-06-29T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-01T00:00:00",
          "2021-07-02T00:00:00",
          "2021-07-05T00:00:00",
          "2021-07-06T00:00:00",
          "2021-07-07T00:00:00",
          "2021-07-08T00:00:00",
          "2021-07-09T00:00:00",
          "2021-07-12T00:00:00",
          "2021-07-13T00:00:00",
          "2021-07-14T00:00:00",
          "2021-07-15T00:00:00",
          "2021-07-16T00:00:00",
          "2021-07-19T00:00:00",
          "2021-07-20T00:00:00",
          "2021-07-21T00:00:00",
          "2021-07-22T00:00:00",
          "2021-07-23T00:00:00",
          "2021-07-26T00:00:00",
          "2021-07-27T00:00:00",
          "2021-07-28T00:00:00",
          "2021-07-29T00:00:00",
          "2021-07-30T00:00:00",
          "2021-08-02T00:00:00",
          "2021-08-03T00:00:00",
          "2021-08-04T00:00:00",
          "2021-08-05T00:00:00",
          "2021-08-06T00:00:00",
          "2021-08-09T00:00:00",
          "2021-08-10T00:00:00",
          "2021-08-11T00:00:00",
          "2021-08-12T00:00:00",
          "2021-08-13T00:00:00",
          "2021-08-16T00:00:00",
          "2021-08-17T00:00:00",
          "2021-08-18T00:00:00",
          "2021-08-19T00:00:00",
          "2021-08-20T00:00:00",
          "2021-08-23T00:00:00",
          "2021-08-24T00:00:00",
          "2021-08-25T00:00:00",
          "2021-08-26T00:00:00",
          "2021-08-27T00:00:00",
          "2021-08-30T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-01T00:00:00",
          "2021-09-02T00:00:00",
          "2021-09-03T00:00:00",
          "2021-09-06T00:00:00",
          "2021-09-07T00:00:00",
          "2021-09-08T00:00:00",
          "2021-09-09T00:00:00",
          "2021-09-10T00:00:00",
          "2021-09-13T00:00:00",
          "2021-09-14T00:00:00",
          "2021-09-15T00:00:00",
          "2021-09-16T00:00:00",
          "2021-09-17T00:00:00",
          "2021-09-22T00:00:00",
          "2021-09-23T00:00:00",
          "2021-09-24T00:00:00",
          "2021-09-27T00:00:00",
          "2021-09-28T00:00:00",
          "2021-09-29T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-01T00:00:00",
          "2021-10-04T00:00:00",
          "2021-10-05T00:00:00",
          "2021-10-06T00:00:00",
          "2021-10-07T00:00:00",
          "2021-10-08T00:00:00",
          "2021-10-12T00:00:00",
          "2021-10-13T00:00:00",
          "2021-10-14T00:00:00",
          "2021-10-15T00:00:00",
          "2021-10-18T00:00:00",
          "2021-10-19T00:00:00",
          "2021-10-20T00:00:00",
          "2021-10-21T00:00:00",
          "2021-10-22T00:00:00",
          "2021-10-25T00:00:00",
          "2021-10-26T00:00:00",
          "2021-10-27T00:00:00",
          "2021-10-28T00:00:00",
          "2021-10-29T00:00:00",
          "2021-11-01T00:00:00",
          "2021-11-02T00:00:00",
          "2021-11-03T00:00:00",
          "2021-11-04T00:00:00",
          "2021-11-05T00:00:00",
          "2021-11-08T00:00:00",
          "2021-11-09T00:00:00",
          "2021-11-10T00:00:00",
          "2021-11-11T00:00:00",
          "2021-11-12T00:00:00",
          "2021-11-15T00:00:00",
          "2021-11-16T00:00:00",
          "2021-11-17T00:00:00",
          "2021-11-18T00:00:00",
          "2021-11-19T00:00:00",
          "2021-11-22T00:00:00",
          "2021-11-23T00:00:00",
          "2021-11-24T00:00:00",
          "2021-11-25T00:00:00",
          "2021-11-26T00:00:00",
          "2021-11-29T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-01T00:00:00",
          "2021-12-02T00:00:00",
          "2021-12-03T00:00:00",
          "2021-12-06T00:00:00",
          "2021-12-07T00:00:00",
          "2021-12-08T00:00:00",
          "2021-12-09T00:00:00",
          "2021-12-10T00:00:00",
          "2021-12-13T00:00:00",
          "2021-12-14T00:00:00",
          "2021-12-15T00:00:00",
          "2021-12-16T00:00:00",
          "2021-12-17T00:00:00",
          "2021-12-20T00:00:00",
          "2021-12-21T00:00:00",
          "2021-12-22T00:00:00",
          "2021-12-23T00:00:00",
          "2021-12-24T00:00:00",
          "2021-12-27T00:00:00",
          "2021-12-28T00:00:00",
          "2021-12-29T00:00:00",
          "2021-12-30T00:00:00",
          "2022-01-03T00:00:00",
          "2022-01-04T00:00:00",
          "2022-01-05T00:00:00",
          "2022-01-06T00:00:00",
          "2022-01-07T00:00:00",
          "2022-01-10T00:00:00",
          "2022-01-11T00:00:00",
          "2022-01-12T00:00:00",
          "2022-01-13T00:00:00",
          "2022-01-14T00:00:00",
          "2022-01-17T00:00:00",
          "2022-01-18T00:00:00",
          "2022-01-19T00:00:00",
          "2022-01-20T00:00:00",
          "2022-01-21T00:00:00",
          "2022-01-24T00:00:00",
          "2022-01-25T00:00:00",
          "2022-01-26T00:00:00",
          "2022-02-07T00:00:00",
          "2022-02-08T00:00:00",
          "2022-02-09T00:00:00",
          "2022-02-10T00:00:00",
          "2022-02-11T00:00:00",
          "2022-02-14T00:00:00",
          "2022-02-15T00:00:00",
          "2022-02-16T00:00:00",
          "2022-02-17T00:00:00",
          "2022-02-18T00:00:00",
          "2022-02-21T00:00:00",
          "2022-02-22T00:00:00",
          "2022-02-23T00:00:00",
          "2022-02-24T00:00:00",
          "2022-02-25T00:00:00",
          "2022-03-01T00:00:00",
          "2022-03-02T00:00:00",
          "2022-03-03T00:00:00",
          "2022-03-04T00:00:00",
          "2022-03-07T00:00:00",
          "2022-03-08T00:00:00",
          "2022-03-09T00:00:00",
          "2022-03-10T00:00:00",
          "2022-03-11T00:00:00",
          "2022-03-14T00:00:00",
          "2022-03-15T00:00:00",
          "2022-03-16T00:00:00",
          "2022-03-17T00:00:00",
          "2022-03-18T00:00:00",
          "2022-03-21T00:00:00",
          "2022-03-22T00:00:00",
          "2022-03-23T00:00:00",
          "2022-03-24T00:00:00",
          "2022-03-25T00:00:00",
          "2022-03-28T00:00:00",
          "2022-03-29T00:00:00",
          "2022-03-30T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-01T00:00:00",
          "2022-04-06T00:00:00",
          "2022-04-07T00:00:00",
          "2022-04-08T00:00:00",
          "2022-04-11T00:00:00",
          "2022-04-12T00:00:00",
          "2022-04-13T00:00:00",
          "2022-04-14T00:00:00",
          "2022-04-15T00:00:00",
          "2022-04-18T00:00:00",
          "2022-04-19T00:00:00",
          "2022-04-20T00:00:00",
          "2022-04-21T00:00:00",
          "2022-04-22T00:00:00",
          "2022-04-25T00:00:00",
          "2022-04-26T00:00:00",
          "2022-04-27T00:00:00",
          "2022-04-28T00:00:00",
          "2022-04-29T00:00:00",
          "2022-05-03T00:00:00",
          "2022-05-04T00:00:00",
          "2022-05-05T00:00:00",
          "2022-05-06T00:00:00",
          "2022-05-09T00:00:00",
          "2022-05-10T00:00:00",
          "2022-05-11T00:00:00",
          "2022-05-12T00:00:00",
          "2022-05-13T00:00:00",
          "2022-05-16T00:00:00",
          "2022-05-17T00:00:00",
          "2022-05-18T00:00:00",
          "2022-05-19T00:00:00",
          "2022-05-20T00:00:00",
          "2022-05-23T00:00:00",
          "2022-05-24T00:00:00",
          "2022-05-25T00:00:00",
          "2022-05-26T00:00:00",
          "2022-05-27T00:00:00",
          "2022-05-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-01T00:00:00",
          "2022-06-02T00:00:00",
          "2022-06-06T00:00:00",
          "2022-06-07T00:00:00",
          "2022-06-08T00:00:00",
          "2022-06-09T00:00:00",
          "2022-06-10T00:00:00",
          "2022-06-13T00:00:00",
          "2022-06-14T00:00:00",
          "2022-06-15T00:00:00"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "台灣加權指數走勢"
        },
        "xaxis": {
         "rangeslider": {
          "visible": false
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"1c55714d-cc2a-43c4-a332-0d6afcdcc557\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1c55714d-cc2a-43c4-a332-0d6afcdcc557\")) {                    Plotly.newPlot(                        \"1c55714d-cc2a-43c4-a332-0d6afcdcc557\",                        [{\"close\":[10530.5400390625,10566.740234375,10775.33984375,10785.009765625,10803.76953125,10779.4501953125,10706.7197265625,10652.5498046875,10773.900390625,10730.830078125,10895.4599609375,10865.1201171875,10743.76953125,10775.900390625,10785.73046875,10751.2197265625,10702.7802734375,10798.48046875,10843.419921875,10824.349609375,10876.4296875,10886.0498046875,10828.48046875,10799.2802734375,10873.1904296875,10944.5302734375,10947.259765625,10935.759765625,10941.41015625,10891.98046875,10885.73046875,10830.900390625,10823.8095703125,10731.75,10549.0400390625,10423.41015625,10394.75,10386.1796875,10494.490234375,10472.3603515625,10362.66015625,10427.73046875,10327.1298828125,10420.8896484375,10488.75,10522.5,10525.7998046875,10529.7802734375,10538.1103515625,10354.5703125,10387.23046875,10434.2900390625,10462.4296875,10618.0498046875,10634.849609375,10558.2099609375,10657.3095703125,10756.9296875,10780.6396484375,10753.580078125,10790.349609375,10827.5498046875,10898.1298828125,10874.5,10929.4501953125,10894.7001953125,10929.6904296875,10919.01953125,10918.009765625,10873.6904296875,10871.990234375,10829.6796875,10967.650390625,10947.8798828125,10875.91015625,10894.48046875,10935.0595703125,11017.3095703125,10889.9599609375,11066.9501953125,11111.7998046875,11162.830078125,11186.8798828125,11180.2197265625,11184.150390625,11271.25,11239.669921875,11320.1396484375,11296.1201171875,11315.01953125,11333.8701171875,11380.2802734375,11358.7099609375,11399.5302734375,11556.849609375,11644.0302734375,11653.0703125,11606.5595703125,11579.5400390625,11427.2802734375,11520.3701171875,11467.830078125,11450.419921875,11525.599609375,11599.7802734375,11656.400390625,11631.2001953125,11558.26953125,11566.7998046875,11561.580078125,11576.8203125,11647.4599609375,11617.080078125,11489.5703125,11502.830078125,11531.580078125,11510.4697265625,11594.650390625,11609.6396484375,11660.76953125,11627.83984375,11700.76953125,11836.419921875,11927.73046875,11939.76953125,12097.009765625,12122.4501953125,12018.900390625,11959.080078125,12022.23046875,11976.3798828125,12008.1298828125,12001.009765625,12091.58984375,12053.3701171875,11997.1396484375,12100.48046875,12110.4296875,11953.3603515625,11880.3203125,11817.099609375,11970.6298828125,12024.650390625,12113.419921875,12179.8095703125,12091.8798828125,12066.9296875,12090.2900390625,12118.7099609375,11421.740234375,11495.099609375,11354.919921875,11555.919921875,11573.6201171875,11749.6796875,11612.8095703125,11574.0703125,11664.0400390625,11774.1904296875,11791.7802734375,11815.7001953125,11763.509765625,11648.98046875,11758.83984375,11725.08984375,11686.349609375,11534.8701171875,11540.23046875,11433.6201171875,11292.169921875,11170.4599609375,11327.7197265625,11392.349609375,11514.8203125,11321.8095703125,10977.6396484375,11003.5400390625,10893.75,10422.3203125,10128.8701171875,9717.76953125,9439.6298828125,9218.669921875,8681.33984375,9234.08984375,8890.0302734375,9285.6201171875,9644.75,9736.3603515625,9698.919921875,9629.4296875,9708.0595703125,9663.6298828125,9818.740234375,9996.3896484375,10137.4697265625,10119.4296875,10157.6103515625,10099.2197265625,10332.9404296875,10447.2099609375,10375.48046875,10597.0400390625,10586.7099609375,10288.419921875,10307.740234375,10366.509765625,10347.3603515625,10567.26953125,10616.0595703125,10772.2197265625,10992.1396484375,10720.48046875,10774.6103515625,10774.98046875,10842.919921875,10901.419921875,11013.259765625,10879.4697265625,10938.26953125,10780.8798828125,10814.919921875,10740.5498046875,10860.4404296875,10907.7998046875,11008.3095703125,10811.150390625,10871.1796875,10997.2099609375,11014.66015625,10944.1904296875,10942.16015625,11079.01953125,11127.9296875,11320.16015625,11393.23046875,11479.400390625,11610.3203125,11637.1103515625,11720.16015625,11535.76953125,11429.9404296875,11306.259765625,11511.6396484375,11534.58984375,11548.330078125,11549.8603515625,11572.9296875,11612.3603515625,11660.669921875,11542.6201171875,11621.240234375,11703.419921875,11805.1396484375,11909.16015625,12116.7001953125,12092.9697265625,12170.1904296875,12192.6904296875,12073.6796875,12211.5595703125,12209.009765625,12202.849609375,12157.740234375,12181.5595703125,12174.5400390625,12397.5498046875,12473.26953125,12413.0400390625,12304.0400390625,12588.2998046875,12586.73046875,12540.9697265625,12722.919921875,12664.7998046875,12513.0302734375,12709.919921875,12802.2998046875,12913.5,12828.8701171875,12894.0,12780.1904296875,12670.349609375,12763.1298828125,12795.4599609375,12956.1103515625,12872.1396484375,12778.6396484375,12362.6396484375,12607.83984375,12647.1298828125,12758.25,12833.2900390625,12797.3095703125,12728.849609375,12591.4501953125,12703.2802734375,12699.5,12757.9697265625,12637.9501953125,12601.400390625,12663.5595703125,12608.580078125,12691.75,12675.9501953125,12787.8203125,12845.650390625,12976.759765625,12872.740234375,12875.6201171875,12795.1201171875,12645.509765625,12583.8798828125,12264.3798828125,12232.91015625,12462.759765625,12467.73046875,12515.6103515625,12548.2802734375,12704.23046875,12746.3701171875,12887.1904296875,12955.91015625,12947.1298828125,12919.3095703125,12827.8203125,12750.3701171875,12908.33984375,12862.3701171875,12877.25,12917.0302734375,12898.8203125,12909.0302734375,12875.009765625,12793.75,12662.91015625,12546.33984375,12591.3095703125,12736.009765625,12867.900390625,12918.7998046875,12973.5302734375,13127.4697265625,13081.7197265625,13262.1904296875,13221.7802734375,13273.330078125,13551.830078125,13593.009765625,13773.2900390625,13722.4296875,13716.4404296875,13878.009765625,13807.1298828125,13738.830078125,13845.66015625,13867.08984375,13722.8896484375,13885.669921875,13989.1396484375,13977.08984375,14132.4404296875,14256.599609375,14360.400390625,14390.1396484375,14249.490234375,14261.6904296875,14211.0498046875,14068.51953125,14304.4599609375,14258.9296875,14249.9599609375,14384.9599609375,14177.4599609375,14223.08984375,14280.2802734375,14331.419921875,14483.0703125,14472.0498046875,14687.7001953125,14732.5302734375,14902.0302734375,15000.0302734375,14983.1298828125,15214.0,15463.9501953125,15557.2998046875,15500.7001953125,15769.98046875,15707.1904296875,15616.3896484375,15612.0,15877.3701171875,15806.1796875,16153.76953125,16019.0302734375,15946.5400390625,15658.849609375,15701.4501953125,15415.8798828125,15138.3095703125,15410.08984375,15760.0498046875,15771.3203125,15706.2197265625,15802.400390625,16362.2900390625,16424.509765625,16341.3798828125,16410.16015625,16443.400390625,16212.5302734375,16452.1796875,15953.7998046875,15946.8798828125,16211.73046875,15906.41015625,15855.23046875,15820.1103515625,15853.08984375,15911.669921875,16179.5595703125,16255.1796875,16249.330078125,16313.16015625,16215.8203125,16287.83984375,16070.240234375,16189.2197265625,16177.58984375,16032.1201171875,16060.1396484375,16305.8798828125,16475.970703125,16554.900390625,16431.130859375,16571.279296875,16815.359375,16926.439453125,16854.099609375,16859.69921875,16824.91015625,16865.970703125,17076.73046875,17158.810546875,17263.279296875,17323.869140625,17202.109375,17096.970703125,17300.26953125,17572.2890625,17595.900390625,17567.529296875,17566.66015625,17222.349609375,16933.779296875,16843.439453125,16994.359375,17285.0,17235.609375,16583.130859375,15902.3701171875,15670.099609375,15827.08984375,15353.8896484375,16145.98046875,16132.66015625,16042.3603515625,16302.0595703125,16338.2900390625,16595.669921875,16643.689453125,16601.609375,16870.859375,17068.4296875,17162.380859375,17165.0390625,17246.16015625,17147.41015625,17083.91015625,17076.2109375,16966.220703125,17159.220703125,17213.51953125,17371.2890625,17307.859375,17390.609375,17318.5390625,17062.98046875,17075.55078125,17336.7109375,17407.9609375,17502.990234375,17590.970703125,17598.189453125,17755.4609375,17713.939453125,17710.150390625,17919.330078125,17913.0703125,17850.689453125,17866.08984375,17661.48046875,17814.330078125,17847.51953125,17845.75,18034.189453125,17895.25,17789.25,17528.740234375,17458.7890625,17572.330078125,17572.919921875,17403.560546875,17269.869140625,17135.220703125,17402.810546875,17247.41015625,17503.279296875,17553.759765625,17623.890625,17603.119140625,17526.279296875,17485.150390625,17323.640625,17227.1796875,17219.939453125,16982.109375,16858.76953125,16661.359375,16826.26953125,16375.400390625,16341.9404296875,16741.83984375,16818.73046875,17045.859375,17066.9609375,17209.9296875,17396.51953125,17490.2890625,17473.990234375,17319.759765625,17516.919921875,17495.30078125,17428.869140625,17270.490234375,17304.330078125,17474.5703125,17446.310546875,17434.900390625,17354.0,17278.69921875,17276.7890625,16925.8203125,17078.220703125,17260.189453125,17313.76953125,17181.439453125,16855.4609375,16934.76953125,16570.890625,16408.349609375,16460.75,16393.16015625,16713.859375,16640.4296875,16462.83984375,16347.990234375,16387.279296875,16781.189453125,16705.4609375,16900.669921875,16887.8203125,16889.509765625,16888.740234375,16894.240234375,17034.33984375,17074.55078125,17041.630859375,16987.41015625,17068.240234375,17065.970703125,17122.16015625,17078.859375,17296.900390625,17415.30078125,17541.359375,17559.650390625,17452.51953125,17518.130859375,17634.470703125,17693.130859375,17764.0390625,17841.369140625,17818.310546875,17803.5390625,17666.119140625,17642.51953125,17654.189453125,17369.390625,17328.08984375,17427.759765625,17585.990234375,17724.880859375,17697.140625,17688.2109375,17796.919921875,17832.419921875,17914.119140625,17826.259765625,17767.599609375,17599.369140625,17660.099609375,17785.740234375,17812.58984375,17669.109375,17789.26953125,17826.830078125,17946.66015625,17961.640625,18048.939453125,18196.810546875,18248.279296875,18218.83984375,18270.509765625,18526.349609375,18499.9609375,18367.919921875,18169.759765625,18239.380859375,18288.2109375,18375.400390625,18436.9296875,18403.330078125,18525.439453125,18378.640625,18227.4609375,18218.279296875,17899.30078125,17989.0390625,17701.119140625,17674.400390625,17900.30078125,17966.560546875,18151.759765625,18338.05078125,18310.939453125,17997.669921875,17951.810546875,18231.470703125,18268.5703125,18232.349609375,18221.490234375,17969.2890625,18055.73046875,17594.55078125,17652.1796875,17898.25,17867.599609375,17934.400390625,17736.51953125,17178.689453125,16825.25,17015.359375,17433.19921875,17264.740234375,17263.0390625,16926.060546875,16940.830078125,17448.220703125,17456.51953125,17560.359375,17559.7109375,17731.369140625,17699.060546875,17676.94921875,17520.009765625,17548.66015625,17740.560546875,17693.470703125,17625.58984375,17522.5,17178.630859375,17284.5390625,17048.369140625,16990.91015625,17301.650390625,17245.650390625,17004.1796875,16898.869140625,16993.400390625,17148.880859375,17127.94921875,17025.08984375,16620.900390625,16644.7890625,16303.349609375,16419.380859375,16592.1796875,16498.900390625,16565.830078125,16696.119140625,16408.19921875,16048.919921875,16061.7001953125,16006.25,15616.6796875,15832.5400390625,15901.0400390625,16056.08984375,16296.8603515625,16020.3203125,16144.849609375,16156.41015625,15963.6298828125,16104.0302734375,15968.830078125,16266.2197265625,16610.619140625,16807.76953125,16675.08984375,16552.5703125,16605.9609375,16512.880859375,16670.509765625,16621.33984375,16460.119140625,16070.98046875,16047.3701171875,15999.25],\"high\":[10562.9697265625,10573.76953125,10778.6396484375,10799.1396484375,10840.2900390625,10786.330078125,10787.6201171875,10690.98046875,10800.2900390625,10788.080078125,10914.849609375,10888.2099609375,10812.650390625,10791.2802734375,10812.3798828125,10781.1103515625,10733.5703125,10810.16015625,10856.509765625,10867.58984375,10880.5595703125,10899.83984375,10861.1103515625,10835.4296875,10919.9599609375,10949.759765625,10994.4404296875,10976.4599609375,10945.919921875,10907.4404296875,10905.740234375,10927.16015625,10835.7998046875,10773.25,10641.2802734375,10542.490234375,10425.599609375,10461.3701171875,10500.76953125,10514.0498046875,10436.6201171875,10518.01953125,10374.75,10456.3203125,10516.0498046875,10538.1201171875,10545.25,10596.41015625,10541.73046875,10453.400390625,10431.7802734375,10456.2998046875,10466.990234375,10618.0498046875,10634.849609375,10650.150390625,10670.4296875,10766.599609375,10803.0703125,10810.0302734375,10815.58984375,10862.6796875,10898.1298828125,10912.23046875,10971.5400390625,10954.1796875,10931.759765625,10948.1904296875,10947.5302734375,10885.669921875,10934.91015625,10921.169921875,10967.650390625,10965.2001953125,10888.400390625,10941.490234375,10954.4501953125,11020.509765625,10989.3896484375,11085.490234375,11126.4501953125,11180.1103515625,11186.8798828125,11237.3798828125,11188.1298828125,11271.25,11281.1201171875,11320.1396484375,11347.1796875,11341.0302734375,11373.9501953125,11380.2802734375,11428.419921875,11399.5302734375,11559.0703125,11644.0302734375,11668.2001953125,11647.4697265625,11642.41015625,11570.330078125,11520.3701171875,11509.3701171875,11498.25,11552.25,11599.7802734375,11656.400390625,11649.0703125,11573.83984375,11577.01953125,11604.75,11642.9599609375,11651.73046875,11654.0595703125,11623.58984375,11524.849609375,11531.580078125,11513.830078125,11604.3896484375,11657.650390625,11678.3603515625,11649.73046875,11700.76953125,11875.98046875,11990.7802734375,11975.73046875,12097.009765625,12125.900390625,12111.76953125,12028.2197265625,12022.23046875,12027.599609375,12032.3701171875,12030.66015625,12093.01953125,12114.8896484375,12047.75,12110.740234375,12197.6396484375,12040.080078125,11986.0302734375,11899.669921875,11992.5498046875,12038.2099609375,12113.419921875,12186.6201171875,12169.919921875,12075.5400390625,12117.5498046875,12151.419921875,11933.23046875,11594.2099609375,11365.900390625,11581.3603515625,11620.419921875,11749.6796875,11712.7099609375,11598.0302734375,11678.580078125,11801.51953125,11854.98046875,11840.7900390625,11775.5595703125,11717.099609375,11783.4296875,11827.830078125,11755.169921875,11615.2802734375,11567.08984375,11494.2197265625,11470.23046875,11282.0498046875,11390.240234375,11392.349609375,11525.3203125,11471.3603515625,11221.759765625,11032.4697265625,11088.23046875,10845.3896484375,10171.2001953125,10130.7998046875,9677.0703125,9509.6103515625,9085.2802734375,9264.419921875,9029.51953125,9415.6396484375,9722.3701171875,9739.73046875,9954.8603515625,9655.7001953125,9789.8095703125,9736.0,9818.740234375,10039.150390625,10149.759765625,10246.83984375,10165.1201171875,10179.400390625,10344.1298828125,10461.83984375,10425.2900390625,10710.150390625,10637.6396484375,10544.7998046875,10310.849609375,10462.7001953125,10386.9296875,10578.9599609375,10620.51953125,10794.830078125,11012.7802734375,10781.509765625,10833.5302734375,10828.73046875,10890.8701171875,10970.7001953125,11039.1103515625,10974.509765625,10938.26953125,10894.169921875,10904.009765625,10812.8701171875,10924.7900390625,10933.98046875,11021.66015625,10977.5498046875,10874.5400390625,11046.2197265625,11069.7197265625,11087.5302734375,10965.0302734375,11109.2900390625,11170.490234375,11330.51953125,11425.419921875,11482.3603515625,11631.8896484375,11640.4501953125,11740.900390625,11771.1201171875,11442.2998046875,11469.83984375,11542.25,11550.3203125,11567.419921875,11584.8798828125,11632.8798828125,11679.400390625,11701.23046875,11608.7998046875,11638.0,11736.3896484375,11805.2099609375,11933.58984375,12116.7001953125,12249.9501953125,12190.26953125,12273.4296875,12253.580078125,12216.240234375,12228.3095703125,12320.48046875,12220.9296875,12266.9296875,12221.330078125,12450.16015625,12486.9501953125,12429.759765625,12502.83984375,12686.3603515625,13031.7001953125,12660.8701171875,12769.4697265625,12733.48046875,12673.01953125,12709.919921875,12816.3896484375,12971.8701171875,12912.6298828125,12933.16015625,12906.009765625,12730.9599609375,12812.7998046875,12801.3095703125,12956.1103515625,12981.580078125,12950.1103515625,12764.75,12638.650390625,12702.9404296875,12814.5595703125,12833.2900390625,12960.6796875,12799.16015625,12831.3798828125,12703.2802734375,12802.6103515625,12857.7900390625,12655.669921875,12725.0498046875,12702.5498046875,12616.5703125,12708.6396484375,12701.7900390625,12793.0703125,12857.73046875,13021.6796875,12972.51953125,12927.8095703125,12952.3095703125,12786.91015625,12700.8896484375,12487.48046875,12385.8095703125,12462.759765625,12571.33984375,12568.6796875,12637.6201171875,12706.3896484375,12774.3603515625,12887.1904296875,12997.7900390625,12994.66015625,12960.0,12909.1103515625,12896.2998046875,12911.41015625,12917.75,12942.1201171875,12917.0302734375,12963.259765625,12971.580078125,12884.3203125,12879.98046875,12699.8203125,12656.2900390625,12594.33984375,12760.1396484375,12885.2802734375,12918.7998046875,12999.16015625,13149.900390625,13142.2197265625,13262.1904296875,13324.8603515625,13273.330078125,13551.830078125,13780.1201171875,13773.2900390625,13785.919921875,13726.2197265625,13921.16015625,13951.169921875,13893.2001953125,13856.759765625,13885.009765625,13969.3896484375,13885.669921875,13995.7001953125,14049.580078125,14149.5595703125,14306.8701171875,14367.0,14427.41015625,14319.3798828125,14353.169921875,14270.33984375,14256.5498046875,14339.5,14338.400390625,14329.1201171875,14384.9599609375,14411.9296875,14247.7900390625,14324.419921875,14400.830078125,14483.0703125,14547.0703125,14695.4404296875,14760.0595703125,14937.1298828125,15000.0302734375,15197.6796875,15270.400390625,15463.9501953125,15557.2998046875,15642.0302734375,15778.51953125,15760.150390625,16041.58984375,15676.4501953125,15928.1396484375,16004.3203125,16238.4599609375,16138.0400390625,16014.9599609375,16023.8701171875,15837.4697265625,15557.01953125,15603.41015625,15429.98046875,15838.150390625,15896.400390625,15801.3701171875,15938.08984375,16406.779296875,16517.73046875,16382.7001953125,16579.169921875,16467.75,16456.939453125,16474.05078125,16190.0400390625,16262.91015625,16211.73046875,16091.75,15934.4296875,16074.9697265625,15864.259765625,15986.0703125,16216.2197265625,16298.0302734375,16281.91015625,16340.66015625,16349.2099609375,16410.01953125,16186.4599609375,16235.6298828125,16351.3798828125,16125.5400390625,16146.580078125,16325.6201171875,16520.890625,16556.189453125,16550.19921875,16602.470703125,16816.33984375,16926.720703125,17016.130859375,16979.349609375,17041.369140625,16935.390625,17076.73046875,17158.810546875,17294.150390625,17323.869140625,17282.759765625,17428.150390625,17300.26953125,17572.2890625,17630.189453125,17628.810546875,17709.23046875,17546.939453125,17328.55078125,17052.19921875,17088.73046875,17285.0,17304.470703125,17137.189453125,16552.630859375,16031.9296875,16075.48046875,15719.740234375,16153.76953125,16273.98046875,16154.080078125,16431.140625,16397.58984375,16657.599609375,16706.2890625,16601.609375,16889.009765625,17113.330078125,17184.30078125,17274.66015625,17311.279296875,17225.060546875,17189.23046875,17181.51953125,17082.51953125,17159.220703125,17278.55078125,17371.2890625,17398.220703125,17390.609375,17417.5,17305.490234375,17270.470703125,17375.7890625,17439.630859375,17597.330078125,17595.509765625,17713.240234375,17797.26953125,17863.900390625,17795.880859375,17945.51953125,18008.369140625,17933.619140625,17935.109375,17778.119140625,17947.900390625,18018.0390625,17940.859375,18034.189453125,17926.380859375,17854.349609375,17724.919921875,17707.560546875,17667.380859375,17672.599609375,17637.359375,17459.349609375,17252.869140625,17402.810546875,17429.009765625,17503.279296875,17553.759765625,17636.109375,17643.970703125,17593.849609375,17510.08984375,17524.91015625,17327.19921875,17233.109375,17216.33984375,16983.58984375,16870.55078125,16826.26953125,16777.779296875,16507.109375,16785.2890625,16904.30078125,17045.859375,17201.419921875,17229.890625,17396.51953125,17490.2890625,17503.9296875,17523.16015625,17540.779296875,17633.669921875,17559.2109375,17447.580078125,17319.08984375,17474.5703125,17482.5703125,17529.470703125,17439.75,17411.369140625,17408.7109375,17196.7890625,17145.25,17273.58984375,17335.990234375,17286.890625,17127.859375,16994.2109375,16883.0,16680.2890625,16460.75,16568.2109375,16731.580078125,16771.7109375,16589.720703125,16543.330078125,16519.9609375,16781.189453125,16862.029296875,16916.720703125,16973.859375,17026.890625,16932.890625,16930.98046875,17067.150390625,17079.599609375,17104.859375,17073.970703125,17119.4296875,17237.08984375,17153.759765625,17219.650390625,17296.900390625,17415.30078125,17581.51953125,17575.439453125,17527.1796875,17602.05078125,17683.189453125,17708.099609375,17764.0390625,17841.369140625,17986.1796875,17857.73046875,17797.900390625,17722.890625,17724.5,17641.7890625,17415.630859375,17535.23046875,17626.2109375,17741.55078125,17781.140625,17763.419921875,17796.919921875,17988.880859375,17925.119140625,17893.2890625,17919.349609375,17754.380859375,17678.779296875,17814.3203125,17822.740234375,17812.810546875,17825.890625,17870.16015625,17960.990234375,18039.849609375,18099.779296875,18197.359375,18283.25,18291.25,18379.689453125,18526.349609375,18619.609375,18427.150390625,18444.119140625,18243.259765625,18293.529296875,18394.69921875,18459.73046875,18509.0,18535.419921875,18575.41015625,18359.140625,18292.140625,18113.939453125,18004.44921875,17890.7109375,17776.419921875,17900.30078125,18063.55078125,18168.599609375,18338.05078125,18310.939453125,18182.73046875,18085.91015625,18233.7890625,18330.630859375,18262.9609375,18253.099609375,18159.509765625,18109.279296875,17939.529296875,17737.3203125,17944.5,17918.98046875,18026.029296875,17845.009765625,17581.5703125,17113.669921875,17072.740234375,17478.4296875,17373.900390625,17363.0390625,17177.26953125,17085.76953125,17472.94921875,17472.380859375,17603.990234375,17573.2890625,17738.689453125,17711.169921875,17747.130859375,17527.150390625,17585.91015625,17770.0703125,17767.810546875,17657.759765625,17528.990234375,17438.599609375,17316.689453125,17316.349609375,17100.849609375,17333.2109375,17374.140625,17141.330078125,16999.359375,17106.259765625,17148.880859375,17213.66015625,17088.759765625,16912.30078125,16729.470703125,16427.3203125,16455.5703125,16663.66015625,16604.869140625,16617.060546875,16783.779296875,16491.109375,16345.83984375,16071.5,16081.150390625,15943.6201171875,15860.6904296875,16032.849609375,16085.849609375,16316.580078125,16111.490234375,16181.66015625,16249.4501953125,16219.7998046875,16169.1796875,16179.009765625,16266.2197265625,16610.619140625,16807.76953125,16811.0390625,16617.259765625,16654.119140625,16593.75,16702.990234375,16643.94921875,16581.810546875,16295.0703125,16067.7998046875,16106.75],\"low\":[10474.1904296875,10521.7001953125,10650.48046875,10745.25,10773.4697265625,10695.490234375,10693.0400390625,10645.3701171875,10674.2001953125,10719.080078125,10821.2998046875,10843.6396484375,10720.66015625,10750.900390625,10756.669921875,10708.919921875,10680.849609375,10723.23046875,10799.650390625,10823.8896484375,10769.5703125,10865.0,10804.33984375,10792.26953125,10861.990234375,10909.150390625,10932.5302734375,10912.919921875,10879.400390625,10879.240234375,10863.330078125,10827.740234375,10754.0703125,10714.73046875,10524.6796875,10418.25,10180.0400390625,10366.5400390625,10383.6201171875,10440.76953125,10361.8896484375,10427.73046875,10287.759765625,10318.3701171875,10449.3896484375,10497.1298828125,10504.490234375,10515.7197265625,10503.0703125,10340.150390625,10368.9599609375,10397.599609375,10406.490234375,10504.2802734375,10589.419921875,10557.0302734375,10566.3203125,10714.26953125,10753.4501953125,10732.7802734375,10746.8701171875,10804.01953125,10822.6904296875,10866.4296875,10878.33984375,10874.66015625,10903.4501953125,10905.919921875,10886.6201171875,10825.849609375,10856.9599609375,10827.6103515625,10855.599609375,10927.3896484375,10809.2802734375,10854.1201171875,10919.4296875,10986.009765625,10889.9599609375,11004.740234375,11091.2900390625,11113.1796875,11131.4501953125,11154.8603515625,11147.8603515625,11218.25,11209.099609375,11254.58984375,11281.3701171875,11295.4404296875,11292.830078125,11306.9501953125,11358.7099609375,11335.509765625,11451.7197265625,11576.1796875,11601.7001953125,11552.080078125,11561.3095703125,11409.830078125,11459.6796875,11439.08984375,11424.76953125,11485.4599609375,11532.9697265625,11576.7900390625,11591.73046875,11478.4296875,11534.150390625,11559.330078125,11576.8203125,11590.8095703125,11604.6796875,11485.16015625,11454.3798828125,11460.0595703125,11457.4296875,11546.75,11577.830078125,11631.0302734375,11607.259765625,11622.580078125,11766.98046875,11913.01953125,11915.6103515625,11939.6201171875,12059.919921875,12008.16015625,11937.4501953125,11960.2099609375,11976.3798828125,11973.9501953125,11982.6396484375,12032.7197265625,12046.2197265625,11997.1396484375,12026.23046875,12023.599609375,11953.3603515625,11822.400390625,11777.4501953125,11889.4599609375,11959.0595703125,12037.240234375,12140.26953125,12048.009765625,12006.080078125,12055.91015625,12101.5498046875,11418.2197265625,11436.9501953125,11138.0302734375,11393.0302734375,11512.7099609375,11605.1103515625,11592.08984375,11423.33984375,11614.25,11693.400390625,11784.9404296875,11788.8701171875,11724.8203125,11642.9501953125,11654.16015625,11714.150390625,11661.6298828125,11512.080078125,11415.4697265625,11408.0595703125,11274.51953125,11049.849609375,11279.8603515625,11297.8095703125,11454.650390625,11310.3203125,10977.6396484375,10830.2197265625,10885.91015625,10359.669921875,9636.150390625,9717.76953125,9371.349609375,9218.669921875,8523.6298828125,8816.8603515625,8750.1396484375,9083.7802734375,9426.4296875,9565.01953125,9691.1396484375,9415.51953125,9630.2998046875,9663.6298828125,9651.51953125,9928.16015625,9984.66015625,10092.3603515625,10103.76953125,10080.650390625,10130.650390625,10366.76953125,10317.1298828125,10552.580078125,10542.9599609375,10278.9501953125,10140.08984375,10294.8798828125,10324.2001953125,10407.490234375,10537.7099609375,10656.400390625,10826.259765625,10658.5,10735.009765625,10708.099609375,10775.16015625,10883.2197265625,10942.1796875,10854.509765625,10828.9296875,10777.4599609375,10730.5498046875,10730.7001953125,10812.7900390625,10860.150390625,10933.2099609375,10804.5595703125,10719.25,10903.3203125,10972.25,10915.2998046875,10861.900390625,10971.400390625,11102.9697265625,11182.51953125,11341.580078125,11411.9404296875,11528.4404296875,11537.3798828125,11621.01953125,11516.58984375,11244.6396484375,11299.4501953125,11380.669921875,11482.3203125,11485.3896484375,11542.3701171875,11553.91015625,11530.849609375,11637.7900390625,11500.650390625,11563.599609375,11622.6396484375,11690.9501953125,11857.900390625,11941.83984375,12028.5703125,12083.83984375,12181.33984375,12030.2001953125,12109.6103515625,12143.91015625,12162.51953125,12107.0400390625,12144.6201171875,12065.7998046875,12242.3203125,12389.400390625,12347.8603515625,12266.5498046875,12586.91015625,12533.1904296875,12488.009765625,12616.830078125,12635.7099609375,12506.58984375,12577.8798828125,12739.990234375,12851.2001953125,12791.1796875,12786.7998046875,12780.1904296875,12625.5400390625,12717.1298828125,12679.1904296875,12800.6904296875,12840.25,12778.6396484375,12144.759765625,12462.51953125,12567.9501953125,12698.849609375,12734.6904296875,12786.4501953125,12674.5703125,12591.4501953125,12565.830078125,12646.4697265625,12732.3203125,12559.7802734375,12575.3603515625,12614.7197265625,12480.5,12650.400390625,12616.6796875,12680.41015625,12773.23046875,12948.7197265625,12851.51953125,12841.08984375,12795.08984375,12632.740234375,12548.0703125,12264.3798828125,12149.8095703125,12282.3603515625,12429.7197265625,12466.580078125,12519.1201171875,12644.08984375,12619.8095703125,12818.009765625,12898.8896484375,12857.099609375,12895.7900390625,12786.259765625,12750.3701171875,12803.3203125,12862.3701171875,12846.419921875,12827.400390625,12875.23046875,12894.1796875,12821.0703125,12761.330078125,12583.3701171875,12546.33984375,12480.66015625,12641.2802734375,12736.01953125,12840.400390625,12927.2998046875,13048.669921875,13022.919921875,13067.0400390625,13193.740234375,13170.1201171875,13356.740234375,13593.009765625,13608.7197265625,13700.0400390625,13666.0703125,13793.5498046875,13798.3203125,13731.8095703125,13763.7900390625,13811.8095703125,13722.8896484375,13749.7099609375,13894.7001953125,13940.0703125,14010.169921875,14142.01953125,14184.580078125,14336.6904296875,14191.849609375,14081.419921875,14182.25,14053.349609375,14145.240234375,14213.490234375,14245.599609375,14166.8896484375,14175.6904296875,14134.8701171875,14256.2900390625,14296.9697265625,14363.4501953125,14435.0302734375,14476.6904296875,14646.330078125,14720.25,14861.990234375,14837.0,15049.8603515625,15275.3798828125,15395.73046875,15421.23046875,15550.16015625,15620.9599609375,15615.1103515625,15320.9697265625,15716.6396484375,15745.48046875,15775.73046875,15973.01953125,15772.6298828125,15589.2099609375,15642.1201171875,15367.4501953125,15138.3095703125,15089.9599609375,15546.6904296875,15741.16015625,15606.740234375,15774.33984375,16197.4501953125,16323.2197265625,16211.25,16410.16015625,16211.75,16212.5302734375,16322.26953125,15953.7998046875,15946.8798828125,15884.5498046875,15840.990234375,15636.4296875,15816.5,15657.919921875,15857.3203125,15947.2099609375,16166.349609375,16194.91015625,16244.990234375,16166.349609375,16264.6103515625,16022.169921875,15983.76953125,16166.2900390625,15967.9501953125,15944.9599609375,16140.3701171875,16411.669921875,16438.400390625,16427.19921875,16496.16015625,16715.3203125,16735.779296875,16815.58984375,16793.5390625,16802.220703125,16559.5703125,16851.060546875,16998.91015625,17135.51953125,17175.01953125,17167.390625,17066.169921875,17055.48046875,17378.359375,17489.7109375,17497.740234375,17548.30078125,17222.349609375,16647.609375,16843.439453125,16764.7109375,17032.23046875,17188.150390625,16460.869140625,15165.26953125,15368.5400390625,15702.099609375,15159.8603515625,15564.9599609375,16009.759765625,15943.4501953125,16136.8603515625,16136.9697265625,16444.75,16523.23046875,16419.419921875,16690.0390625,16939.91015625,17060.44921875,17056.419921875,17198.890625,17084.490234375,16775.849609375,17056.529296875,16907.4296875,16978.009765625,17193.609375,17279.69921875,17275.01953125,17150.51953125,17318.5390625,17023.310546875,17075.55078125,17127.58984375,17319.890625,17471.0703125,17481.23046875,17541.140625,17648.25,17644.359375,17676.98046875,17783.80078125,17895.779296875,17756.8203125,17742.080078125,17597.4609375,17759.900390625,17786.759765625,17716.439453125,17878.779296875,17779.609375,17708.150390625,17500.689453125,17352.240234375,17482.810546875,17511.859375,17403.560546875,17264.5,16893.69921875,17190.48046875,17237.669921875,17231.220703125,17456.19921875,17557.1796875,17566.720703125,17469.6796875,17350.150390625,17305.060546875,17090.25,17139.2109375,16978.109375,16773.5703125,16657.630859375,16418.5390625,16375.400390625,16248.080078125,16459.130859375,16779.900390625,16821.080078125,16984.689453125,17000.259765625,17244.619140625,17207.5703125,17415.51953125,17319.759765625,17380.51953125,17461.0703125,17388.369140625,17167.080078125,17122.94921875,17270.279296875,17387.5703125,17424.5390625,17316.51953125,17254.099609375,17235.44921875,16838.580078125,16998.0703125,17130.740234375,17235.6796875,17113.470703125,16801.779296875,16767.19921875,16503.740234375,16380.0595703125,16162.169921875,16303.6298828125,16465.5703125,16605.490234375,16349.490234375,16328.4697265625,16347.8798828125,16426.759765625,16695.890625,16772.150390625,16855.810546875,16873.990234375,16800.490234375,16784.109375,16909.3203125,16973.16015625,16994.369140625,16920.6796875,17021.76953125,17026.630859375,17080.509765625,17061.4609375,17097.16015625,17279.4609375,17433.099609375,17489.869140625,17403.48046875,17479.55078125,17560.51953125,17629.80078125,17669.580078125,17748.2109375,17786.05078125,17790.640625,17650.0703125,17588.779296875,17609.509765625,17330.439453125,17167.240234375,17369.7890625,17374.58984375,17559.439453125,17670.3203125,17585.94921875,17642.3203125,17832.419921875,17807.119140625,17767.80078125,17767.599609375,17566.990234375,17556.869140625,17719.060546875,17718.26953125,17646.390625,17652.3203125,17799.720703125,17855.779296875,17953.5703125,17975.41015625,18099.7109375,18192.859375,18216.44921875,18238.470703125,18395.140625,18446.51953125,18253.8203125,18134.41015625,18043.970703125,18135.4296875,18255.380859375,18346.810546875,18213.439453125,18435.01953125,18378.640625,18199.349609375,18125.19921875,17851.390625,17682.330078125,17645.66015625,17633.029296875,17712.349609375,17955.94921875,18039.23046875,18145.0390625,18191.75,17965.220703125,17942.6796875,18109.169921875,18190.240234375,18098.51953125,18129.650390625,17840.380859375,17954.75,17561.0703125,17554.970703125,17657.5,17784.640625,17906.240234375,17710.5390625,17135.6796875,16764.779296875,16944.080078125,17224.73046875,17239.900390625,17217.580078125,16911.94921875,16808.4296875,17172.810546875,17359.5,17503.94921875,17468.55078125,17581.619140625,17603.220703125,17633.859375,17368.66015625,17493.009765625,17572.900390625,17682.529296875,17465.609375,17381.0703125,17178.630859375,17210.400390625,17046.669921875,16905.130859375,17080.400390625,17245.650390625,17004.1796875,16845.509765625,16926.33984375,16983.55078125,17106.369140625,16923.869140625,16579.890625,16582.109375,16219.41015625,16256.8798828125,16521.939453125,16465.990234375,16514.30078125,16650.810546875,16312.169921875,16048.919921875,15734.4404296875,15953.26953125,15616.6796875,15687.150390625,15847.4599609375,15915.9296875,16172.7998046875,15892.73046875,16058.2001953125,16125.4501953125,15963.6298828125,15980.900390625,15949.6103515625,16075.91015625,16368.9296875,16493.0703125,16649.91015625,16540.55078125,16509.009765625,16465.869140625,16538.669921875,16557.5390625,16403.330078125,16055.8896484375,15869.0595703125,15981.5595703125],\"open\":[10488.7001953125,10547.150390625,10650.48046875,10749.41015625,10817.7099609375,10734.25,10777.1796875,10661.3896484375,10674.2001953125,10786.66015625,10821.2998046875,10878.009765625,10793.099609375,10755.8701171875,10785.849609375,10742.8095703125,10729.830078125,10723.23046875,10817.599609375,10855.16015625,10819.919921875,10865.0,10861.1103515625,10821.919921875,10861.990234375,10910.5,10963.8603515625,10969.740234375,10892.25,10898.25,10872.6298828125,10909.98046875,10824.150390625,10773.0595703125,10641.2802734375,10528.8701171875,10304.8798828125,10422.8896484375,10383.6201171875,10491.7900390625,10436.6201171875,10444.5,10374.75,10345.419921875,10449.3896484375,10508.419921875,10526.8701171875,10560.009765625,10526.150390625,10453.400390625,10386.2099609375,10397.599609375,10440.150390625,10504.2802734375,10613.1904296875,10629.2197265625,10566.3203125,10718.400390625,10783.4599609375,10791.9501953125,10787.650390625,10830.25,10822.6904296875,10908.849609375,10887.419921875,10947.6201171875,10908.099609375,10924.8095703125,10917.4404296875,10885.669921875,10909.5703125,10904.0,10855.599609375,10945.919921875,10866.5,10907.6904296875,10932.240234375,10986.009765625,10976.9296875,11004.740234375,11103.5,11166.169921875,11156.1396484375,11190.2099609375,11166.240234375,11221.1904296875,11266.1796875,11266.83984375,11336.6396484375,11321.990234375,11347.3896484375,11336.9697265625,11393.6796875,11373.7001953125,11451.7197265625,11576.1796875,11645.080078125,11642.91015625,11620.8896484375,11570.330078125,11463.330078125,11483.240234375,11488.740234375,11485.4599609375,11543.7998046875,11608.48046875,11645.73046875,11573.83984375,11556.75,11590.8095703125,11608.3701171875,11590.8095703125,11641.7001953125,11616.8798828125,11509.9404296875,11473.3203125,11511.8203125,11546.75,11639.7998046875,11635.4697265625,11647.7802734375,11635.0703125,11766.98046875,11937.900390625,11915.6103515625,11939.6201171875,12061.099609375,12082.98046875,12001.5595703125,11969.1396484375,12023.7900390625,11978.9404296875,12018.3798828125,12032.7197265625,12094.8095703125,12019.1904296875,12026.5,12167.4404296875,12035.7099609375,11961.9697265625,11818.759765625,11889.4599609375,12009.26953125,12069.6103515625,12161.73046875,12169.919921875,12006.080078125,12080.7197265625,12107.5595703125,11933.23046875,11494.0302734375,11365.900390625,11399.4599609375,11601.5498046875,11605.1103515625,11712.7099609375,11514.7197265625,11614.25,11693.400390625,11813.5,11806.51953125,11770.2998046875,11700.91015625,11655.3701171875,11785.7802734375,11721.16015625,11615.2802734375,11506.740234375,11468.599609375,11436.9599609375,11184.66015625,11279.8603515625,11368.25,11454.650390625,11471.3603515625,11221.759765625,10907.6201171875,11022.8203125,10845.3896484375,10091.0302734375,10069.419921875,9538.6396484375,9453.98046875,9085.2802734375,8816.8603515625,9025.5498046875,9083.7802734375,9426.4296875,9667.1396484375,9807.900390625,9571.2197265625,9689.6201171875,9726.2001953125,9707.75,9928.16015625,10010.6396484375,10173.259765625,10105.4296875,10147.5595703125,10130.650390625,10366.76953125,10385.7802734375,10554.5498046875,10612.4501953125,10544.7998046875,10256.1103515625,10370.1298828125,10367.9697265625,10407.490234375,10580.25,10656.400390625,10826.259765625,10781.509765625,10756.9501953125,10771.08984375,10778.5302734375,10883.2197265625,10942.1796875,10974.509765625,10861.009765625,10894.169921875,10833.16015625,10753.2099609375,10812.7900390625,10892.759765625,10933.2099609375,10977.5498046875,10812.9404296875,10903.3203125,11023.9296875,11040.6201171875,10921.16015625,10971.400390625,11109.26953125,11182.51953125,11373.9404296875,11418.009765625,11539.0,11600.6103515625,11635.990234375,11738.490234375,11406.5400390625,11436.2802734375,11380.669921875,11525.5,11534.2001953125,11557.26953125,11553.91015625,11618.419921875,11647.8603515625,11608.7998046875,11563.599609375,11622.6396484375,11694.099609375,11857.900390625,11941.83984375,12176.7900390625,12086.1298828125,12222.900390625,12250.4404296875,12109.6103515625,12202.8896484375,12233.990234375,12173.0400390625,12195.7197265625,12205.25,12242.3203125,12389.759765625,12423.01953125,12467.4296875,12618.6904296875,12951.7197265625,12530.740234375,12691.8603515625,12653.4599609375,12642.73046875,12577.8798828125,12762.33984375,12894.240234375,12901.4296875,12786.7998046875,12856.4599609375,12713.6298828125,12758.8701171875,12709.9697265625,12813.9404296875,12974.9599609375,12904.83984375,12764.75,12462.51953125,12629.7802734375,12706.400390625,12768.9599609375,12888.650390625,12759.2998046875,12763.4404296875,12617.3896484375,12772.4296875,12774.9697265625,12645.919921875,12645.9296875,12637.419921875,12592.1796875,12665.740234375,12697.4599609375,12680.41015625,12787.419921875,12989.8095703125,12920.150390625,12858.599609375,12874.6103515625,12786.91015625,12656.2998046875,12487.48046875,12312.9404296875,12282.3603515625,12488.099609375,12483.650390625,12567.8798828125,12644.08984375,12667.509765625,12832.5400390625,12959.150390625,12988.4404296875,12927.08984375,12892.2900390625,12814.16015625,12803.3203125,12889.990234375,12889.8798828125,12854.9697265625,12945.1201171875,12923.5498046875,12867.76953125,12853.759765625,12687.6396484375,12651.349609375,12565.3203125,12641.2802734375,12766.8603515625,12878.009765625,12952.490234375,13053.98046875,13090.650390625,13067.0400390625,13324.8603515625,13221.7998046875,13356.740234375,13723.76953125,13628.419921875,13775.259765625,13702.58984375,13793.5498046875,13918.83984375,13860.9501953125,13763.7900390625,13833.2099609375,13917.650390625,13810.5498046875,13970.509765625,13988.490234375,14010.169921875,14230.0,14251.9404296875,14336.6904296875,14295.150390625,14298.58984375,14238.919921875,14206.330078125,14145.240234375,14326.16015625,14268.1298828125,14273.7900390625,14348.1103515625,14180.7001953125,14256.2900390625,14306.98046875,14363.4501953125,14500.91015625,14485.9697265625,14704.5302734375,14720.25,14913.6396484375,15145.849609375,15059.51953125,15365.1298828125,15425.580078125,15549.5302734375,15550.16015625,15651.2001953125,15987.16015625,15676.4501953125,15716.6396484375,15934.849609375,15775.73046875,15984.8896484375,16006.2099609375,15955.16015625,15711.759765625,15519.849609375,15544.3603515625,15176.5595703125,15546.6904296875,15828.6396484375,15697.75,15805.759765625,16197.4501953125,16366.23046875,16376.91015625,16445.869140625,16320.8798828125,16329.5400390625,16376.7998046875,16190.0400390625,16127.8701171875,15992.3203125,16091.75,15759.580078125,15943.3701171875,15715.0703125,15921.4501953125,15947.2099609375,16241.5498046875,16256.580078125,16253.9296875,16312.98046875,16264.6103515625,16186.4599609375,16065.5,16250.2001953125,15994.9599609375,16010.5400390625,16140.3701171875,16411.669921875,16490.310546875,16529.23046875,16538.279296875,16771.349609375,16786.560546875,16974.279296875,16908.55078125,16872.400390625,16869.779296875,16851.060546875,17028.349609375,17141.25,17231.580078125,17282.759765625,17302.73046875,17117.490234375,17378.359375,17586.01953125,17610.720703125,17646.9296875,17505.51953125,17249.560546875,16968.75,16925.51953125,17032.23046875,17274.23046875,17137.189453125,16515.880859375,15668.330078125,15819.240234375,15554.990234375,15564.9599609375,16112.2099609375,16122.51953125,16136.8603515625,16246.51953125,16444.75,16645.169921875,16591.69921875,16690.0390625,16948.470703125,17098.490234375,17192.91015625,17201.30078125,17225.060546875,17176.08984375,17094.33984375,17082.51953125,17024.5390625,17200.44921875,17279.69921875,17357.369140625,17254.83984375,17384.740234375,17305.490234375,17162.7109375,17127.58984375,17358.5703125,17471.0703125,17531.8203125,17621.150390625,17648.25,17801.189453125,17745.689453125,17783.80078125,17929.220703125,17901.25,17880.900390625,17778.119140625,17843.30078125,17931.470703125,17892.509765625,17878.779296875,17839.580078125,17854.349609375,17724.919921875,17593.599609375,17482.810546875,17603.94921875,17554.9296875,17394.76953125,17252.869140625,17216.580078125,17414.76953125,17256.609375,17489.119140625,17588.640625,17624.8203125,17593.849609375,17505.009765625,17492.44921875,17308.619140625,17221.349609375,17216.33984375,16941.330078125,16841.619140625,16630.990234375,16777.779296875,16426.98046875,16459.130859375,16830.630859375,16821.080078125,17177.390625,17061.099609375,17244.619140625,17384.609375,17463.80078125,17455.919921875,17380.51953125,17534.05078125,17534.380859375,17411.529296875,17175.0390625,17270.279296875,17452.16015625,17463.779296875,17434.01953125,17332.33984375,17279.2890625,17196.7890625,16998.0703125,17130.740234375,17278.109375,17286.890625,17127.859375,16886.5703125,16883.0,16600.51953125,16362.26953125,16488.2109375,16465.5703125,16742.349609375,16589.720703125,16480.98046875,16392.509765625,16426.759765625,16816.939453125,16791.119140625,16936.19921875,16904.4609375,16900.3203125,16879.33984375,16909.3203125,17037.369140625,17081.169921875,17058.0390625,17021.76953125,17094.419921875,17087.890625,17158.279296875,17097.16015625,17323.390625,17433.099609375,17549.08984375,17527.1796875,17479.55078125,17560.51953125,17639.9609375,17705.2890625,17767.060546875,17864.189453125,17828.529296875,17797.900390625,17680.16015625,17686.119140625,17641.7890625,17320.009765625,17369.7890625,17428.609375,17575.91015625,17720.9296875,17650.669921875,17734.189453125,17845.060546875,17880.41015625,17893.2890625,17841.029296875,17754.380859375,17592.869140625,17719.060546875,17744.5390625,17812.810546875,17686.83984375,17805.740234375,17855.779296875,17966.349609375,17975.41015625,18099.7109375,18209.140625,18270.0,18260.23046875,18395.140625,18598.130859375,18395.720703125,18388.16015625,18095.390625,18266.5390625,18348.390625,18356.2109375,18509.0,18512.150390625,18492.810546875,18275.1796875,18212.259765625,18113.939453125,17843.330078125,17890.7109375,17657.970703125,17750.69921875,17955.94921875,18060.330078125,18217.2890625,18258.810546875,18182.73046875,17978.439453125,18109.169921875,18213.30078125,18250.369140625,18196.400390625,18159.509765625,17954.75,17939.529296875,17617.359375,17657.5,17864.109375,17932.150390625,17845.009765625,17581.5703125,17000.0,16944.080078125,17224.73046875,17373.8203125,17287.900390625,17177.26953125,17007.580078125,17172.810546875,17439.91015625,17503.94921875,17512.41015625,17581.619140625,17711.169921875,17695.80078125,17510.150390625,17568.3203125,17572.900390625,17754.099609375,17657.759765625,17484.189453125,17430.130859375,17210.400390625,17273.05078125,17005.83984375,17080.400390625,17353.25,17139.33984375,16958.66015625,16926.33984375,17057.130859375,17159.880859375,17088.759765625,16912.30078125,16677.83984375,16427.3203125,16350.7197265625,16599.83984375,16593.2109375,16531.369140625,16689.98046875,16491.109375,16345.83984375,15891.400390625,16053.75,15943.6201171875,15687.150390625,15943.6904296875,15915.9296875,16174.7802734375,16111.490234375,16061.919921875,16193.919921875,16166.5703125,15987.740234375,16128.2099609375,16075.91015625,16368.9296875,16561.6796875,16718.91015625,16605.4609375,16570.890625,16593.75,16538.669921875,16643.94921875,16581.810546875,16295.0703125,15975.169921875,16025.83984375],\"x\":[\"2019-06-17T00:00:00\",\"2019-06-18T00:00:00\",\"2019-06-19T00:00:00\",\"2019-06-20T00:00:00\",\"2019-06-21T00:00:00\",\"2019-06-24T00:00:00\",\"2019-06-25T00:00:00\",\"2019-06-26T00:00:00\",\"2019-06-27T00:00:00\",\"2019-06-28T00:00:00\",\"2019-07-01T00:00:00\",\"2019-07-02T00:00:00\",\"2019-07-03T00:00:00\",\"2019-07-04T00:00:00\",\"2019-07-05T00:00:00\",\"2019-07-08T00:00:00\",\"2019-07-09T00:00:00\",\"2019-07-10T00:00:00\",\"2019-07-11T00:00:00\",\"2019-07-12T00:00:00\",\"2019-07-15T00:00:00\",\"2019-07-16T00:00:00\",\"2019-07-17T00:00:00\",\"2019-07-18T00:00:00\",\"2019-07-19T00:00:00\",\"2019-07-22T00:00:00\",\"2019-07-23T00:00:00\",\"2019-07-24T00:00:00\",\"2019-07-25T00:00:00\",\"2019-07-26T00:00:00\",\"2019-07-29T00:00:00\",\"2019-07-30T00:00:00\",\"2019-07-31T00:00:00\",\"2019-08-01T00:00:00\",\"2019-08-02T00:00:00\",\"2019-08-05T00:00:00\",\"2019-08-06T00:00:00\",\"2019-08-07T00:00:00\",\"2019-08-08T00:00:00\",\"2019-08-12T00:00:00\",\"2019-08-13T00:00:00\",\"2019-08-14T00:00:00\",\"2019-08-15T00:00:00\",\"2019-08-16T00:00:00\",\"2019-08-19T00:00:00\",\"2019-08-20T00:00:00\",\"2019-08-21T00:00:00\",\"2019-08-22T00:00:00\",\"2019-08-23T00:00:00\",\"2019-08-26T00:00:00\",\"2019-08-27T00:00:00\",\"2019-08-28T00:00:00\",\"2019-08-29T00:00:00\",\"2019-08-30T00:00:00\",\"2019-09-02T00:00:00\",\"2019-09-03T00:00:00\",\"2019-09-04T00:00:00\",\"2019-09-05T00:00:00\",\"2019-09-06T00:00:00\",\"2019-09-10T00:00:00\",\"2019-09-11T00:00:00\",\"2019-09-12T00:00:00\",\"2019-09-16T00:00:00\",\"2019-09-17T00:00:00\",\"2019-09-18T00:00:00\",\"2019-09-19T00:00:00\",\"2019-09-20T00:00:00\",\"2019-09-23T00:00:00\",\"2019-09-24T00:00:00\",\"2019-09-25T00:00:00\",\"2019-09-26T00:00:00\",\"2019-09-27T00:00:00\",\"2019-10-01T00:00:00\",\"2019-10-02T00:00:00\",\"2019-10-03T00:00:00\",\"2019-10-04T00:00:00\",\"2019-10-07T00:00:00\",\"2019-10-08T00:00:00\",\"2019-10-09T00:00:00\",\"2019-10-14T00:00:00\",\"2019-10-15T00:00:00\",\"2019-10-16T00:00:00\",\"2019-10-17T00:00:00\",\"2019-10-18T00:00:00\",\"2019-10-21T00:00:00\",\"2019-10-22T00:00:00\",\"2019-10-23T00:00:00\",\"2019-10-24T00:00:00\",\"2019-10-25T00:00:00\",\"2019-10-28T00:00:00\",\"2019-10-29T00:00:00\",\"2019-10-30T00:00:00\",\"2019-10-31T00:00:00\",\"2019-11-01T00:00:00\",\"2019-11-04T00:00:00\",\"2019-11-05T00:00:00\",\"2019-11-06T00:00:00\",\"2019-11-07T00:00:00\",\"2019-11-08T00:00:00\",\"2019-11-11T00:00:00\",\"2019-11-12T00:00:00\",\"2019-11-13T00:00:00\",\"2019-11-14T00:00:00\",\"2019-11-15T00:00:00\",\"2019-11-18T00:00:00\",\"2019-11-19T00:00:00\",\"2019-11-20T00:00:00\",\"2019-11-21T00:00:00\",\"2019-11-22T00:00:00\",\"2019-11-25T00:00:00\",\"2019-11-26T00:00:00\",\"2019-11-27T00:00:00\",\"2019-11-28T00:00:00\",\"2019-11-29T00:00:00\",\"2019-12-02T00:00:00\",\"2019-12-03T00:00:00\",\"2019-12-04T00:00:00\",\"2019-12-05T00:00:00\",\"2019-12-06T00:00:00\",\"2019-12-09T00:00:00\",\"2019-12-10T00:00:00\",\"2019-12-11T00:00:00\",\"2019-12-12T00:00:00\",\"2019-12-13T00:00:00\",\"2019-12-16T00:00:00\",\"2019-12-17T00:00:00\",\"2019-12-18T00:00:00\",\"2019-12-19T00:00:00\",\"2019-12-20T00:00:00\",\"2019-12-23T00:00:00\",\"2019-12-24T00:00:00\",\"2019-12-25T00:00:00\",\"2019-12-26T00:00:00\",\"2019-12-27T00:00:00\",\"2019-12-30T00:00:00\",\"2019-12-31T00:00:00\",\"2020-01-02T00:00:00\",\"2020-01-03T00:00:00\",\"2020-01-06T00:00:00\",\"2020-01-07T00:00:00\",\"2020-01-08T00:00:00\",\"2020-01-09T00:00:00\",\"2020-01-10T00:00:00\",\"2020-01-13T00:00:00\",\"2020-01-14T00:00:00\",\"2020-01-15T00:00:00\",\"2020-01-16T00:00:00\",\"2020-01-17T00:00:00\",\"2020-01-20T00:00:00\",\"2020-01-30T00:00:00\",\"2020-01-31T00:00:00\",\"2020-02-03T00:00:00\",\"2020-02-04T00:00:00\",\"2020-02-05T00:00:00\",\"2020-02-06T00:00:00\",\"2020-02-07T00:00:00\",\"2020-02-10T00:00:00\",\"2020-02-11T00:00:00\",\"2020-02-12T00:00:00\",\"2020-02-13T00:00:00\",\"2020-02-14T00:00:00\",\"2020-02-17T00:00:00\",\"2020-02-18T00:00:00\",\"2020-02-19T00:00:00\",\"2020-02-20T00:00:00\",\"2020-02-21T00:00:00\",\"2020-02-24T00:00:00\",\"2020-02-25T00:00:00\",\"2020-02-26T00:00:00\",\"2020-02-27T00:00:00\",\"2020-03-02T00:00:00\",\"2020-03-03T00:00:00\",\"2020-03-04T00:00:00\",\"2020-03-05T00:00:00\",\"2020-03-06T00:00:00\",\"2020-03-09T00:00:00\",\"2020-03-10T00:00:00\",\"2020-03-11T00:00:00\",\"2020-03-12T00:00:00\",\"2020-03-13T00:00:00\",\"2020-03-16T00:00:00\",\"2020-03-17T00:00:00\",\"2020-03-18T00:00:00\",\"2020-03-19T00:00:00\",\"2020-03-20T00:00:00\",\"2020-03-23T00:00:00\",\"2020-03-24T00:00:00\",\"2020-03-25T00:00:00\",\"2020-03-26T00:00:00\",\"2020-03-27T00:00:00\",\"2020-03-30T00:00:00\",\"2020-03-31T00:00:00\",\"2020-04-01T00:00:00\",\"2020-04-06T00:00:00\",\"2020-04-07T00:00:00\",\"2020-04-08T00:00:00\",\"2020-04-09T00:00:00\",\"2020-04-10T00:00:00\",\"2020-04-13T00:00:00\",\"2020-04-14T00:00:00\",\"2020-04-15T00:00:00\",\"2020-04-16T00:00:00\",\"2020-04-17T00:00:00\",\"2020-04-20T00:00:00\",\"2020-04-21T00:00:00\",\"2020-04-22T00:00:00\",\"2020-04-23T00:00:00\",\"2020-04-24T00:00:00\",\"2020-04-27T00:00:00\",\"2020-04-28T00:00:00\",\"2020-04-29T00:00:00\",\"2020-04-30T00:00:00\",\"2020-05-04T00:00:00\",\"2020-05-05T00:00:00\",\"2020-05-06T00:00:00\",\"2020-05-07T00:00:00\",\"2020-05-08T00:00:00\",\"2020-05-11T00:00:00\",\"2020-05-12T00:00:00\",\"2020-05-13T00:00:00\",\"2020-05-14T00:00:00\",\"2020-05-15T00:00:00\",\"2020-05-18T00:00:00\",\"2020-05-19T00:00:00\",\"2020-05-20T00:00:00\",\"2020-05-21T00:00:00\",\"2020-05-22T00:00:00\",\"2020-05-25T00:00:00\",\"2020-05-26T00:00:00\",\"2020-05-27T00:00:00\",\"2020-05-28T00:00:00\",\"2020-05-29T00:00:00\",\"2020-06-01T00:00:00\",\"2020-06-02T00:00:00\",\"2020-06-03T00:00:00\",\"2020-06-04T00:00:00\",\"2020-06-05T00:00:00\",\"2020-06-08T00:00:00\",\"2020-06-09T00:00:00\",\"2020-06-10T00:00:00\",\"2020-06-11T00:00:00\",\"2020-06-12T00:00:00\",\"2020-06-15T00:00:00\",\"2020-06-16T00:00:00\",\"2020-06-17T00:00:00\",\"2020-06-18T00:00:00\",\"2020-06-19T00:00:00\",\"2020-06-22T00:00:00\",\"2020-06-23T00:00:00\",\"2020-06-24T00:00:00\",\"2020-06-29T00:00:00\",\"2020-06-30T00:00:00\",\"2020-07-01T00:00:00\",\"2020-07-02T00:00:00\",\"2020-07-03T00:00:00\",\"2020-07-06T00:00:00\",\"2020-07-07T00:00:00\",\"2020-07-08T00:00:00\",\"2020-07-09T00:00:00\",\"2020-07-10T00:00:00\",\"2020-07-13T00:00:00\",\"2020-07-14T00:00:00\",\"2020-07-15T00:00:00\",\"2020-07-16T00:00:00\",\"2020-07-17T00:00:00\",\"2020-07-20T00:00:00\",\"2020-07-21T00:00:00\",\"2020-07-22T00:00:00\",\"2020-07-23T00:00:00\",\"2020-07-24T00:00:00\",\"2020-07-27T00:00:00\",\"2020-07-28T00:00:00\",\"2020-07-29T00:00:00\",\"2020-07-30T00:00:00\",\"2020-07-31T00:00:00\",\"2020-08-03T00:00:00\",\"2020-08-04T00:00:00\",\"2020-08-05T00:00:00\",\"2020-08-06T00:00:00\",\"2020-08-07T00:00:00\",\"2020-08-10T00:00:00\",\"2020-08-11T00:00:00\",\"2020-08-12T00:00:00\",\"2020-08-13T00:00:00\",\"2020-08-14T00:00:00\",\"2020-08-17T00:00:00\",\"2020-08-18T00:00:00\",\"2020-08-19T00:00:00\",\"2020-08-20T00:00:00\",\"2020-08-21T00:00:00\",\"2020-08-24T00:00:00\",\"2020-08-25T00:00:00\",\"2020-08-26T00:00:00\",\"2020-08-27T00:00:00\",\"2020-08-28T00:00:00\",\"2020-08-31T00:00:00\",\"2020-09-01T00:00:00\",\"2020-09-02T00:00:00\",\"2020-09-03T00:00:00\",\"2020-09-04T00:00:00\",\"2020-09-07T00:00:00\",\"2020-09-08T00:00:00\",\"2020-09-09T00:00:00\",\"2020-09-10T00:00:00\",\"2020-09-11T00:00:00\",\"2020-09-14T00:00:00\",\"2020-09-15T00:00:00\",\"2020-09-16T00:00:00\",\"2020-09-17T00:00:00\",\"2020-09-18T00:00:00\",\"2020-09-21T00:00:00\",\"2020-09-22T00:00:00\",\"2020-09-23T00:00:00\",\"2020-09-24T00:00:00\",\"2020-09-25T00:00:00\",\"2020-09-28T00:00:00\",\"2020-09-29T00:00:00\",\"2020-09-30T00:00:00\",\"2020-10-05T00:00:00\",\"2020-10-06T00:00:00\",\"2020-10-07T00:00:00\",\"2020-10-08T00:00:00\",\"2020-10-12T00:00:00\",\"2020-10-13T00:00:00\",\"2020-10-14T00:00:00\",\"2020-10-15T00:00:00\",\"2020-10-16T00:00:00\",\"2020-10-19T00:00:00\",\"2020-10-20T00:00:00\",\"2020-10-21T00:00:00\",\"2020-10-22T00:00:00\",\"2020-10-23T00:00:00\",\"2020-10-26T00:00:00\",\"2020-10-27T00:00:00\",\"2020-10-28T00:00:00\",\"2020-10-29T00:00:00\",\"2020-10-30T00:00:00\",\"2020-11-02T00:00:00\",\"2020-11-03T00:00:00\",\"2020-11-04T00:00:00\",\"2020-11-05T00:00:00\",\"2020-11-06T00:00:00\",\"2020-11-09T00:00:00\",\"2020-11-10T00:00:00\",\"2020-11-11T00:00:00\",\"2020-11-12T00:00:00\",\"2020-11-13T00:00:00\",\"2020-11-16T00:00:00\",\"2020-11-17T00:00:00\",\"2020-11-18T00:00:00\",\"2020-11-19T00:00:00\",\"2020-11-20T00:00:00\",\"2020-11-23T00:00:00\",\"2020-11-24T00:00:00\",\"2020-11-25T00:00:00\",\"2020-11-26T00:00:00\",\"2020-11-27T00:00:00\",\"2020-11-30T00:00:00\",\"2020-12-01T00:00:00\",\"2020-12-02T00:00:00\",\"2020-12-03T00:00:00\",\"2020-12-04T00:00:00\",\"2020-12-07T00:00:00\",\"2020-12-08T00:00:00\",\"2020-12-09T00:00:00\",\"2020-12-10T00:00:00\",\"2020-12-11T00:00:00\",\"2020-12-14T00:00:00\",\"2020-12-15T00:00:00\",\"2020-12-16T00:00:00\",\"2020-12-17T00:00:00\",\"2020-12-18T00:00:00\",\"2020-12-21T00:00:00\",\"2020-12-22T00:00:00\",\"2020-12-23T00:00:00\",\"2020-12-24T00:00:00\",\"2020-12-25T00:00:00\",\"2020-12-28T00:00:00\",\"2020-12-29T00:00:00\",\"2020-12-30T00:00:00\",\"2020-12-31T00:00:00\",\"2021-01-04T00:00:00\",\"2021-01-05T00:00:00\",\"2021-01-06T00:00:00\",\"2021-01-07T00:00:00\",\"2021-01-08T00:00:00\",\"2021-01-11T00:00:00\",\"2021-01-12T00:00:00\",\"2021-01-13T00:00:00\",\"2021-01-14T00:00:00\",\"2021-01-15T00:00:00\",\"2021-01-18T00:00:00\",\"2021-01-19T00:00:00\",\"2021-01-20T00:00:00\",\"2021-01-21T00:00:00\",\"2021-01-22T00:00:00\",\"2021-01-25T00:00:00\",\"2021-01-26T00:00:00\",\"2021-01-27T00:00:00\",\"2021-01-28T00:00:00\",\"2021-01-29T00:00:00\",\"2021-02-01T00:00:00\",\"2021-02-02T00:00:00\",\"2021-02-03T00:00:00\",\"2021-02-04T00:00:00\",\"2021-02-05T00:00:00\",\"2021-02-17T00:00:00\",\"2021-02-18T00:00:00\",\"2021-02-19T00:00:00\",\"2021-02-22T00:00:00\",\"2021-02-23T00:00:00\",\"2021-02-24T00:00:00\",\"2021-02-25T00:00:00\",\"2021-02-26T00:00:00\",\"2021-03-02T00:00:00\",\"2021-03-03T00:00:00\",\"2021-03-04T00:00:00\",\"2021-03-05T00:00:00\",\"2021-03-08T00:00:00\",\"2021-03-09T00:00:00\",\"2021-03-10T00:00:00\",\"2021-03-11T00:00:00\",\"2021-03-12T00:00:00\",\"2021-03-15T00:00:00\",\"2021-03-16T00:00:00\",\"2021-03-17T00:00:00\",\"2021-03-18T00:00:00\",\"2021-03-19T00:00:00\",\"2021-03-22T00:00:00\",\"2021-03-23T00:00:00\",\"2021-03-24T00:00:00\",\"2021-03-25T00:00:00\",\"2021-03-26T00:00:00\",\"2021-03-29T00:00:00\",\"2021-03-30T00:00:00\",\"2021-03-31T00:00:00\",\"2021-04-01T00:00:00\",\"2021-04-07T00:00:00\",\"2021-04-08T00:00:00\",\"2021-04-09T00:00:00\",\"2021-04-12T00:00:00\",\"2021-04-13T00:00:00\",\"2021-04-14T00:00:00\",\"2021-04-15T00:00:00\",\"2021-04-16T00:00:00\",\"2021-04-19T00:00:00\",\"2021-04-20T00:00:00\",\"2021-04-21T00:00:00\",\"2021-04-22T00:00:00\",\"2021-04-23T00:00:00\",\"2021-04-26T00:00:00\",\"2021-04-27T00:00:00\",\"2021-04-28T00:00:00\",\"2021-04-29T00:00:00\",\"2021-05-03T00:00:00\",\"2021-05-04T00:00:00\",\"2021-05-05T00:00:00\",\"2021-05-06T00:00:00\",\"2021-05-07T00:00:00\",\"2021-05-10T00:00:00\",\"2021-05-11T00:00:00\",\"2021-05-12T00:00:00\",\"2021-05-13T00:00:00\",\"2021-05-14T00:00:00\",\"2021-05-17T00:00:00\",\"2021-05-18T00:00:00\",\"2021-05-19T00:00:00\",\"2021-05-20T00:00:00\",\"2021-05-21T00:00:00\",\"2021-05-24T00:00:00\",\"2021-05-25T00:00:00\",\"2021-05-26T00:00:00\",\"2021-05-27T00:00:00\",\"2021-05-28T00:00:00\",\"2021-05-31T00:00:00\",\"2021-06-01T00:00:00\",\"2021-06-02T00:00:00\",\"2021-06-03T00:00:00\",\"2021-06-04T00:00:00\",\"2021-06-07T00:00:00\",\"2021-06-08T00:00:00\",\"2021-06-09T00:00:00\",\"2021-06-10T00:00:00\",\"2021-06-11T00:00:00\",\"2021-06-15T00:00:00\",\"2021-06-16T00:00:00\",\"2021-06-17T00:00:00\",\"2021-06-18T00:00:00\",\"2021-06-21T00:00:00\",\"2021-06-22T00:00:00\",\"2021-06-23T00:00:00\",\"2021-06-24T00:00:00\",\"2021-06-25T00:00:00\",\"2021-06-28T00:00:00\",\"2021-06-29T00:00:00\",\"2021-06-30T00:00:00\",\"2021-07-01T00:00:00\",\"2021-07-02T00:00:00\",\"2021-07-05T00:00:00\",\"2021-07-06T00:00:00\",\"2021-07-07T00:00:00\",\"2021-07-08T00:00:00\",\"2021-07-09T00:00:00\",\"2021-07-12T00:00:00\",\"2021-07-13T00:00:00\",\"2021-07-14T00:00:00\",\"2021-07-15T00:00:00\",\"2021-07-16T00:00:00\",\"2021-07-19T00:00:00\",\"2021-07-20T00:00:00\",\"2021-07-21T00:00:00\",\"2021-07-22T00:00:00\",\"2021-07-23T00:00:00\",\"2021-07-26T00:00:00\",\"2021-07-27T00:00:00\",\"2021-07-28T00:00:00\",\"2021-07-29T00:00:00\",\"2021-07-30T00:00:00\",\"2021-08-02T00:00:00\",\"2021-08-03T00:00:00\",\"2021-08-04T00:00:00\",\"2021-08-05T00:00:00\",\"2021-08-06T00:00:00\",\"2021-08-09T00:00:00\",\"2021-08-10T00:00:00\",\"2021-08-11T00:00:00\",\"2021-08-12T00:00:00\",\"2021-08-13T00:00:00\",\"2021-08-16T00:00:00\",\"2021-08-17T00:00:00\",\"2021-08-18T00:00:00\",\"2021-08-19T00:00:00\",\"2021-08-20T00:00:00\",\"2021-08-23T00:00:00\",\"2021-08-24T00:00:00\",\"2021-08-25T00:00:00\",\"2021-08-26T00:00:00\",\"2021-08-27T00:00:00\",\"2021-08-30T00:00:00\",\"2021-08-31T00:00:00\",\"2021-09-01T00:00:00\",\"2021-09-02T00:00:00\",\"2021-09-03T00:00:00\",\"2021-09-06T00:00:00\",\"2021-09-07T00:00:00\",\"2021-09-08T00:00:00\",\"2021-09-09T00:00:00\",\"2021-09-10T00:00:00\",\"2021-09-13T00:00:00\",\"2021-09-14T00:00:00\",\"2021-09-15T00:00:00\",\"2021-09-16T00:00:00\",\"2021-09-17T00:00:00\",\"2021-09-22T00:00:00\",\"2021-09-23T00:00:00\",\"2021-09-24T00:00:00\",\"2021-09-27T00:00:00\",\"2021-09-28T00:00:00\",\"2021-09-29T00:00:00\",\"2021-09-30T00:00:00\",\"2021-10-01T00:00:00\",\"2021-10-04T00:00:00\",\"2021-10-05T00:00:00\",\"2021-10-06T00:00:00\",\"2021-10-07T00:00:00\",\"2021-10-08T00:00:00\",\"2021-10-12T00:00:00\",\"2021-10-13T00:00:00\",\"2021-10-14T00:00:00\",\"2021-10-15T00:00:00\",\"2021-10-18T00:00:00\",\"2021-10-19T00:00:00\",\"2021-10-20T00:00:00\",\"2021-10-21T00:00:00\",\"2021-10-22T00:00:00\",\"2021-10-25T00:00:00\",\"2021-10-26T00:00:00\",\"2021-10-27T00:00:00\",\"2021-10-28T00:00:00\",\"2021-10-29T00:00:00\",\"2021-11-01T00:00:00\",\"2021-11-02T00:00:00\",\"2021-11-03T00:00:00\",\"2021-11-04T00:00:00\",\"2021-11-05T00:00:00\",\"2021-11-08T00:00:00\",\"2021-11-09T00:00:00\",\"2021-11-10T00:00:00\",\"2021-11-11T00:00:00\",\"2021-11-12T00:00:00\",\"2021-11-15T00:00:00\",\"2021-11-16T00:00:00\",\"2021-11-17T00:00:00\",\"2021-11-18T00:00:00\",\"2021-11-19T00:00:00\",\"2021-11-22T00:00:00\",\"2021-11-23T00:00:00\",\"2021-11-24T00:00:00\",\"2021-11-25T00:00:00\",\"2021-11-26T00:00:00\",\"2021-11-29T00:00:00\",\"2021-11-30T00:00:00\",\"2021-12-01T00:00:00\",\"2021-12-02T00:00:00\",\"2021-12-03T00:00:00\",\"2021-12-06T00:00:00\",\"2021-12-07T00:00:00\",\"2021-12-08T00:00:00\",\"2021-12-09T00:00:00\",\"2021-12-10T00:00:00\",\"2021-12-13T00:00:00\",\"2021-12-14T00:00:00\",\"2021-12-15T00:00:00\",\"2021-12-16T00:00:00\",\"2021-12-17T00:00:00\",\"2021-12-20T00:00:00\",\"2021-12-21T00:00:00\",\"2021-12-22T00:00:00\",\"2021-12-23T00:00:00\",\"2021-12-24T00:00:00\",\"2021-12-27T00:00:00\",\"2021-12-28T00:00:00\",\"2021-12-29T00:00:00\",\"2021-12-30T00:00:00\",\"2022-01-03T00:00:00\",\"2022-01-04T00:00:00\",\"2022-01-05T00:00:00\",\"2022-01-06T00:00:00\",\"2022-01-07T00:00:00\",\"2022-01-10T00:00:00\",\"2022-01-11T00:00:00\",\"2022-01-12T00:00:00\",\"2022-01-13T00:00:00\",\"2022-01-14T00:00:00\",\"2022-01-17T00:00:00\",\"2022-01-18T00:00:00\",\"2022-01-19T00:00:00\",\"2022-01-20T00:00:00\",\"2022-01-21T00:00:00\",\"2022-01-24T00:00:00\",\"2022-01-25T00:00:00\",\"2022-01-26T00:00:00\",\"2022-02-07T00:00:00\",\"2022-02-08T00:00:00\",\"2022-02-09T00:00:00\",\"2022-02-10T00:00:00\",\"2022-02-11T00:00:00\",\"2022-02-14T00:00:00\",\"2022-02-15T00:00:00\",\"2022-02-16T00:00:00\",\"2022-02-17T00:00:00\",\"2022-02-18T00:00:00\",\"2022-02-21T00:00:00\",\"2022-02-22T00:00:00\",\"2022-02-23T00:00:00\",\"2022-02-24T00:00:00\",\"2022-02-25T00:00:00\",\"2022-03-01T00:00:00\",\"2022-03-02T00:00:00\",\"2022-03-03T00:00:00\",\"2022-03-04T00:00:00\",\"2022-03-07T00:00:00\",\"2022-03-08T00:00:00\",\"2022-03-09T00:00:00\",\"2022-03-10T00:00:00\",\"2022-03-11T00:00:00\",\"2022-03-14T00:00:00\",\"2022-03-15T00:00:00\",\"2022-03-16T00:00:00\",\"2022-03-17T00:00:00\",\"2022-03-18T00:00:00\",\"2022-03-21T00:00:00\",\"2022-03-22T00:00:00\",\"2022-03-23T00:00:00\",\"2022-03-24T00:00:00\",\"2022-03-25T00:00:00\",\"2022-03-28T00:00:00\",\"2022-03-29T00:00:00\",\"2022-03-30T00:00:00\",\"2022-03-31T00:00:00\",\"2022-04-01T00:00:00\",\"2022-04-06T00:00:00\",\"2022-04-07T00:00:00\",\"2022-04-08T00:00:00\",\"2022-04-11T00:00:00\",\"2022-04-12T00:00:00\",\"2022-04-13T00:00:00\",\"2022-04-14T00:00:00\",\"2022-04-15T00:00:00\",\"2022-04-18T00:00:00\",\"2022-04-19T00:00:00\",\"2022-04-20T00:00:00\",\"2022-04-21T00:00:00\",\"2022-04-22T00:00:00\",\"2022-04-25T00:00:00\",\"2022-04-26T00:00:00\",\"2022-04-27T00:00:00\",\"2022-04-28T00:00:00\",\"2022-04-29T00:00:00\",\"2022-05-03T00:00:00\",\"2022-05-04T00:00:00\",\"2022-05-05T00:00:00\",\"2022-05-06T00:00:00\",\"2022-05-09T00:00:00\",\"2022-05-10T00:00:00\",\"2022-05-11T00:00:00\",\"2022-05-12T00:00:00\",\"2022-05-13T00:00:00\",\"2022-05-16T00:00:00\",\"2022-05-17T00:00:00\",\"2022-05-18T00:00:00\",\"2022-05-19T00:00:00\",\"2022-05-20T00:00:00\",\"2022-05-23T00:00:00\",\"2022-05-24T00:00:00\",\"2022-05-25T00:00:00\",\"2022-05-26T00:00:00\",\"2022-05-27T00:00:00\",\"2022-05-30T00:00:00\",\"2022-05-31T00:00:00\",\"2022-06-01T00:00:00\",\"2022-06-02T00:00:00\",\"2022-06-06T00:00:00\",\"2022-06-07T00:00:00\",\"2022-06-08T00:00:00\",\"2022-06-09T00:00:00\",\"2022-06-10T00:00:00\",\"2022-06-13T00:00:00\",\"2022-06-14T00:00:00\",\"2022-06-15T00:00:00\"],\"type\":\"candlestick\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"rangeslider\":{\"visible\":false}},\"title\":{\"text\":\"\\u53f0\\u7063\\u52a0\\u6b0a\\u6307\\u6578\\u8d70\\u52e2\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('1c55714d-cc2a-43c4-a332-0d6afcdcc557');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "figure = go.Figure(data=[go.Candlestick(x=data[\"Date\"],\n",
    "                                        open=data[\"Open\"],\n",
    "                                        high=data[\"High\"],\n",
    "                                        low=data[\"Low\"],\n",
    "                                        close=data[\"Close\"])])\n",
    "figure.update_layout(title = \"台灣加權指數走勢\", \n",
    "                     xaxis_rangeslider_visible=False)\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d163c83c",
   "metadata": {},
   "source": [
    "分析時間序列數據的最佳方法之一是創建互動式圖表，您可以在其中手動選擇輸出圖表本身的時間間隔。\n",
    "一種方法是在圖表下方添加一個滑動區塊，並在圖表上方添加按鈕來控制時間的間隔。 以下是創建互動式K線圖表的方法，您可以在其中選擇輸出本身的時間間隔："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99172fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "close": [
          10530.5400390625,
          10566.740234375,
          10775.33984375,
          10785.009765625,
          10803.76953125,
          10779.4501953125,
          10706.7197265625,
          10652.5498046875,
          10773.900390625,
          10730.830078125,
          10895.4599609375,
          10865.1201171875,
          10743.76953125,
          10775.900390625,
          10785.73046875,
          10751.2197265625,
          10702.7802734375,
          10798.48046875,
          10843.419921875,
          10824.349609375,
          10876.4296875,
          10886.0498046875,
          10828.48046875,
          10799.2802734375,
          10873.1904296875,
          10944.5302734375,
          10947.259765625,
          10935.759765625,
          10941.41015625,
          10891.98046875,
          10885.73046875,
          10830.900390625,
          10823.8095703125,
          10731.75,
          10549.0400390625,
          10423.41015625,
          10394.75,
          10386.1796875,
          10494.490234375,
          10472.3603515625,
          10362.66015625,
          10427.73046875,
          10327.1298828125,
          10420.8896484375,
          10488.75,
          10522.5,
          10525.7998046875,
          10529.7802734375,
          10538.1103515625,
          10354.5703125,
          10387.23046875,
          10434.2900390625,
          10462.4296875,
          10618.0498046875,
          10634.849609375,
          10558.2099609375,
          10657.3095703125,
          10756.9296875,
          10780.6396484375,
          10753.580078125,
          10790.349609375,
          10827.5498046875,
          10898.1298828125,
          10874.5,
          10929.4501953125,
          10894.7001953125,
          10929.6904296875,
          10919.01953125,
          10918.009765625,
          10873.6904296875,
          10871.990234375,
          10829.6796875,
          10967.650390625,
          10947.8798828125,
          10875.91015625,
          10894.48046875,
          10935.0595703125,
          11017.3095703125,
          10889.9599609375,
          11066.9501953125,
          11111.7998046875,
          11162.830078125,
          11186.8798828125,
          11180.2197265625,
          11184.150390625,
          11271.25,
          11239.669921875,
          11320.1396484375,
          11296.1201171875,
          11315.01953125,
          11333.8701171875,
          11380.2802734375,
          11358.7099609375,
          11399.5302734375,
          11556.849609375,
          11644.0302734375,
          11653.0703125,
          11606.5595703125,
          11579.5400390625,
          11427.2802734375,
          11520.3701171875,
          11467.830078125,
          11450.419921875,
          11525.599609375,
          11599.7802734375,
          11656.400390625,
          11631.2001953125,
          11558.26953125,
          11566.7998046875,
          11561.580078125,
          11576.8203125,
          11647.4599609375,
          11617.080078125,
          11489.5703125,
          11502.830078125,
          11531.580078125,
          11510.4697265625,
          11594.650390625,
          11609.6396484375,
          11660.76953125,
          11627.83984375,
          11700.76953125,
          11836.419921875,
          11927.73046875,
          11939.76953125,
          12097.009765625,
          12122.4501953125,
          12018.900390625,
          11959.080078125,
          12022.23046875,
          11976.3798828125,
          12008.1298828125,
          12001.009765625,
          12091.58984375,
          12053.3701171875,
          11997.1396484375,
          12100.48046875,
          12110.4296875,
          11953.3603515625,
          11880.3203125,
          11817.099609375,
          11970.6298828125,
          12024.650390625,
          12113.419921875,
          12179.8095703125,
          12091.8798828125,
          12066.9296875,
          12090.2900390625,
          12118.7099609375,
          11421.740234375,
          11495.099609375,
          11354.919921875,
          11555.919921875,
          11573.6201171875,
          11749.6796875,
          11612.8095703125,
          11574.0703125,
          11664.0400390625,
          11774.1904296875,
          11791.7802734375,
          11815.7001953125,
          11763.509765625,
          11648.98046875,
          11758.83984375,
          11725.08984375,
          11686.349609375,
          11534.8701171875,
          11540.23046875,
          11433.6201171875,
          11292.169921875,
          11170.4599609375,
          11327.7197265625,
          11392.349609375,
          11514.8203125,
          11321.8095703125,
          10977.6396484375,
          11003.5400390625,
          10893.75,
          10422.3203125,
          10128.8701171875,
          9717.76953125,
          9439.6298828125,
          9218.669921875,
          8681.33984375,
          9234.08984375,
          8890.0302734375,
          9285.6201171875,
          9644.75,
          9736.3603515625,
          9698.919921875,
          9629.4296875,
          9708.0595703125,
          9663.6298828125,
          9818.740234375,
          9996.3896484375,
          10137.4697265625,
          10119.4296875,
          10157.6103515625,
          10099.2197265625,
          10332.9404296875,
          10447.2099609375,
          10375.48046875,
          10597.0400390625,
          10586.7099609375,
          10288.419921875,
          10307.740234375,
          10366.509765625,
          10347.3603515625,
          10567.26953125,
          10616.0595703125,
          10772.2197265625,
          10992.1396484375,
          10720.48046875,
          10774.6103515625,
          10774.98046875,
          10842.919921875,
          10901.419921875,
          11013.259765625,
          10879.4697265625,
          10938.26953125,
          10780.8798828125,
          10814.919921875,
          10740.5498046875,
          10860.4404296875,
          10907.7998046875,
          11008.3095703125,
          10811.150390625,
          10871.1796875,
          10997.2099609375,
          11014.66015625,
          10944.1904296875,
          10942.16015625,
          11079.01953125,
          11127.9296875,
          11320.16015625,
          11393.23046875,
          11479.400390625,
          11610.3203125,
          11637.1103515625,
          11720.16015625,
          11535.76953125,
          11429.9404296875,
          11306.259765625,
          11511.6396484375,
          11534.58984375,
          11548.330078125,
          11549.8603515625,
          11572.9296875,
          11612.3603515625,
          11660.669921875,
          11542.6201171875,
          11621.240234375,
          11703.419921875,
          11805.1396484375,
          11909.16015625,
          12116.7001953125,
          12092.9697265625,
          12170.1904296875,
          12192.6904296875,
          12073.6796875,
          12211.5595703125,
          12209.009765625,
          12202.849609375,
          12157.740234375,
          12181.5595703125,
          12174.5400390625,
          12397.5498046875,
          12473.26953125,
          12413.0400390625,
          12304.0400390625,
          12588.2998046875,
          12586.73046875,
          12540.9697265625,
          12722.919921875,
          12664.7998046875,
          12513.0302734375,
          12709.919921875,
          12802.2998046875,
          12913.5,
          12828.8701171875,
          12894,
          12780.1904296875,
          12670.349609375,
          12763.1298828125,
          12795.4599609375,
          12956.1103515625,
          12872.1396484375,
          12778.6396484375,
          12362.6396484375,
          12607.83984375,
          12647.1298828125,
          12758.25,
          12833.2900390625,
          12797.3095703125,
          12728.849609375,
          12591.4501953125,
          12703.2802734375,
          12699.5,
          12757.9697265625,
          12637.9501953125,
          12601.400390625,
          12663.5595703125,
          12608.580078125,
          12691.75,
          12675.9501953125,
          12787.8203125,
          12845.650390625,
          12976.759765625,
          12872.740234375,
          12875.6201171875,
          12795.1201171875,
          12645.509765625,
          12583.8798828125,
          12264.3798828125,
          12232.91015625,
          12462.759765625,
          12467.73046875,
          12515.6103515625,
          12548.2802734375,
          12704.23046875,
          12746.3701171875,
          12887.1904296875,
          12955.91015625,
          12947.1298828125,
          12919.3095703125,
          12827.8203125,
          12750.3701171875,
          12908.33984375,
          12862.3701171875,
          12877.25,
          12917.0302734375,
          12898.8203125,
          12909.0302734375,
          12875.009765625,
          12793.75,
          12662.91015625,
          12546.33984375,
          12591.3095703125,
          12736.009765625,
          12867.900390625,
          12918.7998046875,
          12973.5302734375,
          13127.4697265625,
          13081.7197265625,
          13262.1904296875,
          13221.7802734375,
          13273.330078125,
          13551.830078125,
          13593.009765625,
          13773.2900390625,
          13722.4296875,
          13716.4404296875,
          13878.009765625,
          13807.1298828125,
          13738.830078125,
          13845.66015625,
          13867.08984375,
          13722.8896484375,
          13885.669921875,
          13989.1396484375,
          13977.08984375,
          14132.4404296875,
          14256.599609375,
          14360.400390625,
          14390.1396484375,
          14249.490234375,
          14261.6904296875,
          14211.0498046875,
          14068.51953125,
          14304.4599609375,
          14258.9296875,
          14249.9599609375,
          14384.9599609375,
          14177.4599609375,
          14223.08984375,
          14280.2802734375,
          14331.419921875,
          14483.0703125,
          14472.0498046875,
          14687.7001953125,
          14732.5302734375,
          14902.0302734375,
          15000.0302734375,
          14983.1298828125,
          15214,
          15463.9501953125,
          15557.2998046875,
          15500.7001953125,
          15769.98046875,
          15707.1904296875,
          15616.3896484375,
          15612,
          15877.3701171875,
          15806.1796875,
          16153.76953125,
          16019.0302734375,
          15946.5400390625,
          15658.849609375,
          15701.4501953125,
          15415.8798828125,
          15138.3095703125,
          15410.08984375,
          15760.0498046875,
          15771.3203125,
          15706.2197265625,
          15802.400390625,
          16362.2900390625,
          16424.509765625,
          16341.3798828125,
          16410.16015625,
          16443.400390625,
          16212.5302734375,
          16452.1796875,
          15953.7998046875,
          15946.8798828125,
          16211.73046875,
          15906.41015625,
          15855.23046875,
          15820.1103515625,
          15853.08984375,
          15911.669921875,
          16179.5595703125,
          16255.1796875,
          16249.330078125,
          16313.16015625,
          16215.8203125,
          16287.83984375,
          16070.240234375,
          16189.2197265625,
          16177.58984375,
          16032.1201171875,
          16060.1396484375,
          16305.8798828125,
          16475.970703125,
          16554.900390625,
          16431.130859375,
          16571.279296875,
          16815.359375,
          16926.439453125,
          16854.099609375,
          16859.69921875,
          16824.91015625,
          16865.970703125,
          17076.73046875,
          17158.810546875,
          17263.279296875,
          17323.869140625,
          17202.109375,
          17096.970703125,
          17300.26953125,
          17572.2890625,
          17595.900390625,
          17567.529296875,
          17566.66015625,
          17222.349609375,
          16933.779296875,
          16843.439453125,
          16994.359375,
          17285,
          17235.609375,
          16583.130859375,
          15902.3701171875,
          15670.099609375,
          15827.08984375,
          15353.8896484375,
          16145.98046875,
          16132.66015625,
          16042.3603515625,
          16302.0595703125,
          16338.2900390625,
          16595.669921875,
          16643.689453125,
          16601.609375,
          16870.859375,
          17068.4296875,
          17162.380859375,
          17165.0390625,
          17246.16015625,
          17147.41015625,
          17083.91015625,
          17076.2109375,
          16966.220703125,
          17159.220703125,
          17213.51953125,
          17371.2890625,
          17307.859375,
          17390.609375,
          17318.5390625,
          17062.98046875,
          17075.55078125,
          17336.7109375,
          17407.9609375,
          17502.990234375,
          17590.970703125,
          17598.189453125,
          17755.4609375,
          17713.939453125,
          17710.150390625,
          17919.330078125,
          17913.0703125,
          17850.689453125,
          17866.08984375,
          17661.48046875,
          17814.330078125,
          17847.51953125,
          17845.75,
          18034.189453125,
          17895.25,
          17789.25,
          17528.740234375,
          17458.7890625,
          17572.330078125,
          17572.919921875,
          17403.560546875,
          17269.869140625,
          17135.220703125,
          17402.810546875,
          17247.41015625,
          17503.279296875,
          17553.759765625,
          17623.890625,
          17603.119140625,
          17526.279296875,
          17485.150390625,
          17323.640625,
          17227.1796875,
          17219.939453125,
          16982.109375,
          16858.76953125,
          16661.359375,
          16826.26953125,
          16375.400390625,
          16341.9404296875,
          16741.83984375,
          16818.73046875,
          17045.859375,
          17066.9609375,
          17209.9296875,
          17396.51953125,
          17490.2890625,
          17473.990234375,
          17319.759765625,
          17516.919921875,
          17495.30078125,
          17428.869140625,
          17270.490234375,
          17304.330078125,
          17474.5703125,
          17446.310546875,
          17434.900390625,
          17354,
          17278.69921875,
          17276.7890625,
          16925.8203125,
          17078.220703125,
          17260.189453125,
          17313.76953125,
          17181.439453125,
          16855.4609375,
          16934.76953125,
          16570.890625,
          16408.349609375,
          16460.75,
          16393.16015625,
          16713.859375,
          16640.4296875,
          16462.83984375,
          16347.990234375,
          16387.279296875,
          16781.189453125,
          16705.4609375,
          16900.669921875,
          16887.8203125,
          16889.509765625,
          16888.740234375,
          16894.240234375,
          17034.33984375,
          17074.55078125,
          17041.630859375,
          16987.41015625,
          17068.240234375,
          17065.970703125,
          17122.16015625,
          17078.859375,
          17296.900390625,
          17415.30078125,
          17541.359375,
          17559.650390625,
          17452.51953125,
          17518.130859375,
          17634.470703125,
          17693.130859375,
          17764.0390625,
          17841.369140625,
          17818.310546875,
          17803.5390625,
          17666.119140625,
          17642.51953125,
          17654.189453125,
          17369.390625,
          17328.08984375,
          17427.759765625,
          17585.990234375,
          17724.880859375,
          17697.140625,
          17688.2109375,
          17796.919921875,
          17832.419921875,
          17914.119140625,
          17826.259765625,
          17767.599609375,
          17599.369140625,
          17660.099609375,
          17785.740234375,
          17812.58984375,
          17669.109375,
          17789.26953125,
          17826.830078125,
          17946.66015625,
          17961.640625,
          18048.939453125,
          18196.810546875,
          18248.279296875,
          18218.83984375,
          18270.509765625,
          18526.349609375,
          18499.9609375,
          18367.919921875,
          18169.759765625,
          18239.380859375,
          18288.2109375,
          18375.400390625,
          18436.9296875,
          18403.330078125,
          18525.439453125,
          18378.640625,
          18227.4609375,
          18218.279296875,
          17899.30078125,
          17989.0390625,
          17701.119140625,
          17674.400390625,
          17900.30078125,
          17966.560546875,
          18151.759765625,
          18338.05078125,
          18310.939453125,
          17997.669921875,
          17951.810546875,
          18231.470703125,
          18268.5703125,
          18232.349609375,
          18221.490234375,
          17969.2890625,
          18055.73046875,
          17594.55078125,
          17652.1796875,
          17898.25,
          17867.599609375,
          17934.400390625,
          17736.51953125,
          17178.689453125,
          16825.25,
          17015.359375,
          17433.19921875,
          17264.740234375,
          17263.0390625,
          16926.060546875,
          16940.830078125,
          17448.220703125,
          17456.51953125,
          17560.359375,
          17559.7109375,
          17731.369140625,
          17699.060546875,
          17676.94921875,
          17520.009765625,
          17548.66015625,
          17740.560546875,
          17693.470703125,
          17625.58984375,
          17522.5,
          17178.630859375,
          17284.5390625,
          17048.369140625,
          16990.91015625,
          17301.650390625,
          17245.650390625,
          17004.1796875,
          16898.869140625,
          16993.400390625,
          17148.880859375,
          17127.94921875,
          17025.08984375,
          16620.900390625,
          16644.7890625,
          16303.349609375,
          16419.380859375,
          16592.1796875,
          16498.900390625,
          16565.830078125,
          16696.119140625,
          16408.19921875,
          16048.919921875,
          16061.7001953125,
          16006.25,
          15616.6796875,
          15832.5400390625,
          15901.0400390625,
          16056.08984375,
          16296.8603515625,
          16020.3203125,
          16144.849609375,
          16156.41015625,
          15963.6298828125,
          16104.0302734375,
          15968.830078125,
          16266.2197265625,
          16610.619140625,
          16807.76953125,
          16675.08984375,
          16552.5703125,
          16605.9609375,
          16512.880859375,
          16670.509765625,
          16621.33984375,
          16460.119140625,
          16070.98046875,
          16047.3701171875,
          15999.25
         ],
         "high": [
          10562.9697265625,
          10573.76953125,
          10778.6396484375,
          10799.1396484375,
          10840.2900390625,
          10786.330078125,
          10787.6201171875,
          10690.98046875,
          10800.2900390625,
          10788.080078125,
          10914.849609375,
          10888.2099609375,
          10812.650390625,
          10791.2802734375,
          10812.3798828125,
          10781.1103515625,
          10733.5703125,
          10810.16015625,
          10856.509765625,
          10867.58984375,
          10880.5595703125,
          10899.83984375,
          10861.1103515625,
          10835.4296875,
          10919.9599609375,
          10949.759765625,
          10994.4404296875,
          10976.4599609375,
          10945.919921875,
          10907.4404296875,
          10905.740234375,
          10927.16015625,
          10835.7998046875,
          10773.25,
          10641.2802734375,
          10542.490234375,
          10425.599609375,
          10461.3701171875,
          10500.76953125,
          10514.0498046875,
          10436.6201171875,
          10518.01953125,
          10374.75,
          10456.3203125,
          10516.0498046875,
          10538.1201171875,
          10545.25,
          10596.41015625,
          10541.73046875,
          10453.400390625,
          10431.7802734375,
          10456.2998046875,
          10466.990234375,
          10618.0498046875,
          10634.849609375,
          10650.150390625,
          10670.4296875,
          10766.599609375,
          10803.0703125,
          10810.0302734375,
          10815.58984375,
          10862.6796875,
          10898.1298828125,
          10912.23046875,
          10971.5400390625,
          10954.1796875,
          10931.759765625,
          10948.1904296875,
          10947.5302734375,
          10885.669921875,
          10934.91015625,
          10921.169921875,
          10967.650390625,
          10965.2001953125,
          10888.400390625,
          10941.490234375,
          10954.4501953125,
          11020.509765625,
          10989.3896484375,
          11085.490234375,
          11126.4501953125,
          11180.1103515625,
          11186.8798828125,
          11237.3798828125,
          11188.1298828125,
          11271.25,
          11281.1201171875,
          11320.1396484375,
          11347.1796875,
          11341.0302734375,
          11373.9501953125,
          11380.2802734375,
          11428.419921875,
          11399.5302734375,
          11559.0703125,
          11644.0302734375,
          11668.2001953125,
          11647.4697265625,
          11642.41015625,
          11570.330078125,
          11520.3701171875,
          11509.3701171875,
          11498.25,
          11552.25,
          11599.7802734375,
          11656.400390625,
          11649.0703125,
          11573.83984375,
          11577.01953125,
          11604.75,
          11642.9599609375,
          11651.73046875,
          11654.0595703125,
          11623.58984375,
          11524.849609375,
          11531.580078125,
          11513.830078125,
          11604.3896484375,
          11657.650390625,
          11678.3603515625,
          11649.73046875,
          11700.76953125,
          11875.98046875,
          11990.7802734375,
          11975.73046875,
          12097.009765625,
          12125.900390625,
          12111.76953125,
          12028.2197265625,
          12022.23046875,
          12027.599609375,
          12032.3701171875,
          12030.66015625,
          12093.01953125,
          12114.8896484375,
          12047.75,
          12110.740234375,
          12197.6396484375,
          12040.080078125,
          11986.0302734375,
          11899.669921875,
          11992.5498046875,
          12038.2099609375,
          12113.419921875,
          12186.6201171875,
          12169.919921875,
          12075.5400390625,
          12117.5498046875,
          12151.419921875,
          11933.23046875,
          11594.2099609375,
          11365.900390625,
          11581.3603515625,
          11620.419921875,
          11749.6796875,
          11712.7099609375,
          11598.0302734375,
          11678.580078125,
          11801.51953125,
          11854.98046875,
          11840.7900390625,
          11775.5595703125,
          11717.099609375,
          11783.4296875,
          11827.830078125,
          11755.169921875,
          11615.2802734375,
          11567.08984375,
          11494.2197265625,
          11470.23046875,
          11282.0498046875,
          11390.240234375,
          11392.349609375,
          11525.3203125,
          11471.3603515625,
          11221.759765625,
          11032.4697265625,
          11088.23046875,
          10845.3896484375,
          10171.2001953125,
          10130.7998046875,
          9677.0703125,
          9509.6103515625,
          9085.2802734375,
          9264.419921875,
          9029.51953125,
          9415.6396484375,
          9722.3701171875,
          9739.73046875,
          9954.8603515625,
          9655.7001953125,
          9789.8095703125,
          9736,
          9818.740234375,
          10039.150390625,
          10149.759765625,
          10246.83984375,
          10165.1201171875,
          10179.400390625,
          10344.1298828125,
          10461.83984375,
          10425.2900390625,
          10710.150390625,
          10637.6396484375,
          10544.7998046875,
          10310.849609375,
          10462.7001953125,
          10386.9296875,
          10578.9599609375,
          10620.51953125,
          10794.830078125,
          11012.7802734375,
          10781.509765625,
          10833.5302734375,
          10828.73046875,
          10890.8701171875,
          10970.7001953125,
          11039.1103515625,
          10974.509765625,
          10938.26953125,
          10894.169921875,
          10904.009765625,
          10812.8701171875,
          10924.7900390625,
          10933.98046875,
          11021.66015625,
          10977.5498046875,
          10874.5400390625,
          11046.2197265625,
          11069.7197265625,
          11087.5302734375,
          10965.0302734375,
          11109.2900390625,
          11170.490234375,
          11330.51953125,
          11425.419921875,
          11482.3603515625,
          11631.8896484375,
          11640.4501953125,
          11740.900390625,
          11771.1201171875,
          11442.2998046875,
          11469.83984375,
          11542.25,
          11550.3203125,
          11567.419921875,
          11584.8798828125,
          11632.8798828125,
          11679.400390625,
          11701.23046875,
          11608.7998046875,
          11638,
          11736.3896484375,
          11805.2099609375,
          11933.58984375,
          12116.7001953125,
          12249.9501953125,
          12190.26953125,
          12273.4296875,
          12253.580078125,
          12216.240234375,
          12228.3095703125,
          12320.48046875,
          12220.9296875,
          12266.9296875,
          12221.330078125,
          12450.16015625,
          12486.9501953125,
          12429.759765625,
          12502.83984375,
          12686.3603515625,
          13031.7001953125,
          12660.8701171875,
          12769.4697265625,
          12733.48046875,
          12673.01953125,
          12709.919921875,
          12816.3896484375,
          12971.8701171875,
          12912.6298828125,
          12933.16015625,
          12906.009765625,
          12730.9599609375,
          12812.7998046875,
          12801.3095703125,
          12956.1103515625,
          12981.580078125,
          12950.1103515625,
          12764.75,
          12638.650390625,
          12702.9404296875,
          12814.5595703125,
          12833.2900390625,
          12960.6796875,
          12799.16015625,
          12831.3798828125,
          12703.2802734375,
          12802.6103515625,
          12857.7900390625,
          12655.669921875,
          12725.0498046875,
          12702.5498046875,
          12616.5703125,
          12708.6396484375,
          12701.7900390625,
          12793.0703125,
          12857.73046875,
          13021.6796875,
          12972.51953125,
          12927.8095703125,
          12952.3095703125,
          12786.91015625,
          12700.8896484375,
          12487.48046875,
          12385.8095703125,
          12462.759765625,
          12571.33984375,
          12568.6796875,
          12637.6201171875,
          12706.3896484375,
          12774.3603515625,
          12887.1904296875,
          12997.7900390625,
          12994.66015625,
          12960,
          12909.1103515625,
          12896.2998046875,
          12911.41015625,
          12917.75,
          12942.1201171875,
          12917.0302734375,
          12963.259765625,
          12971.580078125,
          12884.3203125,
          12879.98046875,
          12699.8203125,
          12656.2900390625,
          12594.33984375,
          12760.1396484375,
          12885.2802734375,
          12918.7998046875,
          12999.16015625,
          13149.900390625,
          13142.2197265625,
          13262.1904296875,
          13324.8603515625,
          13273.330078125,
          13551.830078125,
          13780.1201171875,
          13773.2900390625,
          13785.919921875,
          13726.2197265625,
          13921.16015625,
          13951.169921875,
          13893.2001953125,
          13856.759765625,
          13885.009765625,
          13969.3896484375,
          13885.669921875,
          13995.7001953125,
          14049.580078125,
          14149.5595703125,
          14306.8701171875,
          14367,
          14427.41015625,
          14319.3798828125,
          14353.169921875,
          14270.33984375,
          14256.5498046875,
          14339.5,
          14338.400390625,
          14329.1201171875,
          14384.9599609375,
          14411.9296875,
          14247.7900390625,
          14324.419921875,
          14400.830078125,
          14483.0703125,
          14547.0703125,
          14695.4404296875,
          14760.0595703125,
          14937.1298828125,
          15000.0302734375,
          15197.6796875,
          15270.400390625,
          15463.9501953125,
          15557.2998046875,
          15642.0302734375,
          15778.51953125,
          15760.150390625,
          16041.58984375,
          15676.4501953125,
          15928.1396484375,
          16004.3203125,
          16238.4599609375,
          16138.0400390625,
          16014.9599609375,
          16023.8701171875,
          15837.4697265625,
          15557.01953125,
          15603.41015625,
          15429.98046875,
          15838.150390625,
          15896.400390625,
          15801.3701171875,
          15938.08984375,
          16406.779296875,
          16517.73046875,
          16382.7001953125,
          16579.169921875,
          16467.75,
          16456.939453125,
          16474.05078125,
          16190.0400390625,
          16262.91015625,
          16211.73046875,
          16091.75,
          15934.4296875,
          16074.9697265625,
          15864.259765625,
          15986.0703125,
          16216.2197265625,
          16298.0302734375,
          16281.91015625,
          16340.66015625,
          16349.2099609375,
          16410.01953125,
          16186.4599609375,
          16235.6298828125,
          16351.3798828125,
          16125.5400390625,
          16146.580078125,
          16325.6201171875,
          16520.890625,
          16556.189453125,
          16550.19921875,
          16602.470703125,
          16816.33984375,
          16926.720703125,
          17016.130859375,
          16979.349609375,
          17041.369140625,
          16935.390625,
          17076.73046875,
          17158.810546875,
          17294.150390625,
          17323.869140625,
          17282.759765625,
          17428.150390625,
          17300.26953125,
          17572.2890625,
          17630.189453125,
          17628.810546875,
          17709.23046875,
          17546.939453125,
          17328.55078125,
          17052.19921875,
          17088.73046875,
          17285,
          17304.470703125,
          17137.189453125,
          16552.630859375,
          16031.9296875,
          16075.48046875,
          15719.740234375,
          16153.76953125,
          16273.98046875,
          16154.080078125,
          16431.140625,
          16397.58984375,
          16657.599609375,
          16706.2890625,
          16601.609375,
          16889.009765625,
          17113.330078125,
          17184.30078125,
          17274.66015625,
          17311.279296875,
          17225.060546875,
          17189.23046875,
          17181.51953125,
          17082.51953125,
          17159.220703125,
          17278.55078125,
          17371.2890625,
          17398.220703125,
          17390.609375,
          17417.5,
          17305.490234375,
          17270.470703125,
          17375.7890625,
          17439.630859375,
          17597.330078125,
          17595.509765625,
          17713.240234375,
          17797.26953125,
          17863.900390625,
          17795.880859375,
          17945.51953125,
          18008.369140625,
          17933.619140625,
          17935.109375,
          17778.119140625,
          17947.900390625,
          18018.0390625,
          17940.859375,
          18034.189453125,
          17926.380859375,
          17854.349609375,
          17724.919921875,
          17707.560546875,
          17667.380859375,
          17672.599609375,
          17637.359375,
          17459.349609375,
          17252.869140625,
          17402.810546875,
          17429.009765625,
          17503.279296875,
          17553.759765625,
          17636.109375,
          17643.970703125,
          17593.849609375,
          17510.08984375,
          17524.91015625,
          17327.19921875,
          17233.109375,
          17216.33984375,
          16983.58984375,
          16870.55078125,
          16826.26953125,
          16777.779296875,
          16507.109375,
          16785.2890625,
          16904.30078125,
          17045.859375,
          17201.419921875,
          17229.890625,
          17396.51953125,
          17490.2890625,
          17503.9296875,
          17523.16015625,
          17540.779296875,
          17633.669921875,
          17559.2109375,
          17447.580078125,
          17319.08984375,
          17474.5703125,
          17482.5703125,
          17529.470703125,
          17439.75,
          17411.369140625,
          17408.7109375,
          17196.7890625,
          17145.25,
          17273.58984375,
          17335.990234375,
          17286.890625,
          17127.859375,
          16994.2109375,
          16883,
          16680.2890625,
          16460.75,
          16568.2109375,
          16731.580078125,
          16771.7109375,
          16589.720703125,
          16543.330078125,
          16519.9609375,
          16781.189453125,
          16862.029296875,
          16916.720703125,
          16973.859375,
          17026.890625,
          16932.890625,
          16930.98046875,
          17067.150390625,
          17079.599609375,
          17104.859375,
          17073.970703125,
          17119.4296875,
          17237.08984375,
          17153.759765625,
          17219.650390625,
          17296.900390625,
          17415.30078125,
          17581.51953125,
          17575.439453125,
          17527.1796875,
          17602.05078125,
          17683.189453125,
          17708.099609375,
          17764.0390625,
          17841.369140625,
          17986.1796875,
          17857.73046875,
          17797.900390625,
          17722.890625,
          17724.5,
          17641.7890625,
          17415.630859375,
          17535.23046875,
          17626.2109375,
          17741.55078125,
          17781.140625,
          17763.419921875,
          17796.919921875,
          17988.880859375,
          17925.119140625,
          17893.2890625,
          17919.349609375,
          17754.380859375,
          17678.779296875,
          17814.3203125,
          17822.740234375,
          17812.810546875,
          17825.890625,
          17870.16015625,
          17960.990234375,
          18039.849609375,
          18099.779296875,
          18197.359375,
          18283.25,
          18291.25,
          18379.689453125,
          18526.349609375,
          18619.609375,
          18427.150390625,
          18444.119140625,
          18243.259765625,
          18293.529296875,
          18394.69921875,
          18459.73046875,
          18509,
          18535.419921875,
          18575.41015625,
          18359.140625,
          18292.140625,
          18113.939453125,
          18004.44921875,
          17890.7109375,
          17776.419921875,
          17900.30078125,
          18063.55078125,
          18168.599609375,
          18338.05078125,
          18310.939453125,
          18182.73046875,
          18085.91015625,
          18233.7890625,
          18330.630859375,
          18262.9609375,
          18253.099609375,
          18159.509765625,
          18109.279296875,
          17939.529296875,
          17737.3203125,
          17944.5,
          17918.98046875,
          18026.029296875,
          17845.009765625,
          17581.5703125,
          17113.669921875,
          17072.740234375,
          17478.4296875,
          17373.900390625,
          17363.0390625,
          17177.26953125,
          17085.76953125,
          17472.94921875,
          17472.380859375,
          17603.990234375,
          17573.2890625,
          17738.689453125,
          17711.169921875,
          17747.130859375,
          17527.150390625,
          17585.91015625,
          17770.0703125,
          17767.810546875,
          17657.759765625,
          17528.990234375,
          17438.599609375,
          17316.689453125,
          17316.349609375,
          17100.849609375,
          17333.2109375,
          17374.140625,
          17141.330078125,
          16999.359375,
          17106.259765625,
          17148.880859375,
          17213.66015625,
          17088.759765625,
          16912.30078125,
          16729.470703125,
          16427.3203125,
          16455.5703125,
          16663.66015625,
          16604.869140625,
          16617.060546875,
          16783.779296875,
          16491.109375,
          16345.83984375,
          16071.5,
          16081.150390625,
          15943.6201171875,
          15860.6904296875,
          16032.849609375,
          16085.849609375,
          16316.580078125,
          16111.490234375,
          16181.66015625,
          16249.4501953125,
          16219.7998046875,
          16169.1796875,
          16179.009765625,
          16266.2197265625,
          16610.619140625,
          16807.76953125,
          16811.0390625,
          16617.259765625,
          16654.119140625,
          16593.75,
          16702.990234375,
          16643.94921875,
          16581.810546875,
          16295.0703125,
          16067.7998046875,
          16106.75
         ],
         "low": [
          10474.1904296875,
          10521.7001953125,
          10650.48046875,
          10745.25,
          10773.4697265625,
          10695.490234375,
          10693.0400390625,
          10645.3701171875,
          10674.2001953125,
          10719.080078125,
          10821.2998046875,
          10843.6396484375,
          10720.66015625,
          10750.900390625,
          10756.669921875,
          10708.919921875,
          10680.849609375,
          10723.23046875,
          10799.650390625,
          10823.8896484375,
          10769.5703125,
          10865,
          10804.33984375,
          10792.26953125,
          10861.990234375,
          10909.150390625,
          10932.5302734375,
          10912.919921875,
          10879.400390625,
          10879.240234375,
          10863.330078125,
          10827.740234375,
          10754.0703125,
          10714.73046875,
          10524.6796875,
          10418.25,
          10180.0400390625,
          10366.5400390625,
          10383.6201171875,
          10440.76953125,
          10361.8896484375,
          10427.73046875,
          10287.759765625,
          10318.3701171875,
          10449.3896484375,
          10497.1298828125,
          10504.490234375,
          10515.7197265625,
          10503.0703125,
          10340.150390625,
          10368.9599609375,
          10397.599609375,
          10406.490234375,
          10504.2802734375,
          10589.419921875,
          10557.0302734375,
          10566.3203125,
          10714.26953125,
          10753.4501953125,
          10732.7802734375,
          10746.8701171875,
          10804.01953125,
          10822.6904296875,
          10866.4296875,
          10878.33984375,
          10874.66015625,
          10903.4501953125,
          10905.919921875,
          10886.6201171875,
          10825.849609375,
          10856.9599609375,
          10827.6103515625,
          10855.599609375,
          10927.3896484375,
          10809.2802734375,
          10854.1201171875,
          10919.4296875,
          10986.009765625,
          10889.9599609375,
          11004.740234375,
          11091.2900390625,
          11113.1796875,
          11131.4501953125,
          11154.8603515625,
          11147.8603515625,
          11218.25,
          11209.099609375,
          11254.58984375,
          11281.3701171875,
          11295.4404296875,
          11292.830078125,
          11306.9501953125,
          11358.7099609375,
          11335.509765625,
          11451.7197265625,
          11576.1796875,
          11601.7001953125,
          11552.080078125,
          11561.3095703125,
          11409.830078125,
          11459.6796875,
          11439.08984375,
          11424.76953125,
          11485.4599609375,
          11532.9697265625,
          11576.7900390625,
          11591.73046875,
          11478.4296875,
          11534.150390625,
          11559.330078125,
          11576.8203125,
          11590.8095703125,
          11604.6796875,
          11485.16015625,
          11454.3798828125,
          11460.0595703125,
          11457.4296875,
          11546.75,
          11577.830078125,
          11631.0302734375,
          11607.259765625,
          11622.580078125,
          11766.98046875,
          11913.01953125,
          11915.6103515625,
          11939.6201171875,
          12059.919921875,
          12008.16015625,
          11937.4501953125,
          11960.2099609375,
          11976.3798828125,
          11973.9501953125,
          11982.6396484375,
          12032.7197265625,
          12046.2197265625,
          11997.1396484375,
          12026.23046875,
          12023.599609375,
          11953.3603515625,
          11822.400390625,
          11777.4501953125,
          11889.4599609375,
          11959.0595703125,
          12037.240234375,
          12140.26953125,
          12048.009765625,
          12006.080078125,
          12055.91015625,
          12101.5498046875,
          11418.2197265625,
          11436.9501953125,
          11138.0302734375,
          11393.0302734375,
          11512.7099609375,
          11605.1103515625,
          11592.08984375,
          11423.33984375,
          11614.25,
          11693.400390625,
          11784.9404296875,
          11788.8701171875,
          11724.8203125,
          11642.9501953125,
          11654.16015625,
          11714.150390625,
          11661.6298828125,
          11512.080078125,
          11415.4697265625,
          11408.0595703125,
          11274.51953125,
          11049.849609375,
          11279.8603515625,
          11297.8095703125,
          11454.650390625,
          11310.3203125,
          10977.6396484375,
          10830.2197265625,
          10885.91015625,
          10359.669921875,
          9636.150390625,
          9717.76953125,
          9371.349609375,
          9218.669921875,
          8523.6298828125,
          8816.8603515625,
          8750.1396484375,
          9083.7802734375,
          9426.4296875,
          9565.01953125,
          9691.1396484375,
          9415.51953125,
          9630.2998046875,
          9663.6298828125,
          9651.51953125,
          9928.16015625,
          9984.66015625,
          10092.3603515625,
          10103.76953125,
          10080.650390625,
          10130.650390625,
          10366.76953125,
          10317.1298828125,
          10552.580078125,
          10542.9599609375,
          10278.9501953125,
          10140.08984375,
          10294.8798828125,
          10324.2001953125,
          10407.490234375,
          10537.7099609375,
          10656.400390625,
          10826.259765625,
          10658.5,
          10735.009765625,
          10708.099609375,
          10775.16015625,
          10883.2197265625,
          10942.1796875,
          10854.509765625,
          10828.9296875,
          10777.4599609375,
          10730.5498046875,
          10730.7001953125,
          10812.7900390625,
          10860.150390625,
          10933.2099609375,
          10804.5595703125,
          10719.25,
          10903.3203125,
          10972.25,
          10915.2998046875,
          10861.900390625,
          10971.400390625,
          11102.9697265625,
          11182.51953125,
          11341.580078125,
          11411.9404296875,
          11528.4404296875,
          11537.3798828125,
          11621.01953125,
          11516.58984375,
          11244.6396484375,
          11299.4501953125,
          11380.669921875,
          11482.3203125,
          11485.3896484375,
          11542.3701171875,
          11553.91015625,
          11530.849609375,
          11637.7900390625,
          11500.650390625,
          11563.599609375,
          11622.6396484375,
          11690.9501953125,
          11857.900390625,
          11941.83984375,
          12028.5703125,
          12083.83984375,
          12181.33984375,
          12030.2001953125,
          12109.6103515625,
          12143.91015625,
          12162.51953125,
          12107.0400390625,
          12144.6201171875,
          12065.7998046875,
          12242.3203125,
          12389.400390625,
          12347.8603515625,
          12266.5498046875,
          12586.91015625,
          12533.1904296875,
          12488.009765625,
          12616.830078125,
          12635.7099609375,
          12506.58984375,
          12577.8798828125,
          12739.990234375,
          12851.2001953125,
          12791.1796875,
          12786.7998046875,
          12780.1904296875,
          12625.5400390625,
          12717.1298828125,
          12679.1904296875,
          12800.6904296875,
          12840.25,
          12778.6396484375,
          12144.759765625,
          12462.51953125,
          12567.9501953125,
          12698.849609375,
          12734.6904296875,
          12786.4501953125,
          12674.5703125,
          12591.4501953125,
          12565.830078125,
          12646.4697265625,
          12732.3203125,
          12559.7802734375,
          12575.3603515625,
          12614.7197265625,
          12480.5,
          12650.400390625,
          12616.6796875,
          12680.41015625,
          12773.23046875,
          12948.7197265625,
          12851.51953125,
          12841.08984375,
          12795.08984375,
          12632.740234375,
          12548.0703125,
          12264.3798828125,
          12149.8095703125,
          12282.3603515625,
          12429.7197265625,
          12466.580078125,
          12519.1201171875,
          12644.08984375,
          12619.8095703125,
          12818.009765625,
          12898.8896484375,
          12857.099609375,
          12895.7900390625,
          12786.259765625,
          12750.3701171875,
          12803.3203125,
          12862.3701171875,
          12846.419921875,
          12827.400390625,
          12875.23046875,
          12894.1796875,
          12821.0703125,
          12761.330078125,
          12583.3701171875,
          12546.33984375,
          12480.66015625,
          12641.2802734375,
          12736.01953125,
          12840.400390625,
          12927.2998046875,
          13048.669921875,
          13022.919921875,
          13067.0400390625,
          13193.740234375,
          13170.1201171875,
          13356.740234375,
          13593.009765625,
          13608.7197265625,
          13700.0400390625,
          13666.0703125,
          13793.5498046875,
          13798.3203125,
          13731.8095703125,
          13763.7900390625,
          13811.8095703125,
          13722.8896484375,
          13749.7099609375,
          13894.7001953125,
          13940.0703125,
          14010.169921875,
          14142.01953125,
          14184.580078125,
          14336.6904296875,
          14191.849609375,
          14081.419921875,
          14182.25,
          14053.349609375,
          14145.240234375,
          14213.490234375,
          14245.599609375,
          14166.8896484375,
          14175.6904296875,
          14134.8701171875,
          14256.2900390625,
          14296.9697265625,
          14363.4501953125,
          14435.0302734375,
          14476.6904296875,
          14646.330078125,
          14720.25,
          14861.990234375,
          14837,
          15049.8603515625,
          15275.3798828125,
          15395.73046875,
          15421.23046875,
          15550.16015625,
          15620.9599609375,
          15615.1103515625,
          15320.9697265625,
          15716.6396484375,
          15745.48046875,
          15775.73046875,
          15973.01953125,
          15772.6298828125,
          15589.2099609375,
          15642.1201171875,
          15367.4501953125,
          15138.3095703125,
          15089.9599609375,
          15546.6904296875,
          15741.16015625,
          15606.740234375,
          15774.33984375,
          16197.4501953125,
          16323.2197265625,
          16211.25,
          16410.16015625,
          16211.75,
          16212.5302734375,
          16322.26953125,
          15953.7998046875,
          15946.8798828125,
          15884.5498046875,
          15840.990234375,
          15636.4296875,
          15816.5,
          15657.919921875,
          15857.3203125,
          15947.2099609375,
          16166.349609375,
          16194.91015625,
          16244.990234375,
          16166.349609375,
          16264.6103515625,
          16022.169921875,
          15983.76953125,
          16166.2900390625,
          15967.9501953125,
          15944.9599609375,
          16140.3701171875,
          16411.669921875,
          16438.400390625,
          16427.19921875,
          16496.16015625,
          16715.3203125,
          16735.779296875,
          16815.58984375,
          16793.5390625,
          16802.220703125,
          16559.5703125,
          16851.060546875,
          16998.91015625,
          17135.51953125,
          17175.01953125,
          17167.390625,
          17066.169921875,
          17055.48046875,
          17378.359375,
          17489.7109375,
          17497.740234375,
          17548.30078125,
          17222.349609375,
          16647.609375,
          16843.439453125,
          16764.7109375,
          17032.23046875,
          17188.150390625,
          16460.869140625,
          15165.26953125,
          15368.5400390625,
          15702.099609375,
          15159.8603515625,
          15564.9599609375,
          16009.759765625,
          15943.4501953125,
          16136.8603515625,
          16136.9697265625,
          16444.75,
          16523.23046875,
          16419.419921875,
          16690.0390625,
          16939.91015625,
          17060.44921875,
          17056.419921875,
          17198.890625,
          17084.490234375,
          16775.849609375,
          17056.529296875,
          16907.4296875,
          16978.009765625,
          17193.609375,
          17279.69921875,
          17275.01953125,
          17150.51953125,
          17318.5390625,
          17023.310546875,
          17075.55078125,
          17127.58984375,
          17319.890625,
          17471.0703125,
          17481.23046875,
          17541.140625,
          17648.25,
          17644.359375,
          17676.98046875,
          17783.80078125,
          17895.779296875,
          17756.8203125,
          17742.080078125,
          17597.4609375,
          17759.900390625,
          17786.759765625,
          17716.439453125,
          17878.779296875,
          17779.609375,
          17708.150390625,
          17500.689453125,
          17352.240234375,
          17482.810546875,
          17511.859375,
          17403.560546875,
          17264.5,
          16893.69921875,
          17190.48046875,
          17237.669921875,
          17231.220703125,
          17456.19921875,
          17557.1796875,
          17566.720703125,
          17469.6796875,
          17350.150390625,
          17305.060546875,
          17090.25,
          17139.2109375,
          16978.109375,
          16773.5703125,
          16657.630859375,
          16418.5390625,
          16375.400390625,
          16248.080078125,
          16459.130859375,
          16779.900390625,
          16821.080078125,
          16984.689453125,
          17000.259765625,
          17244.619140625,
          17207.5703125,
          17415.51953125,
          17319.759765625,
          17380.51953125,
          17461.0703125,
          17388.369140625,
          17167.080078125,
          17122.94921875,
          17270.279296875,
          17387.5703125,
          17424.5390625,
          17316.51953125,
          17254.099609375,
          17235.44921875,
          16838.580078125,
          16998.0703125,
          17130.740234375,
          17235.6796875,
          17113.470703125,
          16801.779296875,
          16767.19921875,
          16503.740234375,
          16380.0595703125,
          16162.169921875,
          16303.6298828125,
          16465.5703125,
          16605.490234375,
          16349.490234375,
          16328.4697265625,
          16347.8798828125,
          16426.759765625,
          16695.890625,
          16772.150390625,
          16855.810546875,
          16873.990234375,
          16800.490234375,
          16784.109375,
          16909.3203125,
          16973.16015625,
          16994.369140625,
          16920.6796875,
          17021.76953125,
          17026.630859375,
          17080.509765625,
          17061.4609375,
          17097.16015625,
          17279.4609375,
          17433.099609375,
          17489.869140625,
          17403.48046875,
          17479.55078125,
          17560.51953125,
          17629.80078125,
          17669.580078125,
          17748.2109375,
          17786.05078125,
          17790.640625,
          17650.0703125,
          17588.779296875,
          17609.509765625,
          17330.439453125,
          17167.240234375,
          17369.7890625,
          17374.58984375,
          17559.439453125,
          17670.3203125,
          17585.94921875,
          17642.3203125,
          17832.419921875,
          17807.119140625,
          17767.80078125,
          17767.599609375,
          17566.990234375,
          17556.869140625,
          17719.060546875,
          17718.26953125,
          17646.390625,
          17652.3203125,
          17799.720703125,
          17855.779296875,
          17953.5703125,
          17975.41015625,
          18099.7109375,
          18192.859375,
          18216.44921875,
          18238.470703125,
          18395.140625,
          18446.51953125,
          18253.8203125,
          18134.41015625,
          18043.970703125,
          18135.4296875,
          18255.380859375,
          18346.810546875,
          18213.439453125,
          18435.01953125,
          18378.640625,
          18199.349609375,
          18125.19921875,
          17851.390625,
          17682.330078125,
          17645.66015625,
          17633.029296875,
          17712.349609375,
          17955.94921875,
          18039.23046875,
          18145.0390625,
          18191.75,
          17965.220703125,
          17942.6796875,
          18109.169921875,
          18190.240234375,
          18098.51953125,
          18129.650390625,
          17840.380859375,
          17954.75,
          17561.0703125,
          17554.970703125,
          17657.5,
          17784.640625,
          17906.240234375,
          17710.5390625,
          17135.6796875,
          16764.779296875,
          16944.080078125,
          17224.73046875,
          17239.900390625,
          17217.580078125,
          16911.94921875,
          16808.4296875,
          17172.810546875,
          17359.5,
          17503.94921875,
          17468.55078125,
          17581.619140625,
          17603.220703125,
          17633.859375,
          17368.66015625,
          17493.009765625,
          17572.900390625,
          17682.529296875,
          17465.609375,
          17381.0703125,
          17178.630859375,
          17210.400390625,
          17046.669921875,
          16905.130859375,
          17080.400390625,
          17245.650390625,
          17004.1796875,
          16845.509765625,
          16926.33984375,
          16983.55078125,
          17106.369140625,
          16923.869140625,
          16579.890625,
          16582.109375,
          16219.41015625,
          16256.8798828125,
          16521.939453125,
          16465.990234375,
          16514.30078125,
          16650.810546875,
          16312.169921875,
          16048.919921875,
          15734.4404296875,
          15953.26953125,
          15616.6796875,
          15687.150390625,
          15847.4599609375,
          15915.9296875,
          16172.7998046875,
          15892.73046875,
          16058.2001953125,
          16125.4501953125,
          15963.6298828125,
          15980.900390625,
          15949.6103515625,
          16075.91015625,
          16368.9296875,
          16493.0703125,
          16649.91015625,
          16540.55078125,
          16509.009765625,
          16465.869140625,
          16538.669921875,
          16557.5390625,
          16403.330078125,
          16055.8896484375,
          15869.0595703125,
          15981.5595703125
         ],
         "open": [
          10488.7001953125,
          10547.150390625,
          10650.48046875,
          10749.41015625,
          10817.7099609375,
          10734.25,
          10777.1796875,
          10661.3896484375,
          10674.2001953125,
          10786.66015625,
          10821.2998046875,
          10878.009765625,
          10793.099609375,
          10755.8701171875,
          10785.849609375,
          10742.8095703125,
          10729.830078125,
          10723.23046875,
          10817.599609375,
          10855.16015625,
          10819.919921875,
          10865,
          10861.1103515625,
          10821.919921875,
          10861.990234375,
          10910.5,
          10963.8603515625,
          10969.740234375,
          10892.25,
          10898.25,
          10872.6298828125,
          10909.98046875,
          10824.150390625,
          10773.0595703125,
          10641.2802734375,
          10528.8701171875,
          10304.8798828125,
          10422.8896484375,
          10383.6201171875,
          10491.7900390625,
          10436.6201171875,
          10444.5,
          10374.75,
          10345.419921875,
          10449.3896484375,
          10508.419921875,
          10526.8701171875,
          10560.009765625,
          10526.150390625,
          10453.400390625,
          10386.2099609375,
          10397.599609375,
          10440.150390625,
          10504.2802734375,
          10613.1904296875,
          10629.2197265625,
          10566.3203125,
          10718.400390625,
          10783.4599609375,
          10791.9501953125,
          10787.650390625,
          10830.25,
          10822.6904296875,
          10908.849609375,
          10887.419921875,
          10947.6201171875,
          10908.099609375,
          10924.8095703125,
          10917.4404296875,
          10885.669921875,
          10909.5703125,
          10904,
          10855.599609375,
          10945.919921875,
          10866.5,
          10907.6904296875,
          10932.240234375,
          10986.009765625,
          10976.9296875,
          11004.740234375,
          11103.5,
          11166.169921875,
          11156.1396484375,
          11190.2099609375,
          11166.240234375,
          11221.1904296875,
          11266.1796875,
          11266.83984375,
          11336.6396484375,
          11321.990234375,
          11347.3896484375,
          11336.9697265625,
          11393.6796875,
          11373.7001953125,
          11451.7197265625,
          11576.1796875,
          11645.080078125,
          11642.91015625,
          11620.8896484375,
          11570.330078125,
          11463.330078125,
          11483.240234375,
          11488.740234375,
          11485.4599609375,
          11543.7998046875,
          11608.48046875,
          11645.73046875,
          11573.83984375,
          11556.75,
          11590.8095703125,
          11608.3701171875,
          11590.8095703125,
          11641.7001953125,
          11616.8798828125,
          11509.9404296875,
          11473.3203125,
          11511.8203125,
          11546.75,
          11639.7998046875,
          11635.4697265625,
          11647.7802734375,
          11635.0703125,
          11766.98046875,
          11937.900390625,
          11915.6103515625,
          11939.6201171875,
          12061.099609375,
          12082.98046875,
          12001.5595703125,
          11969.1396484375,
          12023.7900390625,
          11978.9404296875,
          12018.3798828125,
          12032.7197265625,
          12094.8095703125,
          12019.1904296875,
          12026.5,
          12167.4404296875,
          12035.7099609375,
          11961.9697265625,
          11818.759765625,
          11889.4599609375,
          12009.26953125,
          12069.6103515625,
          12161.73046875,
          12169.919921875,
          12006.080078125,
          12080.7197265625,
          12107.5595703125,
          11933.23046875,
          11494.0302734375,
          11365.900390625,
          11399.4599609375,
          11601.5498046875,
          11605.1103515625,
          11712.7099609375,
          11514.7197265625,
          11614.25,
          11693.400390625,
          11813.5,
          11806.51953125,
          11770.2998046875,
          11700.91015625,
          11655.3701171875,
          11785.7802734375,
          11721.16015625,
          11615.2802734375,
          11506.740234375,
          11468.599609375,
          11436.9599609375,
          11184.66015625,
          11279.8603515625,
          11368.25,
          11454.650390625,
          11471.3603515625,
          11221.759765625,
          10907.6201171875,
          11022.8203125,
          10845.3896484375,
          10091.0302734375,
          10069.419921875,
          9538.6396484375,
          9453.98046875,
          9085.2802734375,
          8816.8603515625,
          9025.5498046875,
          9083.7802734375,
          9426.4296875,
          9667.1396484375,
          9807.900390625,
          9571.2197265625,
          9689.6201171875,
          9726.2001953125,
          9707.75,
          9928.16015625,
          10010.6396484375,
          10173.259765625,
          10105.4296875,
          10147.5595703125,
          10130.650390625,
          10366.76953125,
          10385.7802734375,
          10554.5498046875,
          10612.4501953125,
          10544.7998046875,
          10256.1103515625,
          10370.1298828125,
          10367.9697265625,
          10407.490234375,
          10580.25,
          10656.400390625,
          10826.259765625,
          10781.509765625,
          10756.9501953125,
          10771.08984375,
          10778.5302734375,
          10883.2197265625,
          10942.1796875,
          10974.509765625,
          10861.009765625,
          10894.169921875,
          10833.16015625,
          10753.2099609375,
          10812.7900390625,
          10892.759765625,
          10933.2099609375,
          10977.5498046875,
          10812.9404296875,
          10903.3203125,
          11023.9296875,
          11040.6201171875,
          10921.16015625,
          10971.400390625,
          11109.26953125,
          11182.51953125,
          11373.9404296875,
          11418.009765625,
          11539,
          11600.6103515625,
          11635.990234375,
          11738.490234375,
          11406.5400390625,
          11436.2802734375,
          11380.669921875,
          11525.5,
          11534.2001953125,
          11557.26953125,
          11553.91015625,
          11618.419921875,
          11647.8603515625,
          11608.7998046875,
          11563.599609375,
          11622.6396484375,
          11694.099609375,
          11857.900390625,
          11941.83984375,
          12176.7900390625,
          12086.1298828125,
          12222.900390625,
          12250.4404296875,
          12109.6103515625,
          12202.8896484375,
          12233.990234375,
          12173.0400390625,
          12195.7197265625,
          12205.25,
          12242.3203125,
          12389.759765625,
          12423.01953125,
          12467.4296875,
          12618.6904296875,
          12951.7197265625,
          12530.740234375,
          12691.8603515625,
          12653.4599609375,
          12642.73046875,
          12577.8798828125,
          12762.33984375,
          12894.240234375,
          12901.4296875,
          12786.7998046875,
          12856.4599609375,
          12713.6298828125,
          12758.8701171875,
          12709.9697265625,
          12813.9404296875,
          12974.9599609375,
          12904.83984375,
          12764.75,
          12462.51953125,
          12629.7802734375,
          12706.400390625,
          12768.9599609375,
          12888.650390625,
          12759.2998046875,
          12763.4404296875,
          12617.3896484375,
          12772.4296875,
          12774.9697265625,
          12645.919921875,
          12645.9296875,
          12637.419921875,
          12592.1796875,
          12665.740234375,
          12697.4599609375,
          12680.41015625,
          12787.419921875,
          12989.8095703125,
          12920.150390625,
          12858.599609375,
          12874.6103515625,
          12786.91015625,
          12656.2998046875,
          12487.48046875,
          12312.9404296875,
          12282.3603515625,
          12488.099609375,
          12483.650390625,
          12567.8798828125,
          12644.08984375,
          12667.509765625,
          12832.5400390625,
          12959.150390625,
          12988.4404296875,
          12927.08984375,
          12892.2900390625,
          12814.16015625,
          12803.3203125,
          12889.990234375,
          12889.8798828125,
          12854.9697265625,
          12945.1201171875,
          12923.5498046875,
          12867.76953125,
          12853.759765625,
          12687.6396484375,
          12651.349609375,
          12565.3203125,
          12641.2802734375,
          12766.8603515625,
          12878.009765625,
          12952.490234375,
          13053.98046875,
          13090.650390625,
          13067.0400390625,
          13324.8603515625,
          13221.7998046875,
          13356.740234375,
          13723.76953125,
          13628.419921875,
          13775.259765625,
          13702.58984375,
          13793.5498046875,
          13918.83984375,
          13860.9501953125,
          13763.7900390625,
          13833.2099609375,
          13917.650390625,
          13810.5498046875,
          13970.509765625,
          13988.490234375,
          14010.169921875,
          14230,
          14251.9404296875,
          14336.6904296875,
          14295.150390625,
          14298.58984375,
          14238.919921875,
          14206.330078125,
          14145.240234375,
          14326.16015625,
          14268.1298828125,
          14273.7900390625,
          14348.1103515625,
          14180.7001953125,
          14256.2900390625,
          14306.98046875,
          14363.4501953125,
          14500.91015625,
          14485.9697265625,
          14704.5302734375,
          14720.25,
          14913.6396484375,
          15145.849609375,
          15059.51953125,
          15365.1298828125,
          15425.580078125,
          15549.5302734375,
          15550.16015625,
          15651.2001953125,
          15987.16015625,
          15676.4501953125,
          15716.6396484375,
          15934.849609375,
          15775.73046875,
          15984.8896484375,
          16006.2099609375,
          15955.16015625,
          15711.759765625,
          15519.849609375,
          15544.3603515625,
          15176.5595703125,
          15546.6904296875,
          15828.6396484375,
          15697.75,
          15805.759765625,
          16197.4501953125,
          16366.23046875,
          16376.91015625,
          16445.869140625,
          16320.8798828125,
          16329.5400390625,
          16376.7998046875,
          16190.0400390625,
          16127.8701171875,
          15992.3203125,
          16091.75,
          15759.580078125,
          15943.3701171875,
          15715.0703125,
          15921.4501953125,
          15947.2099609375,
          16241.5498046875,
          16256.580078125,
          16253.9296875,
          16312.98046875,
          16264.6103515625,
          16186.4599609375,
          16065.5,
          16250.2001953125,
          15994.9599609375,
          16010.5400390625,
          16140.3701171875,
          16411.669921875,
          16490.310546875,
          16529.23046875,
          16538.279296875,
          16771.349609375,
          16786.560546875,
          16974.279296875,
          16908.55078125,
          16872.400390625,
          16869.779296875,
          16851.060546875,
          17028.349609375,
          17141.25,
          17231.580078125,
          17282.759765625,
          17302.73046875,
          17117.490234375,
          17378.359375,
          17586.01953125,
          17610.720703125,
          17646.9296875,
          17505.51953125,
          17249.560546875,
          16968.75,
          16925.51953125,
          17032.23046875,
          17274.23046875,
          17137.189453125,
          16515.880859375,
          15668.330078125,
          15819.240234375,
          15554.990234375,
          15564.9599609375,
          16112.2099609375,
          16122.51953125,
          16136.8603515625,
          16246.51953125,
          16444.75,
          16645.169921875,
          16591.69921875,
          16690.0390625,
          16948.470703125,
          17098.490234375,
          17192.91015625,
          17201.30078125,
          17225.060546875,
          17176.08984375,
          17094.33984375,
          17082.51953125,
          17024.5390625,
          17200.44921875,
          17279.69921875,
          17357.369140625,
          17254.83984375,
          17384.740234375,
          17305.490234375,
          17162.7109375,
          17127.58984375,
          17358.5703125,
          17471.0703125,
          17531.8203125,
          17621.150390625,
          17648.25,
          17801.189453125,
          17745.689453125,
          17783.80078125,
          17929.220703125,
          17901.25,
          17880.900390625,
          17778.119140625,
          17843.30078125,
          17931.470703125,
          17892.509765625,
          17878.779296875,
          17839.580078125,
          17854.349609375,
          17724.919921875,
          17593.599609375,
          17482.810546875,
          17603.94921875,
          17554.9296875,
          17394.76953125,
          17252.869140625,
          17216.580078125,
          17414.76953125,
          17256.609375,
          17489.119140625,
          17588.640625,
          17624.8203125,
          17593.849609375,
          17505.009765625,
          17492.44921875,
          17308.619140625,
          17221.349609375,
          17216.33984375,
          16941.330078125,
          16841.619140625,
          16630.990234375,
          16777.779296875,
          16426.98046875,
          16459.130859375,
          16830.630859375,
          16821.080078125,
          17177.390625,
          17061.099609375,
          17244.619140625,
          17384.609375,
          17463.80078125,
          17455.919921875,
          17380.51953125,
          17534.05078125,
          17534.380859375,
          17411.529296875,
          17175.0390625,
          17270.279296875,
          17452.16015625,
          17463.779296875,
          17434.01953125,
          17332.33984375,
          17279.2890625,
          17196.7890625,
          16998.0703125,
          17130.740234375,
          17278.109375,
          17286.890625,
          17127.859375,
          16886.5703125,
          16883,
          16600.51953125,
          16362.26953125,
          16488.2109375,
          16465.5703125,
          16742.349609375,
          16589.720703125,
          16480.98046875,
          16392.509765625,
          16426.759765625,
          16816.939453125,
          16791.119140625,
          16936.19921875,
          16904.4609375,
          16900.3203125,
          16879.33984375,
          16909.3203125,
          17037.369140625,
          17081.169921875,
          17058.0390625,
          17021.76953125,
          17094.419921875,
          17087.890625,
          17158.279296875,
          17097.16015625,
          17323.390625,
          17433.099609375,
          17549.08984375,
          17527.1796875,
          17479.55078125,
          17560.51953125,
          17639.9609375,
          17705.2890625,
          17767.060546875,
          17864.189453125,
          17828.529296875,
          17797.900390625,
          17680.16015625,
          17686.119140625,
          17641.7890625,
          17320.009765625,
          17369.7890625,
          17428.609375,
          17575.91015625,
          17720.9296875,
          17650.669921875,
          17734.189453125,
          17845.060546875,
          17880.41015625,
          17893.2890625,
          17841.029296875,
          17754.380859375,
          17592.869140625,
          17719.060546875,
          17744.5390625,
          17812.810546875,
          17686.83984375,
          17805.740234375,
          17855.779296875,
          17966.349609375,
          17975.41015625,
          18099.7109375,
          18209.140625,
          18270,
          18260.23046875,
          18395.140625,
          18598.130859375,
          18395.720703125,
          18388.16015625,
          18095.390625,
          18266.5390625,
          18348.390625,
          18356.2109375,
          18509,
          18512.150390625,
          18492.810546875,
          18275.1796875,
          18212.259765625,
          18113.939453125,
          17843.330078125,
          17890.7109375,
          17657.970703125,
          17750.69921875,
          17955.94921875,
          18060.330078125,
          18217.2890625,
          18258.810546875,
          18182.73046875,
          17978.439453125,
          18109.169921875,
          18213.30078125,
          18250.369140625,
          18196.400390625,
          18159.509765625,
          17954.75,
          17939.529296875,
          17617.359375,
          17657.5,
          17864.109375,
          17932.150390625,
          17845.009765625,
          17581.5703125,
          17000,
          16944.080078125,
          17224.73046875,
          17373.8203125,
          17287.900390625,
          17177.26953125,
          17007.580078125,
          17172.810546875,
          17439.91015625,
          17503.94921875,
          17512.41015625,
          17581.619140625,
          17711.169921875,
          17695.80078125,
          17510.150390625,
          17568.3203125,
          17572.900390625,
          17754.099609375,
          17657.759765625,
          17484.189453125,
          17430.130859375,
          17210.400390625,
          17273.05078125,
          17005.83984375,
          17080.400390625,
          17353.25,
          17139.33984375,
          16958.66015625,
          16926.33984375,
          17057.130859375,
          17159.880859375,
          17088.759765625,
          16912.30078125,
          16677.83984375,
          16427.3203125,
          16350.7197265625,
          16599.83984375,
          16593.2109375,
          16531.369140625,
          16689.98046875,
          16491.109375,
          16345.83984375,
          15891.400390625,
          16053.75,
          15943.6201171875,
          15687.150390625,
          15943.6904296875,
          15915.9296875,
          16174.7802734375,
          16111.490234375,
          16061.919921875,
          16193.919921875,
          16166.5703125,
          15987.740234375,
          16128.2099609375,
          16075.91015625,
          16368.9296875,
          16561.6796875,
          16718.91015625,
          16605.4609375,
          16570.890625,
          16593.75,
          16538.669921875,
          16643.94921875,
          16581.810546875,
          16295.0703125,
          15975.169921875,
          16025.83984375
         ],
         "type": "candlestick",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "台灣加權指數走勢 (互動式圖表)"
        },
        "xaxis": {
         "rangeselector": {
          "buttons": [
           {
            "count": 1,
            "label": "1m",
            "step": "month",
            "stepmode": "backward"
           },
           {
            "count": 6,
            "label": "6m",
            "step": "month",
            "stepmode": "backward"
           },
           {
            "count": 1,
            "label": "YTD",
            "step": "year",
            "stepmode": "todate"
           },
           {
            "count": 1,
            "label": "1y",
            "step": "year",
            "stepmode": "backward"
           },
           {
            "step": "all"
           }
          ]
         },
         "rangeslider": {
          "visible": true
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"9f4ab9e9-0ce6-4d49-a558-ab173beb8646\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9f4ab9e9-0ce6-4d49-a558-ab173beb8646\")) {                    Plotly.newPlot(                        \"9f4ab9e9-0ce6-4d49-a558-ab173beb8646\",                        [{\"close\":[10530.5400390625,10566.740234375,10775.33984375,10785.009765625,10803.76953125,10779.4501953125,10706.7197265625,10652.5498046875,10773.900390625,10730.830078125,10895.4599609375,10865.1201171875,10743.76953125,10775.900390625,10785.73046875,10751.2197265625,10702.7802734375,10798.48046875,10843.419921875,10824.349609375,10876.4296875,10886.0498046875,10828.48046875,10799.2802734375,10873.1904296875,10944.5302734375,10947.259765625,10935.759765625,10941.41015625,10891.98046875,10885.73046875,10830.900390625,10823.8095703125,10731.75,10549.0400390625,10423.41015625,10394.75,10386.1796875,10494.490234375,10472.3603515625,10362.66015625,10427.73046875,10327.1298828125,10420.8896484375,10488.75,10522.5,10525.7998046875,10529.7802734375,10538.1103515625,10354.5703125,10387.23046875,10434.2900390625,10462.4296875,10618.0498046875,10634.849609375,10558.2099609375,10657.3095703125,10756.9296875,10780.6396484375,10753.580078125,10790.349609375,10827.5498046875,10898.1298828125,10874.5,10929.4501953125,10894.7001953125,10929.6904296875,10919.01953125,10918.009765625,10873.6904296875,10871.990234375,10829.6796875,10967.650390625,10947.8798828125,10875.91015625,10894.48046875,10935.0595703125,11017.3095703125,10889.9599609375,11066.9501953125,11111.7998046875,11162.830078125,11186.8798828125,11180.2197265625,11184.150390625,11271.25,11239.669921875,11320.1396484375,11296.1201171875,11315.01953125,11333.8701171875,11380.2802734375,11358.7099609375,11399.5302734375,11556.849609375,11644.0302734375,11653.0703125,11606.5595703125,11579.5400390625,11427.2802734375,11520.3701171875,11467.830078125,11450.419921875,11525.599609375,11599.7802734375,11656.400390625,11631.2001953125,11558.26953125,11566.7998046875,11561.580078125,11576.8203125,11647.4599609375,11617.080078125,11489.5703125,11502.830078125,11531.580078125,11510.4697265625,11594.650390625,11609.6396484375,11660.76953125,11627.83984375,11700.76953125,11836.419921875,11927.73046875,11939.76953125,12097.009765625,12122.4501953125,12018.900390625,11959.080078125,12022.23046875,11976.3798828125,12008.1298828125,12001.009765625,12091.58984375,12053.3701171875,11997.1396484375,12100.48046875,12110.4296875,11953.3603515625,11880.3203125,11817.099609375,11970.6298828125,12024.650390625,12113.419921875,12179.8095703125,12091.8798828125,12066.9296875,12090.2900390625,12118.7099609375,11421.740234375,11495.099609375,11354.919921875,11555.919921875,11573.6201171875,11749.6796875,11612.8095703125,11574.0703125,11664.0400390625,11774.1904296875,11791.7802734375,11815.7001953125,11763.509765625,11648.98046875,11758.83984375,11725.08984375,11686.349609375,11534.8701171875,11540.23046875,11433.6201171875,11292.169921875,11170.4599609375,11327.7197265625,11392.349609375,11514.8203125,11321.8095703125,10977.6396484375,11003.5400390625,10893.75,10422.3203125,10128.8701171875,9717.76953125,9439.6298828125,9218.669921875,8681.33984375,9234.08984375,8890.0302734375,9285.6201171875,9644.75,9736.3603515625,9698.919921875,9629.4296875,9708.0595703125,9663.6298828125,9818.740234375,9996.3896484375,10137.4697265625,10119.4296875,10157.6103515625,10099.2197265625,10332.9404296875,10447.2099609375,10375.48046875,10597.0400390625,10586.7099609375,10288.419921875,10307.740234375,10366.509765625,10347.3603515625,10567.26953125,10616.0595703125,10772.2197265625,10992.1396484375,10720.48046875,10774.6103515625,10774.98046875,10842.919921875,10901.419921875,11013.259765625,10879.4697265625,10938.26953125,10780.8798828125,10814.919921875,10740.5498046875,10860.4404296875,10907.7998046875,11008.3095703125,10811.150390625,10871.1796875,10997.2099609375,11014.66015625,10944.1904296875,10942.16015625,11079.01953125,11127.9296875,11320.16015625,11393.23046875,11479.400390625,11610.3203125,11637.1103515625,11720.16015625,11535.76953125,11429.9404296875,11306.259765625,11511.6396484375,11534.58984375,11548.330078125,11549.8603515625,11572.9296875,11612.3603515625,11660.669921875,11542.6201171875,11621.240234375,11703.419921875,11805.1396484375,11909.16015625,12116.7001953125,12092.9697265625,12170.1904296875,12192.6904296875,12073.6796875,12211.5595703125,12209.009765625,12202.849609375,12157.740234375,12181.5595703125,12174.5400390625,12397.5498046875,12473.26953125,12413.0400390625,12304.0400390625,12588.2998046875,12586.73046875,12540.9697265625,12722.919921875,12664.7998046875,12513.0302734375,12709.919921875,12802.2998046875,12913.5,12828.8701171875,12894.0,12780.1904296875,12670.349609375,12763.1298828125,12795.4599609375,12956.1103515625,12872.1396484375,12778.6396484375,12362.6396484375,12607.83984375,12647.1298828125,12758.25,12833.2900390625,12797.3095703125,12728.849609375,12591.4501953125,12703.2802734375,12699.5,12757.9697265625,12637.9501953125,12601.400390625,12663.5595703125,12608.580078125,12691.75,12675.9501953125,12787.8203125,12845.650390625,12976.759765625,12872.740234375,12875.6201171875,12795.1201171875,12645.509765625,12583.8798828125,12264.3798828125,12232.91015625,12462.759765625,12467.73046875,12515.6103515625,12548.2802734375,12704.23046875,12746.3701171875,12887.1904296875,12955.91015625,12947.1298828125,12919.3095703125,12827.8203125,12750.3701171875,12908.33984375,12862.3701171875,12877.25,12917.0302734375,12898.8203125,12909.0302734375,12875.009765625,12793.75,12662.91015625,12546.33984375,12591.3095703125,12736.009765625,12867.900390625,12918.7998046875,12973.5302734375,13127.4697265625,13081.7197265625,13262.1904296875,13221.7802734375,13273.330078125,13551.830078125,13593.009765625,13773.2900390625,13722.4296875,13716.4404296875,13878.009765625,13807.1298828125,13738.830078125,13845.66015625,13867.08984375,13722.8896484375,13885.669921875,13989.1396484375,13977.08984375,14132.4404296875,14256.599609375,14360.400390625,14390.1396484375,14249.490234375,14261.6904296875,14211.0498046875,14068.51953125,14304.4599609375,14258.9296875,14249.9599609375,14384.9599609375,14177.4599609375,14223.08984375,14280.2802734375,14331.419921875,14483.0703125,14472.0498046875,14687.7001953125,14732.5302734375,14902.0302734375,15000.0302734375,14983.1298828125,15214.0,15463.9501953125,15557.2998046875,15500.7001953125,15769.98046875,15707.1904296875,15616.3896484375,15612.0,15877.3701171875,15806.1796875,16153.76953125,16019.0302734375,15946.5400390625,15658.849609375,15701.4501953125,15415.8798828125,15138.3095703125,15410.08984375,15760.0498046875,15771.3203125,15706.2197265625,15802.400390625,16362.2900390625,16424.509765625,16341.3798828125,16410.16015625,16443.400390625,16212.5302734375,16452.1796875,15953.7998046875,15946.8798828125,16211.73046875,15906.41015625,15855.23046875,15820.1103515625,15853.08984375,15911.669921875,16179.5595703125,16255.1796875,16249.330078125,16313.16015625,16215.8203125,16287.83984375,16070.240234375,16189.2197265625,16177.58984375,16032.1201171875,16060.1396484375,16305.8798828125,16475.970703125,16554.900390625,16431.130859375,16571.279296875,16815.359375,16926.439453125,16854.099609375,16859.69921875,16824.91015625,16865.970703125,17076.73046875,17158.810546875,17263.279296875,17323.869140625,17202.109375,17096.970703125,17300.26953125,17572.2890625,17595.900390625,17567.529296875,17566.66015625,17222.349609375,16933.779296875,16843.439453125,16994.359375,17285.0,17235.609375,16583.130859375,15902.3701171875,15670.099609375,15827.08984375,15353.8896484375,16145.98046875,16132.66015625,16042.3603515625,16302.0595703125,16338.2900390625,16595.669921875,16643.689453125,16601.609375,16870.859375,17068.4296875,17162.380859375,17165.0390625,17246.16015625,17147.41015625,17083.91015625,17076.2109375,16966.220703125,17159.220703125,17213.51953125,17371.2890625,17307.859375,17390.609375,17318.5390625,17062.98046875,17075.55078125,17336.7109375,17407.9609375,17502.990234375,17590.970703125,17598.189453125,17755.4609375,17713.939453125,17710.150390625,17919.330078125,17913.0703125,17850.689453125,17866.08984375,17661.48046875,17814.330078125,17847.51953125,17845.75,18034.189453125,17895.25,17789.25,17528.740234375,17458.7890625,17572.330078125,17572.919921875,17403.560546875,17269.869140625,17135.220703125,17402.810546875,17247.41015625,17503.279296875,17553.759765625,17623.890625,17603.119140625,17526.279296875,17485.150390625,17323.640625,17227.1796875,17219.939453125,16982.109375,16858.76953125,16661.359375,16826.26953125,16375.400390625,16341.9404296875,16741.83984375,16818.73046875,17045.859375,17066.9609375,17209.9296875,17396.51953125,17490.2890625,17473.990234375,17319.759765625,17516.919921875,17495.30078125,17428.869140625,17270.490234375,17304.330078125,17474.5703125,17446.310546875,17434.900390625,17354.0,17278.69921875,17276.7890625,16925.8203125,17078.220703125,17260.189453125,17313.76953125,17181.439453125,16855.4609375,16934.76953125,16570.890625,16408.349609375,16460.75,16393.16015625,16713.859375,16640.4296875,16462.83984375,16347.990234375,16387.279296875,16781.189453125,16705.4609375,16900.669921875,16887.8203125,16889.509765625,16888.740234375,16894.240234375,17034.33984375,17074.55078125,17041.630859375,16987.41015625,17068.240234375,17065.970703125,17122.16015625,17078.859375,17296.900390625,17415.30078125,17541.359375,17559.650390625,17452.51953125,17518.130859375,17634.470703125,17693.130859375,17764.0390625,17841.369140625,17818.310546875,17803.5390625,17666.119140625,17642.51953125,17654.189453125,17369.390625,17328.08984375,17427.759765625,17585.990234375,17724.880859375,17697.140625,17688.2109375,17796.919921875,17832.419921875,17914.119140625,17826.259765625,17767.599609375,17599.369140625,17660.099609375,17785.740234375,17812.58984375,17669.109375,17789.26953125,17826.830078125,17946.66015625,17961.640625,18048.939453125,18196.810546875,18248.279296875,18218.83984375,18270.509765625,18526.349609375,18499.9609375,18367.919921875,18169.759765625,18239.380859375,18288.2109375,18375.400390625,18436.9296875,18403.330078125,18525.439453125,18378.640625,18227.4609375,18218.279296875,17899.30078125,17989.0390625,17701.119140625,17674.400390625,17900.30078125,17966.560546875,18151.759765625,18338.05078125,18310.939453125,17997.669921875,17951.810546875,18231.470703125,18268.5703125,18232.349609375,18221.490234375,17969.2890625,18055.73046875,17594.55078125,17652.1796875,17898.25,17867.599609375,17934.400390625,17736.51953125,17178.689453125,16825.25,17015.359375,17433.19921875,17264.740234375,17263.0390625,16926.060546875,16940.830078125,17448.220703125,17456.51953125,17560.359375,17559.7109375,17731.369140625,17699.060546875,17676.94921875,17520.009765625,17548.66015625,17740.560546875,17693.470703125,17625.58984375,17522.5,17178.630859375,17284.5390625,17048.369140625,16990.91015625,17301.650390625,17245.650390625,17004.1796875,16898.869140625,16993.400390625,17148.880859375,17127.94921875,17025.08984375,16620.900390625,16644.7890625,16303.349609375,16419.380859375,16592.1796875,16498.900390625,16565.830078125,16696.119140625,16408.19921875,16048.919921875,16061.7001953125,16006.25,15616.6796875,15832.5400390625,15901.0400390625,16056.08984375,16296.8603515625,16020.3203125,16144.849609375,16156.41015625,15963.6298828125,16104.0302734375,15968.830078125,16266.2197265625,16610.619140625,16807.76953125,16675.08984375,16552.5703125,16605.9609375,16512.880859375,16670.509765625,16621.33984375,16460.119140625,16070.98046875,16047.3701171875,15999.25],\"high\":[10562.9697265625,10573.76953125,10778.6396484375,10799.1396484375,10840.2900390625,10786.330078125,10787.6201171875,10690.98046875,10800.2900390625,10788.080078125,10914.849609375,10888.2099609375,10812.650390625,10791.2802734375,10812.3798828125,10781.1103515625,10733.5703125,10810.16015625,10856.509765625,10867.58984375,10880.5595703125,10899.83984375,10861.1103515625,10835.4296875,10919.9599609375,10949.759765625,10994.4404296875,10976.4599609375,10945.919921875,10907.4404296875,10905.740234375,10927.16015625,10835.7998046875,10773.25,10641.2802734375,10542.490234375,10425.599609375,10461.3701171875,10500.76953125,10514.0498046875,10436.6201171875,10518.01953125,10374.75,10456.3203125,10516.0498046875,10538.1201171875,10545.25,10596.41015625,10541.73046875,10453.400390625,10431.7802734375,10456.2998046875,10466.990234375,10618.0498046875,10634.849609375,10650.150390625,10670.4296875,10766.599609375,10803.0703125,10810.0302734375,10815.58984375,10862.6796875,10898.1298828125,10912.23046875,10971.5400390625,10954.1796875,10931.759765625,10948.1904296875,10947.5302734375,10885.669921875,10934.91015625,10921.169921875,10967.650390625,10965.2001953125,10888.400390625,10941.490234375,10954.4501953125,11020.509765625,10989.3896484375,11085.490234375,11126.4501953125,11180.1103515625,11186.8798828125,11237.3798828125,11188.1298828125,11271.25,11281.1201171875,11320.1396484375,11347.1796875,11341.0302734375,11373.9501953125,11380.2802734375,11428.419921875,11399.5302734375,11559.0703125,11644.0302734375,11668.2001953125,11647.4697265625,11642.41015625,11570.330078125,11520.3701171875,11509.3701171875,11498.25,11552.25,11599.7802734375,11656.400390625,11649.0703125,11573.83984375,11577.01953125,11604.75,11642.9599609375,11651.73046875,11654.0595703125,11623.58984375,11524.849609375,11531.580078125,11513.830078125,11604.3896484375,11657.650390625,11678.3603515625,11649.73046875,11700.76953125,11875.98046875,11990.7802734375,11975.73046875,12097.009765625,12125.900390625,12111.76953125,12028.2197265625,12022.23046875,12027.599609375,12032.3701171875,12030.66015625,12093.01953125,12114.8896484375,12047.75,12110.740234375,12197.6396484375,12040.080078125,11986.0302734375,11899.669921875,11992.5498046875,12038.2099609375,12113.419921875,12186.6201171875,12169.919921875,12075.5400390625,12117.5498046875,12151.419921875,11933.23046875,11594.2099609375,11365.900390625,11581.3603515625,11620.419921875,11749.6796875,11712.7099609375,11598.0302734375,11678.580078125,11801.51953125,11854.98046875,11840.7900390625,11775.5595703125,11717.099609375,11783.4296875,11827.830078125,11755.169921875,11615.2802734375,11567.08984375,11494.2197265625,11470.23046875,11282.0498046875,11390.240234375,11392.349609375,11525.3203125,11471.3603515625,11221.759765625,11032.4697265625,11088.23046875,10845.3896484375,10171.2001953125,10130.7998046875,9677.0703125,9509.6103515625,9085.2802734375,9264.419921875,9029.51953125,9415.6396484375,9722.3701171875,9739.73046875,9954.8603515625,9655.7001953125,9789.8095703125,9736.0,9818.740234375,10039.150390625,10149.759765625,10246.83984375,10165.1201171875,10179.400390625,10344.1298828125,10461.83984375,10425.2900390625,10710.150390625,10637.6396484375,10544.7998046875,10310.849609375,10462.7001953125,10386.9296875,10578.9599609375,10620.51953125,10794.830078125,11012.7802734375,10781.509765625,10833.5302734375,10828.73046875,10890.8701171875,10970.7001953125,11039.1103515625,10974.509765625,10938.26953125,10894.169921875,10904.009765625,10812.8701171875,10924.7900390625,10933.98046875,11021.66015625,10977.5498046875,10874.5400390625,11046.2197265625,11069.7197265625,11087.5302734375,10965.0302734375,11109.2900390625,11170.490234375,11330.51953125,11425.419921875,11482.3603515625,11631.8896484375,11640.4501953125,11740.900390625,11771.1201171875,11442.2998046875,11469.83984375,11542.25,11550.3203125,11567.419921875,11584.8798828125,11632.8798828125,11679.400390625,11701.23046875,11608.7998046875,11638.0,11736.3896484375,11805.2099609375,11933.58984375,12116.7001953125,12249.9501953125,12190.26953125,12273.4296875,12253.580078125,12216.240234375,12228.3095703125,12320.48046875,12220.9296875,12266.9296875,12221.330078125,12450.16015625,12486.9501953125,12429.759765625,12502.83984375,12686.3603515625,13031.7001953125,12660.8701171875,12769.4697265625,12733.48046875,12673.01953125,12709.919921875,12816.3896484375,12971.8701171875,12912.6298828125,12933.16015625,12906.009765625,12730.9599609375,12812.7998046875,12801.3095703125,12956.1103515625,12981.580078125,12950.1103515625,12764.75,12638.650390625,12702.9404296875,12814.5595703125,12833.2900390625,12960.6796875,12799.16015625,12831.3798828125,12703.2802734375,12802.6103515625,12857.7900390625,12655.669921875,12725.0498046875,12702.5498046875,12616.5703125,12708.6396484375,12701.7900390625,12793.0703125,12857.73046875,13021.6796875,12972.51953125,12927.8095703125,12952.3095703125,12786.91015625,12700.8896484375,12487.48046875,12385.8095703125,12462.759765625,12571.33984375,12568.6796875,12637.6201171875,12706.3896484375,12774.3603515625,12887.1904296875,12997.7900390625,12994.66015625,12960.0,12909.1103515625,12896.2998046875,12911.41015625,12917.75,12942.1201171875,12917.0302734375,12963.259765625,12971.580078125,12884.3203125,12879.98046875,12699.8203125,12656.2900390625,12594.33984375,12760.1396484375,12885.2802734375,12918.7998046875,12999.16015625,13149.900390625,13142.2197265625,13262.1904296875,13324.8603515625,13273.330078125,13551.830078125,13780.1201171875,13773.2900390625,13785.919921875,13726.2197265625,13921.16015625,13951.169921875,13893.2001953125,13856.759765625,13885.009765625,13969.3896484375,13885.669921875,13995.7001953125,14049.580078125,14149.5595703125,14306.8701171875,14367.0,14427.41015625,14319.3798828125,14353.169921875,14270.33984375,14256.5498046875,14339.5,14338.400390625,14329.1201171875,14384.9599609375,14411.9296875,14247.7900390625,14324.419921875,14400.830078125,14483.0703125,14547.0703125,14695.4404296875,14760.0595703125,14937.1298828125,15000.0302734375,15197.6796875,15270.400390625,15463.9501953125,15557.2998046875,15642.0302734375,15778.51953125,15760.150390625,16041.58984375,15676.4501953125,15928.1396484375,16004.3203125,16238.4599609375,16138.0400390625,16014.9599609375,16023.8701171875,15837.4697265625,15557.01953125,15603.41015625,15429.98046875,15838.150390625,15896.400390625,15801.3701171875,15938.08984375,16406.779296875,16517.73046875,16382.7001953125,16579.169921875,16467.75,16456.939453125,16474.05078125,16190.0400390625,16262.91015625,16211.73046875,16091.75,15934.4296875,16074.9697265625,15864.259765625,15986.0703125,16216.2197265625,16298.0302734375,16281.91015625,16340.66015625,16349.2099609375,16410.01953125,16186.4599609375,16235.6298828125,16351.3798828125,16125.5400390625,16146.580078125,16325.6201171875,16520.890625,16556.189453125,16550.19921875,16602.470703125,16816.33984375,16926.720703125,17016.130859375,16979.349609375,17041.369140625,16935.390625,17076.73046875,17158.810546875,17294.150390625,17323.869140625,17282.759765625,17428.150390625,17300.26953125,17572.2890625,17630.189453125,17628.810546875,17709.23046875,17546.939453125,17328.55078125,17052.19921875,17088.73046875,17285.0,17304.470703125,17137.189453125,16552.630859375,16031.9296875,16075.48046875,15719.740234375,16153.76953125,16273.98046875,16154.080078125,16431.140625,16397.58984375,16657.599609375,16706.2890625,16601.609375,16889.009765625,17113.330078125,17184.30078125,17274.66015625,17311.279296875,17225.060546875,17189.23046875,17181.51953125,17082.51953125,17159.220703125,17278.55078125,17371.2890625,17398.220703125,17390.609375,17417.5,17305.490234375,17270.470703125,17375.7890625,17439.630859375,17597.330078125,17595.509765625,17713.240234375,17797.26953125,17863.900390625,17795.880859375,17945.51953125,18008.369140625,17933.619140625,17935.109375,17778.119140625,17947.900390625,18018.0390625,17940.859375,18034.189453125,17926.380859375,17854.349609375,17724.919921875,17707.560546875,17667.380859375,17672.599609375,17637.359375,17459.349609375,17252.869140625,17402.810546875,17429.009765625,17503.279296875,17553.759765625,17636.109375,17643.970703125,17593.849609375,17510.08984375,17524.91015625,17327.19921875,17233.109375,17216.33984375,16983.58984375,16870.55078125,16826.26953125,16777.779296875,16507.109375,16785.2890625,16904.30078125,17045.859375,17201.419921875,17229.890625,17396.51953125,17490.2890625,17503.9296875,17523.16015625,17540.779296875,17633.669921875,17559.2109375,17447.580078125,17319.08984375,17474.5703125,17482.5703125,17529.470703125,17439.75,17411.369140625,17408.7109375,17196.7890625,17145.25,17273.58984375,17335.990234375,17286.890625,17127.859375,16994.2109375,16883.0,16680.2890625,16460.75,16568.2109375,16731.580078125,16771.7109375,16589.720703125,16543.330078125,16519.9609375,16781.189453125,16862.029296875,16916.720703125,16973.859375,17026.890625,16932.890625,16930.98046875,17067.150390625,17079.599609375,17104.859375,17073.970703125,17119.4296875,17237.08984375,17153.759765625,17219.650390625,17296.900390625,17415.30078125,17581.51953125,17575.439453125,17527.1796875,17602.05078125,17683.189453125,17708.099609375,17764.0390625,17841.369140625,17986.1796875,17857.73046875,17797.900390625,17722.890625,17724.5,17641.7890625,17415.630859375,17535.23046875,17626.2109375,17741.55078125,17781.140625,17763.419921875,17796.919921875,17988.880859375,17925.119140625,17893.2890625,17919.349609375,17754.380859375,17678.779296875,17814.3203125,17822.740234375,17812.810546875,17825.890625,17870.16015625,17960.990234375,18039.849609375,18099.779296875,18197.359375,18283.25,18291.25,18379.689453125,18526.349609375,18619.609375,18427.150390625,18444.119140625,18243.259765625,18293.529296875,18394.69921875,18459.73046875,18509.0,18535.419921875,18575.41015625,18359.140625,18292.140625,18113.939453125,18004.44921875,17890.7109375,17776.419921875,17900.30078125,18063.55078125,18168.599609375,18338.05078125,18310.939453125,18182.73046875,18085.91015625,18233.7890625,18330.630859375,18262.9609375,18253.099609375,18159.509765625,18109.279296875,17939.529296875,17737.3203125,17944.5,17918.98046875,18026.029296875,17845.009765625,17581.5703125,17113.669921875,17072.740234375,17478.4296875,17373.900390625,17363.0390625,17177.26953125,17085.76953125,17472.94921875,17472.380859375,17603.990234375,17573.2890625,17738.689453125,17711.169921875,17747.130859375,17527.150390625,17585.91015625,17770.0703125,17767.810546875,17657.759765625,17528.990234375,17438.599609375,17316.689453125,17316.349609375,17100.849609375,17333.2109375,17374.140625,17141.330078125,16999.359375,17106.259765625,17148.880859375,17213.66015625,17088.759765625,16912.30078125,16729.470703125,16427.3203125,16455.5703125,16663.66015625,16604.869140625,16617.060546875,16783.779296875,16491.109375,16345.83984375,16071.5,16081.150390625,15943.6201171875,15860.6904296875,16032.849609375,16085.849609375,16316.580078125,16111.490234375,16181.66015625,16249.4501953125,16219.7998046875,16169.1796875,16179.009765625,16266.2197265625,16610.619140625,16807.76953125,16811.0390625,16617.259765625,16654.119140625,16593.75,16702.990234375,16643.94921875,16581.810546875,16295.0703125,16067.7998046875,16106.75],\"low\":[10474.1904296875,10521.7001953125,10650.48046875,10745.25,10773.4697265625,10695.490234375,10693.0400390625,10645.3701171875,10674.2001953125,10719.080078125,10821.2998046875,10843.6396484375,10720.66015625,10750.900390625,10756.669921875,10708.919921875,10680.849609375,10723.23046875,10799.650390625,10823.8896484375,10769.5703125,10865.0,10804.33984375,10792.26953125,10861.990234375,10909.150390625,10932.5302734375,10912.919921875,10879.400390625,10879.240234375,10863.330078125,10827.740234375,10754.0703125,10714.73046875,10524.6796875,10418.25,10180.0400390625,10366.5400390625,10383.6201171875,10440.76953125,10361.8896484375,10427.73046875,10287.759765625,10318.3701171875,10449.3896484375,10497.1298828125,10504.490234375,10515.7197265625,10503.0703125,10340.150390625,10368.9599609375,10397.599609375,10406.490234375,10504.2802734375,10589.419921875,10557.0302734375,10566.3203125,10714.26953125,10753.4501953125,10732.7802734375,10746.8701171875,10804.01953125,10822.6904296875,10866.4296875,10878.33984375,10874.66015625,10903.4501953125,10905.919921875,10886.6201171875,10825.849609375,10856.9599609375,10827.6103515625,10855.599609375,10927.3896484375,10809.2802734375,10854.1201171875,10919.4296875,10986.009765625,10889.9599609375,11004.740234375,11091.2900390625,11113.1796875,11131.4501953125,11154.8603515625,11147.8603515625,11218.25,11209.099609375,11254.58984375,11281.3701171875,11295.4404296875,11292.830078125,11306.9501953125,11358.7099609375,11335.509765625,11451.7197265625,11576.1796875,11601.7001953125,11552.080078125,11561.3095703125,11409.830078125,11459.6796875,11439.08984375,11424.76953125,11485.4599609375,11532.9697265625,11576.7900390625,11591.73046875,11478.4296875,11534.150390625,11559.330078125,11576.8203125,11590.8095703125,11604.6796875,11485.16015625,11454.3798828125,11460.0595703125,11457.4296875,11546.75,11577.830078125,11631.0302734375,11607.259765625,11622.580078125,11766.98046875,11913.01953125,11915.6103515625,11939.6201171875,12059.919921875,12008.16015625,11937.4501953125,11960.2099609375,11976.3798828125,11973.9501953125,11982.6396484375,12032.7197265625,12046.2197265625,11997.1396484375,12026.23046875,12023.599609375,11953.3603515625,11822.400390625,11777.4501953125,11889.4599609375,11959.0595703125,12037.240234375,12140.26953125,12048.009765625,12006.080078125,12055.91015625,12101.5498046875,11418.2197265625,11436.9501953125,11138.0302734375,11393.0302734375,11512.7099609375,11605.1103515625,11592.08984375,11423.33984375,11614.25,11693.400390625,11784.9404296875,11788.8701171875,11724.8203125,11642.9501953125,11654.16015625,11714.150390625,11661.6298828125,11512.080078125,11415.4697265625,11408.0595703125,11274.51953125,11049.849609375,11279.8603515625,11297.8095703125,11454.650390625,11310.3203125,10977.6396484375,10830.2197265625,10885.91015625,10359.669921875,9636.150390625,9717.76953125,9371.349609375,9218.669921875,8523.6298828125,8816.8603515625,8750.1396484375,9083.7802734375,9426.4296875,9565.01953125,9691.1396484375,9415.51953125,9630.2998046875,9663.6298828125,9651.51953125,9928.16015625,9984.66015625,10092.3603515625,10103.76953125,10080.650390625,10130.650390625,10366.76953125,10317.1298828125,10552.580078125,10542.9599609375,10278.9501953125,10140.08984375,10294.8798828125,10324.2001953125,10407.490234375,10537.7099609375,10656.400390625,10826.259765625,10658.5,10735.009765625,10708.099609375,10775.16015625,10883.2197265625,10942.1796875,10854.509765625,10828.9296875,10777.4599609375,10730.5498046875,10730.7001953125,10812.7900390625,10860.150390625,10933.2099609375,10804.5595703125,10719.25,10903.3203125,10972.25,10915.2998046875,10861.900390625,10971.400390625,11102.9697265625,11182.51953125,11341.580078125,11411.9404296875,11528.4404296875,11537.3798828125,11621.01953125,11516.58984375,11244.6396484375,11299.4501953125,11380.669921875,11482.3203125,11485.3896484375,11542.3701171875,11553.91015625,11530.849609375,11637.7900390625,11500.650390625,11563.599609375,11622.6396484375,11690.9501953125,11857.900390625,11941.83984375,12028.5703125,12083.83984375,12181.33984375,12030.2001953125,12109.6103515625,12143.91015625,12162.51953125,12107.0400390625,12144.6201171875,12065.7998046875,12242.3203125,12389.400390625,12347.8603515625,12266.5498046875,12586.91015625,12533.1904296875,12488.009765625,12616.830078125,12635.7099609375,12506.58984375,12577.8798828125,12739.990234375,12851.2001953125,12791.1796875,12786.7998046875,12780.1904296875,12625.5400390625,12717.1298828125,12679.1904296875,12800.6904296875,12840.25,12778.6396484375,12144.759765625,12462.51953125,12567.9501953125,12698.849609375,12734.6904296875,12786.4501953125,12674.5703125,12591.4501953125,12565.830078125,12646.4697265625,12732.3203125,12559.7802734375,12575.3603515625,12614.7197265625,12480.5,12650.400390625,12616.6796875,12680.41015625,12773.23046875,12948.7197265625,12851.51953125,12841.08984375,12795.08984375,12632.740234375,12548.0703125,12264.3798828125,12149.8095703125,12282.3603515625,12429.7197265625,12466.580078125,12519.1201171875,12644.08984375,12619.8095703125,12818.009765625,12898.8896484375,12857.099609375,12895.7900390625,12786.259765625,12750.3701171875,12803.3203125,12862.3701171875,12846.419921875,12827.400390625,12875.23046875,12894.1796875,12821.0703125,12761.330078125,12583.3701171875,12546.33984375,12480.66015625,12641.2802734375,12736.01953125,12840.400390625,12927.2998046875,13048.669921875,13022.919921875,13067.0400390625,13193.740234375,13170.1201171875,13356.740234375,13593.009765625,13608.7197265625,13700.0400390625,13666.0703125,13793.5498046875,13798.3203125,13731.8095703125,13763.7900390625,13811.8095703125,13722.8896484375,13749.7099609375,13894.7001953125,13940.0703125,14010.169921875,14142.01953125,14184.580078125,14336.6904296875,14191.849609375,14081.419921875,14182.25,14053.349609375,14145.240234375,14213.490234375,14245.599609375,14166.8896484375,14175.6904296875,14134.8701171875,14256.2900390625,14296.9697265625,14363.4501953125,14435.0302734375,14476.6904296875,14646.330078125,14720.25,14861.990234375,14837.0,15049.8603515625,15275.3798828125,15395.73046875,15421.23046875,15550.16015625,15620.9599609375,15615.1103515625,15320.9697265625,15716.6396484375,15745.48046875,15775.73046875,15973.01953125,15772.6298828125,15589.2099609375,15642.1201171875,15367.4501953125,15138.3095703125,15089.9599609375,15546.6904296875,15741.16015625,15606.740234375,15774.33984375,16197.4501953125,16323.2197265625,16211.25,16410.16015625,16211.75,16212.5302734375,16322.26953125,15953.7998046875,15946.8798828125,15884.5498046875,15840.990234375,15636.4296875,15816.5,15657.919921875,15857.3203125,15947.2099609375,16166.349609375,16194.91015625,16244.990234375,16166.349609375,16264.6103515625,16022.169921875,15983.76953125,16166.2900390625,15967.9501953125,15944.9599609375,16140.3701171875,16411.669921875,16438.400390625,16427.19921875,16496.16015625,16715.3203125,16735.779296875,16815.58984375,16793.5390625,16802.220703125,16559.5703125,16851.060546875,16998.91015625,17135.51953125,17175.01953125,17167.390625,17066.169921875,17055.48046875,17378.359375,17489.7109375,17497.740234375,17548.30078125,17222.349609375,16647.609375,16843.439453125,16764.7109375,17032.23046875,17188.150390625,16460.869140625,15165.26953125,15368.5400390625,15702.099609375,15159.8603515625,15564.9599609375,16009.759765625,15943.4501953125,16136.8603515625,16136.9697265625,16444.75,16523.23046875,16419.419921875,16690.0390625,16939.91015625,17060.44921875,17056.419921875,17198.890625,17084.490234375,16775.849609375,17056.529296875,16907.4296875,16978.009765625,17193.609375,17279.69921875,17275.01953125,17150.51953125,17318.5390625,17023.310546875,17075.55078125,17127.58984375,17319.890625,17471.0703125,17481.23046875,17541.140625,17648.25,17644.359375,17676.98046875,17783.80078125,17895.779296875,17756.8203125,17742.080078125,17597.4609375,17759.900390625,17786.759765625,17716.439453125,17878.779296875,17779.609375,17708.150390625,17500.689453125,17352.240234375,17482.810546875,17511.859375,17403.560546875,17264.5,16893.69921875,17190.48046875,17237.669921875,17231.220703125,17456.19921875,17557.1796875,17566.720703125,17469.6796875,17350.150390625,17305.060546875,17090.25,17139.2109375,16978.109375,16773.5703125,16657.630859375,16418.5390625,16375.400390625,16248.080078125,16459.130859375,16779.900390625,16821.080078125,16984.689453125,17000.259765625,17244.619140625,17207.5703125,17415.51953125,17319.759765625,17380.51953125,17461.0703125,17388.369140625,17167.080078125,17122.94921875,17270.279296875,17387.5703125,17424.5390625,17316.51953125,17254.099609375,17235.44921875,16838.580078125,16998.0703125,17130.740234375,17235.6796875,17113.470703125,16801.779296875,16767.19921875,16503.740234375,16380.0595703125,16162.169921875,16303.6298828125,16465.5703125,16605.490234375,16349.490234375,16328.4697265625,16347.8798828125,16426.759765625,16695.890625,16772.150390625,16855.810546875,16873.990234375,16800.490234375,16784.109375,16909.3203125,16973.16015625,16994.369140625,16920.6796875,17021.76953125,17026.630859375,17080.509765625,17061.4609375,17097.16015625,17279.4609375,17433.099609375,17489.869140625,17403.48046875,17479.55078125,17560.51953125,17629.80078125,17669.580078125,17748.2109375,17786.05078125,17790.640625,17650.0703125,17588.779296875,17609.509765625,17330.439453125,17167.240234375,17369.7890625,17374.58984375,17559.439453125,17670.3203125,17585.94921875,17642.3203125,17832.419921875,17807.119140625,17767.80078125,17767.599609375,17566.990234375,17556.869140625,17719.060546875,17718.26953125,17646.390625,17652.3203125,17799.720703125,17855.779296875,17953.5703125,17975.41015625,18099.7109375,18192.859375,18216.44921875,18238.470703125,18395.140625,18446.51953125,18253.8203125,18134.41015625,18043.970703125,18135.4296875,18255.380859375,18346.810546875,18213.439453125,18435.01953125,18378.640625,18199.349609375,18125.19921875,17851.390625,17682.330078125,17645.66015625,17633.029296875,17712.349609375,17955.94921875,18039.23046875,18145.0390625,18191.75,17965.220703125,17942.6796875,18109.169921875,18190.240234375,18098.51953125,18129.650390625,17840.380859375,17954.75,17561.0703125,17554.970703125,17657.5,17784.640625,17906.240234375,17710.5390625,17135.6796875,16764.779296875,16944.080078125,17224.73046875,17239.900390625,17217.580078125,16911.94921875,16808.4296875,17172.810546875,17359.5,17503.94921875,17468.55078125,17581.619140625,17603.220703125,17633.859375,17368.66015625,17493.009765625,17572.900390625,17682.529296875,17465.609375,17381.0703125,17178.630859375,17210.400390625,17046.669921875,16905.130859375,17080.400390625,17245.650390625,17004.1796875,16845.509765625,16926.33984375,16983.55078125,17106.369140625,16923.869140625,16579.890625,16582.109375,16219.41015625,16256.8798828125,16521.939453125,16465.990234375,16514.30078125,16650.810546875,16312.169921875,16048.919921875,15734.4404296875,15953.26953125,15616.6796875,15687.150390625,15847.4599609375,15915.9296875,16172.7998046875,15892.73046875,16058.2001953125,16125.4501953125,15963.6298828125,15980.900390625,15949.6103515625,16075.91015625,16368.9296875,16493.0703125,16649.91015625,16540.55078125,16509.009765625,16465.869140625,16538.669921875,16557.5390625,16403.330078125,16055.8896484375,15869.0595703125,15981.5595703125],\"open\":[10488.7001953125,10547.150390625,10650.48046875,10749.41015625,10817.7099609375,10734.25,10777.1796875,10661.3896484375,10674.2001953125,10786.66015625,10821.2998046875,10878.009765625,10793.099609375,10755.8701171875,10785.849609375,10742.8095703125,10729.830078125,10723.23046875,10817.599609375,10855.16015625,10819.919921875,10865.0,10861.1103515625,10821.919921875,10861.990234375,10910.5,10963.8603515625,10969.740234375,10892.25,10898.25,10872.6298828125,10909.98046875,10824.150390625,10773.0595703125,10641.2802734375,10528.8701171875,10304.8798828125,10422.8896484375,10383.6201171875,10491.7900390625,10436.6201171875,10444.5,10374.75,10345.419921875,10449.3896484375,10508.419921875,10526.8701171875,10560.009765625,10526.150390625,10453.400390625,10386.2099609375,10397.599609375,10440.150390625,10504.2802734375,10613.1904296875,10629.2197265625,10566.3203125,10718.400390625,10783.4599609375,10791.9501953125,10787.650390625,10830.25,10822.6904296875,10908.849609375,10887.419921875,10947.6201171875,10908.099609375,10924.8095703125,10917.4404296875,10885.669921875,10909.5703125,10904.0,10855.599609375,10945.919921875,10866.5,10907.6904296875,10932.240234375,10986.009765625,10976.9296875,11004.740234375,11103.5,11166.169921875,11156.1396484375,11190.2099609375,11166.240234375,11221.1904296875,11266.1796875,11266.83984375,11336.6396484375,11321.990234375,11347.3896484375,11336.9697265625,11393.6796875,11373.7001953125,11451.7197265625,11576.1796875,11645.080078125,11642.91015625,11620.8896484375,11570.330078125,11463.330078125,11483.240234375,11488.740234375,11485.4599609375,11543.7998046875,11608.48046875,11645.73046875,11573.83984375,11556.75,11590.8095703125,11608.3701171875,11590.8095703125,11641.7001953125,11616.8798828125,11509.9404296875,11473.3203125,11511.8203125,11546.75,11639.7998046875,11635.4697265625,11647.7802734375,11635.0703125,11766.98046875,11937.900390625,11915.6103515625,11939.6201171875,12061.099609375,12082.98046875,12001.5595703125,11969.1396484375,12023.7900390625,11978.9404296875,12018.3798828125,12032.7197265625,12094.8095703125,12019.1904296875,12026.5,12167.4404296875,12035.7099609375,11961.9697265625,11818.759765625,11889.4599609375,12009.26953125,12069.6103515625,12161.73046875,12169.919921875,12006.080078125,12080.7197265625,12107.5595703125,11933.23046875,11494.0302734375,11365.900390625,11399.4599609375,11601.5498046875,11605.1103515625,11712.7099609375,11514.7197265625,11614.25,11693.400390625,11813.5,11806.51953125,11770.2998046875,11700.91015625,11655.3701171875,11785.7802734375,11721.16015625,11615.2802734375,11506.740234375,11468.599609375,11436.9599609375,11184.66015625,11279.8603515625,11368.25,11454.650390625,11471.3603515625,11221.759765625,10907.6201171875,11022.8203125,10845.3896484375,10091.0302734375,10069.419921875,9538.6396484375,9453.98046875,9085.2802734375,8816.8603515625,9025.5498046875,9083.7802734375,9426.4296875,9667.1396484375,9807.900390625,9571.2197265625,9689.6201171875,9726.2001953125,9707.75,9928.16015625,10010.6396484375,10173.259765625,10105.4296875,10147.5595703125,10130.650390625,10366.76953125,10385.7802734375,10554.5498046875,10612.4501953125,10544.7998046875,10256.1103515625,10370.1298828125,10367.9697265625,10407.490234375,10580.25,10656.400390625,10826.259765625,10781.509765625,10756.9501953125,10771.08984375,10778.5302734375,10883.2197265625,10942.1796875,10974.509765625,10861.009765625,10894.169921875,10833.16015625,10753.2099609375,10812.7900390625,10892.759765625,10933.2099609375,10977.5498046875,10812.9404296875,10903.3203125,11023.9296875,11040.6201171875,10921.16015625,10971.400390625,11109.26953125,11182.51953125,11373.9404296875,11418.009765625,11539.0,11600.6103515625,11635.990234375,11738.490234375,11406.5400390625,11436.2802734375,11380.669921875,11525.5,11534.2001953125,11557.26953125,11553.91015625,11618.419921875,11647.8603515625,11608.7998046875,11563.599609375,11622.6396484375,11694.099609375,11857.900390625,11941.83984375,12176.7900390625,12086.1298828125,12222.900390625,12250.4404296875,12109.6103515625,12202.8896484375,12233.990234375,12173.0400390625,12195.7197265625,12205.25,12242.3203125,12389.759765625,12423.01953125,12467.4296875,12618.6904296875,12951.7197265625,12530.740234375,12691.8603515625,12653.4599609375,12642.73046875,12577.8798828125,12762.33984375,12894.240234375,12901.4296875,12786.7998046875,12856.4599609375,12713.6298828125,12758.8701171875,12709.9697265625,12813.9404296875,12974.9599609375,12904.83984375,12764.75,12462.51953125,12629.7802734375,12706.400390625,12768.9599609375,12888.650390625,12759.2998046875,12763.4404296875,12617.3896484375,12772.4296875,12774.9697265625,12645.919921875,12645.9296875,12637.419921875,12592.1796875,12665.740234375,12697.4599609375,12680.41015625,12787.419921875,12989.8095703125,12920.150390625,12858.599609375,12874.6103515625,12786.91015625,12656.2998046875,12487.48046875,12312.9404296875,12282.3603515625,12488.099609375,12483.650390625,12567.8798828125,12644.08984375,12667.509765625,12832.5400390625,12959.150390625,12988.4404296875,12927.08984375,12892.2900390625,12814.16015625,12803.3203125,12889.990234375,12889.8798828125,12854.9697265625,12945.1201171875,12923.5498046875,12867.76953125,12853.759765625,12687.6396484375,12651.349609375,12565.3203125,12641.2802734375,12766.8603515625,12878.009765625,12952.490234375,13053.98046875,13090.650390625,13067.0400390625,13324.8603515625,13221.7998046875,13356.740234375,13723.76953125,13628.419921875,13775.259765625,13702.58984375,13793.5498046875,13918.83984375,13860.9501953125,13763.7900390625,13833.2099609375,13917.650390625,13810.5498046875,13970.509765625,13988.490234375,14010.169921875,14230.0,14251.9404296875,14336.6904296875,14295.150390625,14298.58984375,14238.919921875,14206.330078125,14145.240234375,14326.16015625,14268.1298828125,14273.7900390625,14348.1103515625,14180.7001953125,14256.2900390625,14306.98046875,14363.4501953125,14500.91015625,14485.9697265625,14704.5302734375,14720.25,14913.6396484375,15145.849609375,15059.51953125,15365.1298828125,15425.580078125,15549.5302734375,15550.16015625,15651.2001953125,15987.16015625,15676.4501953125,15716.6396484375,15934.849609375,15775.73046875,15984.8896484375,16006.2099609375,15955.16015625,15711.759765625,15519.849609375,15544.3603515625,15176.5595703125,15546.6904296875,15828.6396484375,15697.75,15805.759765625,16197.4501953125,16366.23046875,16376.91015625,16445.869140625,16320.8798828125,16329.5400390625,16376.7998046875,16190.0400390625,16127.8701171875,15992.3203125,16091.75,15759.580078125,15943.3701171875,15715.0703125,15921.4501953125,15947.2099609375,16241.5498046875,16256.580078125,16253.9296875,16312.98046875,16264.6103515625,16186.4599609375,16065.5,16250.2001953125,15994.9599609375,16010.5400390625,16140.3701171875,16411.669921875,16490.310546875,16529.23046875,16538.279296875,16771.349609375,16786.560546875,16974.279296875,16908.55078125,16872.400390625,16869.779296875,16851.060546875,17028.349609375,17141.25,17231.580078125,17282.759765625,17302.73046875,17117.490234375,17378.359375,17586.01953125,17610.720703125,17646.9296875,17505.51953125,17249.560546875,16968.75,16925.51953125,17032.23046875,17274.23046875,17137.189453125,16515.880859375,15668.330078125,15819.240234375,15554.990234375,15564.9599609375,16112.2099609375,16122.51953125,16136.8603515625,16246.51953125,16444.75,16645.169921875,16591.69921875,16690.0390625,16948.470703125,17098.490234375,17192.91015625,17201.30078125,17225.060546875,17176.08984375,17094.33984375,17082.51953125,17024.5390625,17200.44921875,17279.69921875,17357.369140625,17254.83984375,17384.740234375,17305.490234375,17162.7109375,17127.58984375,17358.5703125,17471.0703125,17531.8203125,17621.150390625,17648.25,17801.189453125,17745.689453125,17783.80078125,17929.220703125,17901.25,17880.900390625,17778.119140625,17843.30078125,17931.470703125,17892.509765625,17878.779296875,17839.580078125,17854.349609375,17724.919921875,17593.599609375,17482.810546875,17603.94921875,17554.9296875,17394.76953125,17252.869140625,17216.580078125,17414.76953125,17256.609375,17489.119140625,17588.640625,17624.8203125,17593.849609375,17505.009765625,17492.44921875,17308.619140625,17221.349609375,17216.33984375,16941.330078125,16841.619140625,16630.990234375,16777.779296875,16426.98046875,16459.130859375,16830.630859375,16821.080078125,17177.390625,17061.099609375,17244.619140625,17384.609375,17463.80078125,17455.919921875,17380.51953125,17534.05078125,17534.380859375,17411.529296875,17175.0390625,17270.279296875,17452.16015625,17463.779296875,17434.01953125,17332.33984375,17279.2890625,17196.7890625,16998.0703125,17130.740234375,17278.109375,17286.890625,17127.859375,16886.5703125,16883.0,16600.51953125,16362.26953125,16488.2109375,16465.5703125,16742.349609375,16589.720703125,16480.98046875,16392.509765625,16426.759765625,16816.939453125,16791.119140625,16936.19921875,16904.4609375,16900.3203125,16879.33984375,16909.3203125,17037.369140625,17081.169921875,17058.0390625,17021.76953125,17094.419921875,17087.890625,17158.279296875,17097.16015625,17323.390625,17433.099609375,17549.08984375,17527.1796875,17479.55078125,17560.51953125,17639.9609375,17705.2890625,17767.060546875,17864.189453125,17828.529296875,17797.900390625,17680.16015625,17686.119140625,17641.7890625,17320.009765625,17369.7890625,17428.609375,17575.91015625,17720.9296875,17650.669921875,17734.189453125,17845.060546875,17880.41015625,17893.2890625,17841.029296875,17754.380859375,17592.869140625,17719.060546875,17744.5390625,17812.810546875,17686.83984375,17805.740234375,17855.779296875,17966.349609375,17975.41015625,18099.7109375,18209.140625,18270.0,18260.23046875,18395.140625,18598.130859375,18395.720703125,18388.16015625,18095.390625,18266.5390625,18348.390625,18356.2109375,18509.0,18512.150390625,18492.810546875,18275.1796875,18212.259765625,18113.939453125,17843.330078125,17890.7109375,17657.970703125,17750.69921875,17955.94921875,18060.330078125,18217.2890625,18258.810546875,18182.73046875,17978.439453125,18109.169921875,18213.30078125,18250.369140625,18196.400390625,18159.509765625,17954.75,17939.529296875,17617.359375,17657.5,17864.109375,17932.150390625,17845.009765625,17581.5703125,17000.0,16944.080078125,17224.73046875,17373.8203125,17287.900390625,17177.26953125,17007.580078125,17172.810546875,17439.91015625,17503.94921875,17512.41015625,17581.619140625,17711.169921875,17695.80078125,17510.150390625,17568.3203125,17572.900390625,17754.099609375,17657.759765625,17484.189453125,17430.130859375,17210.400390625,17273.05078125,17005.83984375,17080.400390625,17353.25,17139.33984375,16958.66015625,16926.33984375,17057.130859375,17159.880859375,17088.759765625,16912.30078125,16677.83984375,16427.3203125,16350.7197265625,16599.83984375,16593.2109375,16531.369140625,16689.98046875,16491.109375,16345.83984375,15891.400390625,16053.75,15943.6201171875,15687.150390625,15943.6904296875,15915.9296875,16174.7802734375,16111.490234375,16061.919921875,16193.919921875,16166.5703125,15987.740234375,16128.2099609375,16075.91015625,16368.9296875,16561.6796875,16718.91015625,16605.4609375,16570.890625,16593.75,16538.669921875,16643.94921875,16581.810546875,16295.0703125,15975.169921875,16025.83984375],\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729],\"type\":\"candlestick\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"\\u53f0\\u7063\\u52a0\\u6b0a\\u6307\\u6578\\u8d70\\u52e2 (\\u4e92\\u52d5\\u5f0f\\u5716\\u8868)\"},\"xaxis\":{\"rangeslider\":{\"visible\":true},\"rangeselector\":{\"buttons\":[{\"count\":1,\"label\":\"1m\",\"step\":\"month\",\"stepmode\":\"backward\"},{\"count\":6,\"label\":\"6m\",\"step\":\"month\",\"stepmode\":\"backward\"},{\"count\":1,\"label\":\"YTD\",\"step\":\"year\",\"stepmode\":\"todate\"},{\"count\":1,\"label\":\"1y\",\"step\":\"year\",\"stepmode\":\"backward\"},{\"step\":\"all\"}]}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('9f4ab9e9-0ce6-4d49-a558-ab173beb8646');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = go.Figure(data = [go.Candlestick(x = data.index,\n",
    "                                          open = data[\"Open\"],\n",
    "                                          high = data[\"High\"],\n",
    "                                          low = data[\"Low\"],\n",
    "                                          close = data[\"Close\"])])\n",
    "figure.update_layout(title = \"台灣加權指數走勢 (互動式圖表)\")\n",
    "\n",
    "figure.update_xaxes(\n",
    "    rangeslider_visible = True,\n",
    "    rangeselector = dict(\n",
    "        buttons = list([\n",
    "            dict(count = 1, label = \"1m\", step = \"month\", stepmode = \"backward\"),\n",
    "            dict(count = 6, label = \"6m\", step = \"month\", stepmode = \"backward\"),\n",
    "            dict(count = 1, label = \"YTD\", step = \"year\", stepmode = \"todate\"),\n",
    "            dict(count = 1, label = \"1y\", step = \"year\", stepmode = \"backward\"),\n",
    "            dict(step = \"all\")\n",
    "        ])\n",
    "    )\n",
    ")\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede9f58",
   "metadata": {},
   "source": [
    "## AutoTS 時間序列下的價格預測\n",
    "\n",
    "接下來，我們透過AutoTS函式庫，來預測接下來30天的台灣加權股價指數（收盤價格）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d11571d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred frequency is: B\n",
      "Model Number: 1 with model AverageValueNaive in generation 0 of 10\n",
      "Model Number: 2 with model AverageValueNaive in generation 0 of 10\n",
      "Model Number: 3 with model AverageValueNaive in generation 0 of 10\n",
      "Model Number: 4 with model DatepartRegression in generation 0 of 10\n",
      "Model Number: 5 with model DatepartRegression in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning:\n",
      "\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 6 with model DatepartRegression in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 7 with model DatepartRegression in generation 0 of 10\n",
      "Epoch 1/50\n",
      "24/24 [==============================] - 9s 8ms/step - loss: 0.4147\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4123\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4116\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4103\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4097\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4109\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4109\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4082\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4064\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4070\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4073\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4084\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4095\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4060\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4064\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4070\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4044\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4086\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4056\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4052\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4043\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4070\n",
      "Epoch 23/50\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4051\n",
      "Epoch 24/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4038\n",
      "Epoch 25/50\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4033\n",
      "Epoch 26/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4054\n",
      "Epoch 27/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4050\n",
      "Epoch 28/50\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4059\n",
      "Epoch 29/50\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4025\n",
      "Epoch 30/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4032\n",
      "Epoch 31/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4049\n",
      "Epoch 32/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4048\n",
      "Epoch 33/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4054\n",
      "Epoch 34/50\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4037\n",
      "Epoch 35/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4058\n",
      "Epoch 36/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4049\n",
      "Epoch 37/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4032\n",
      "Epoch 38/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4011\n",
      "Epoch 39/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4022\n",
      "Epoch 40/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4034\n",
      "Epoch 41/50\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.4010\n",
      "Epoch 42/50\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.4032\n",
      "Epoch 43/50\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4015\n",
      "Epoch 44/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4010\n",
      "Epoch 45/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4028\n",
      "Epoch 46/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4020\n",
      "Epoch 47/50\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3999\n",
      "Epoch 48/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4000\n",
      "Epoch 49/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4012\n",
      "Epoch 50/50\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.3992\n",
      "Template Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 7: DatepartRegression\n",
      "Model Number: 8 with model ETS in generation 0 of 10\n",
      "Model Number: 9 with model ETS in generation 0 of 10\n",
      "Model Number: 10 with model GLM in generation 0 of 10\n",
      "Template Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 10: GLM\n",
      "Model Number: 11 with model GLM in generation 0 of 10\n",
      "Model Number: 12 with model GLS in generation 0 of 10\n",
      "Model Number: 13 with model GLS in generation 0 of 10\n",
      "Model Number: 14 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 15 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 16 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 17 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 18 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 19 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 20 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 21 with model UnobservedComponents in generation 0 of 10\n",
      "Model Number: 22 with model UnobservedComponents in generation 0 of 10\n",
      "Model Number: 23 with model UnobservedComponents in generation 0 of 10\n",
      "Model Number: 24 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 24: VAR\n",
      "Model Number: 25 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 25: VAR\n",
      "Model Number: 26 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 26: VECM\n",
      "Model Number: 27 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 27: VECM\n",
      "Model Number: 28 with model WindowRegression in generation 0 of 10\n",
      "Model Number: 29 with model ConstantNaive in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 30 with model FBProphet in generation 0 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 30: FBProphet\n",
      "Model Number: 31 with model MultivariateRegression in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-2)]: Done 200 out of 200 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 32 with model MultivariateRegression in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 32: MultivariateRegression\n",
      "Model Number: 33 with model DatepartRegression in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 33: DatepartRegression\n",
      "Model Number: 34 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 35 with model DatepartRegression in generation 0 of 10\n",
      "Model Number: 36 with model UnobservedComponents in generation 0 of 10\n",
      "Model Number: 37 with model UnobservedComponents in generation 0 of 10\n",
      "Model Number: 38 with model ETS in generation 0 of 10\n",
      "Model Number: 39 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 39: VECM\n",
      "Model Number: 40 with model ARDL in generation 0 of 10\n",
      "Model Number: 41 with model MultivariateMotif in generation 0 of 10\n",
      "Model Number: 42 with model MultivariateMotif in generation 0 of 10\n",
      "Model Number: 43 with model UnivariateMotif in generation 0 of 10\n",
      "Model Number: 44 with model UnivariateMotif in generation 0 of 10\n",
      "Model Number: 45 with model SectionalMotif in generation 0 of 10\n",
      "Model Number: 46 with model SectionalMotif in generation 0 of 10\n",
      "Model Number: 47 with model MultivariateRegression in generation 0 of 10\n",
      "Model Number: 48 with model FBProphet in generation 0 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 48: FBProphet\n",
      "Model Number: 49 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 50 with model DatepartRegression in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 51 with model NVAR in generation 0 of 10\n",
      "Model Number: 52 with model Theta in generation 0 of 10\n",
      "Model Number: 53 with model ConstantNaive in generation 0 of 10\n",
      "Model Number: 54 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 55 with model AverageValueNaive in generation 0 of 10\n",
      "Model Number: 56 with model GLS in generation 0 of 10\n",
      "Model Number: 57 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 58 with model GLM in generation 0 of 10\n",
      "Template Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 58: GLM\n",
      "Model Number: 59 with model ETS in generation 0 of 10\n",
      "Model Number: 60 with model FBProphet in generation 0 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 60: FBProphet\n",
      "Model Number: 61 with model UnobservedComponents in generation 0 of 10\n",
      "Model Number: 62 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 62: VAR\n",
      "Model Number: 63 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 63: VECM\n",
      "Model Number: 64 with model WindowRegression in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 65 with model DatepartRegression in generation 0 of 10\n",
      "Model Number: 66 with model MultivariateRegression in generation 0 of 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000170 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Model Number: 67 with model UnivariateMotif in generation 0 of 10\n",
      "Model Number: 68 with model MultivariateMotif in generation 0 of 10\n",
      "Model Number: 69 with model SectionalMotif in generation 0 of 10\n",
      "Model Number: 70 with model NVAR in generation 0 of 10\n",
      "Model Number: 71 with model Theta in generation 0 of 10\n",
      "Model Number: 72 with model ARDL in generation 0 of 10\n",
      "Model Number: 73 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 73: VECM\n",
      "Model Number: 74 with model MultivariateRegression in generation 0 of 10\n",
      "Model Number: 75 with model WindowRegression in generation 0 of 10\n",
      "Model Number: 76 with model SectionalMotif in generation 0 of 10\n",
      "Model Number: 77 with model Theta in generation 0 of 10\n",
      "Model Number: 78 with model GLS in generation 0 of 10\n",
      "Model Number: 79 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 79: VECM\n",
      "Model Number: 80 with model WindowRegression in generation 0 of 10\n",
      "Model Number: 81 with model SectionalMotif in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"regression_type=='User' but no future_regressor supplied\") in model 81: SectionalMotif\n",
      "Model Number: 82 with model MultivariateMotif in generation 0 of 10\n",
      "Model Number: 83 with model ConstantNaive in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"Model returned NaN due to a preprocessing transformer {'fillna': 'rolling_mean', 'transformations': {'0': 'StandardScaler', '1': 'StandardScaler', '2': 'bkfilter', '3': 'CumSumTransformer', '4': 'cffilter'}, 'transformation_params': {'0': {}, '1': {}, '2': {}, '3': {}, '4': {}}}. fail_on_forecast_nan=True\") in model 83: ConstantNaive\n",
      "Model Number: 84 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 84: VECM\n",
      "Model Number: 85 with model SectionalMotif in generation 0 of 10\n",
      "Model Number: 86 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 87 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 88 with model UnivariateMotif in generation 0 of 10\n",
      "Model Number: 89 with model ETS in generation 0 of 10\n",
      "Model Number: 90 with model ConstantNaive in generation 0 of 10\n",
      "Model Number: 91 with model MultivariateRegression in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 91: MultivariateRegression\n",
      "Model Number: 92 with model ARDL in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 92: ARDL\n",
      "Model Number: 93 with model NVAR in generation 0 of 10\n",
      "Model Number: 94 with model SectionalMotif in generation 0 of 10\n",
      "Model Number: 95 with model UnivariateMotif in generation 0 of 10\n",
      "Template Eval Error: ValueError('Model UnivariateMotif returned NaN for one or more series. fail_on_forecast_nan=True') in model 95: UnivariateMotif\n",
      "Model Number: 96 with model UnobservedComponents in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\numpy\\core\\_methods.py:48: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in reduce\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\numpy\\lib\\function_base.py:412: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 97 with model DatepartRegression in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 97: DatepartRegression\n",
      "Model Number: 98 with model Theta in generation 0 of 10\n",
      "Model Number: 99 with model ConstantNaive in generation 0 of 10\n",
      "Model Number: 100 with model NVAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Model NVAR returned NaN for one or more series. fail_on_forecast_nan=True') in model 100: NVAR\n",
      "Model Number: 101 with model ConstantNaive in generation 0 of 10\n",
      "Model Number: 102 with model FBProphet in generation 0 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 102: FBProphet\n",
      "Model Number: 103 with model ConstantNaive in generation 0 of 10\n",
      "Model Number: 104 with model DatepartRegression in generation 0 of 10\n",
      "Epoch 1/50\n",
      "11/11 [==============================] - 9s 13ms/step - loss: 101.4933\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 101.4572\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 100.5987\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 100.9200\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 102.1344\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 100.7201\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 101.8251\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 101.3982\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 100.6088\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 100.6589\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 100.9847\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 100.8364\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 101.1523\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 101.1174\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 100.8739\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 100.5690\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 100.2768\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 100.2152\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 99.9066\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 101.0432\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 100.7747\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 100.0857\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 100.1763\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 99.9129\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 100.6050\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 99.7978\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 100.1098\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 99.8247\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 100.2941\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 100.6660\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 100.0311\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 100.3587\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 99.8673\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 100.0126\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 100.2276\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 99.8899\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 100.3090\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 99.8032\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 100.1839\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 99.9995\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 100.4775\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 99.9295\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 100.1395\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 100.1845\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 99.9617\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 100.4952\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 100.1195\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 100.3531\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 99.7688\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 100.1043\n",
      "Template Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 104: DatepartRegression\n",
      "Model Number: 105 with model FBProphet in generation 0 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 105: FBProphet\n",
      "Model Number: 106 with model FBProphet in generation 0 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 106: FBProphet\n",
      "Model Number: 107 with model MultivariateRegression in generation 0 of 10\n",
      "Model Number: 108 with model Theta in generation 0 of 10\n",
      "Model Number: 109 with model MultivariateRegression in generation 0 of 10\n",
      "Model Number: 110 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 111 with model GLS in generation 0 of 10\n",
      "Model Number: 112 with model ConstantNaive in generation 0 of 10\n",
      "Model Number: 113 with model DatepartRegression in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 113: DatepartRegression\n",
      "Model Number: 114 with model GLS in generation 0 of 10\n",
      "Model Number: 115 with model UnobservedComponents in generation 0 of 10\n",
      "Template Eval Error: LinAlgError('Singular matrix') in model 115: UnobservedComponents\n",
      "Model Number: 116 with model DatepartRegression in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning:\n",
      "\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 117 with model DatepartRegression in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 117: DatepartRegression\n",
      "Model Number: 118 with model UnobservedComponents in generation 0 of 10\n",
      "Model Number: 119 with model MultivariateRegression in generation 0 of 10\n",
      "Epoch 1/50\n",
      "47/47 [==============================] - 9s 14ms/step - loss: 0.0636\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.0115\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.0016\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.0011\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 9.5277e-04\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 7.7466e-04\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 7.2776e-04\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 6.6100e-04\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 5.7689e-04\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 5.2501e-04\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 5.3545e-04\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 4.4294e-04\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 4.7798e-04\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 4.0527e-04\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.7283e-04\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.7459e-04\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.7376e-04\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.7424e-04\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.9106e-04\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.7385e-04\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.5949e-04\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.4457e-04\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 4.1480e-04\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.7439e-04\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.2071e-04\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.9268e-04\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.2402e-04\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.0804e-04\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.7656e-04\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 4.0517e-04\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 3.3970e-04\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 3.5555e-04\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 3.2561e-04\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 3.2613e-04\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 3.7432e-04\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 3.0226e-04\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 3.2493e-04\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 3.6247e-04\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 3.0538e-04\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.8124e-04\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.3715e-04\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 3.7144e-04\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.0612e-04\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 4.1523e-04\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 2.9862e-04\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 2.8963e-04\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.0635e-04\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 3.5204e-04\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.3942e-04\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 2.9167e-04\n",
      "Template Eval Error: ValueError('Model MultivariateRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 119: MultivariateRegression\n",
      "Model Number: 120 with model UnivariateMotif in generation 0 of 10\n",
      "Model Number: 121 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 122 with model GLS in generation 0 of 10\n",
      "Model Number: 123 with model SectionalMotif in generation 0 of 10\n",
      "Model Number: 124 with model MultivariateMotif in generation 0 of 10\n",
      "Template Eval Error: PicklingError('Could not pickle the task to send it to the workers.') in model 124: MultivariateMotif\n",
      "Model Number: 125 with model UnivariateMotif in generation 0 of 10\n",
      "Model Number: 126 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 126: VAR\n",
      "Model Number: 127 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 128 with model UnivariateMotif in generation 0 of 10\n",
      "Model Number: 129 with model GLM in generation 0 of 10\n",
      "Template Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 129: GLM\n",
      "Model Number: 130 with model MultivariateMotif in generation 0 of 10\n",
      "Model Number: 131 with model GLM in generation 0 of 10\n",
      "Template Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 131: GLM\n",
      "Model Number: 132 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 133 with model UnobservedComponents in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 133: UnobservedComponents\n",
      "Model Number: 134 with model Theta in generation 0 of 10\n",
      "Model Number: 135 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 135: VAR\n",
      "Model Number: 136 with model NVAR in generation 0 of 10\n",
      "Model Number: 137 with model UnivariateMotif in generation 0 of 10\n",
      "Model Number: 138 with model ConstantNaive in generation 0 of 10\n",
      "Model Number: 139 with model SectionalMotif in generation 0 of 10\n",
      "Model Number: 140 with model ConstantNaive in generation 0 of 10\n",
      "Model Number: 141 with model AverageValueNaive in generation 0 of 10\n",
      "Model Number: 142 with model AverageValueNaive in generation 0 of 10\n",
      "Model Number: 143 with model GLS in generation 0 of 10\n",
      "Model Number: 144 with model ARDL in generation 0 of 10\n",
      "Model Number: 145 with model UnivariateMotif in generation 0 of 10\n",
      "Model Number: 146 with model WindowRegression in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-2)]: Done 444 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-2)]: Done 794 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-2)]: Done 1000 out of 1000 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 147 with model ARDL in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 147: ARDL\n",
      "Model Number: 148 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 149 with model ConstantNaive in generation 0 of 10\n",
      "Model Number: 150 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 150: VECM\n",
      "Model Number: 151 with model NVAR in generation 0 of 10\n",
      "Model Number: 152 with model NVAR in generation 0 of 10\n",
      "New Generation: 1 of 10\n",
      "Model Number: 153 with model LastValueNaive in generation 1 of 10\n",
      "Model Number: 154 with model LastValueNaive in generation 1 of 10\n",
      "Model Number: 155 with model LastValueNaive in generation 1 of 10\n",
      "Model Number: 156 with model Theta in generation 1 of 10\n",
      "Model Number: 157 with model Theta in generation 1 of 10\n",
      "Model Number: 158 with model Theta in generation 1 of 10\n",
      "Model Number: 159 with model Theta in generation 1 of 10\n",
      "Model Number: 160 with model NVAR in generation 1 of 10\n",
      "Model Number: 161 with model NVAR in generation 1 of 10\n",
      "Model Number: 162 with model NVAR in generation 1 of 10\n",
      "Model Number: 163 with model NVAR in generation 1 of 10\n",
      "Model Number: 164 with model ConstantNaive in generation 1 of 10\n",
      "Model Number: 165 with model ConstantNaive in generation 1 of 10\n",
      "Model Number: 166 with model ConstantNaive in generation 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1187: RuntimeWarning:\n",
      "\n",
      "All-NaN slice encountered\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: Exception('Transformer MaxAbsScaler failed on fit') in model 166: ConstantNaive\n",
      "Model Number: 167 with model UnobservedComponents in generation 1 of 10\n",
      "Model Number: 168 with model UnobservedComponents in generation 1 of 10\n",
      "Model Number: 169 with model UnobservedComponents in generation 1 of 10\n",
      "Model Number: 170 with model GLS in generation 1 of 10\n",
      "Model Number: 171 with model GLS in generation 1 of 10\n",
      "Model Number: 172 with model GLS in generation 1 of 10\n",
      "Model Number: 173 with model SeasonalNaive in generation 1 of 10\n",
      "Model Number: 174 with model SeasonalNaive in generation 1 of 10\n",
      "Model Number: 175 with model SeasonalNaive in generation 1 of 10\n",
      "Model Number: 176 with model SeasonalNaive in generation 1 of 10\n",
      "Model Number: 177 with model WindowRegression in generation 1 of 10\n",
      "Model Number: 178 with model WindowRegression in generation 1 of 10\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 20s 422ms/step - loss: 103.9447 - val_loss: 121.9874\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 8s 458ms/step - loss: 94.0657 - val_loss: 125.3344\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 8s 418ms/step - loss: 91.0484 - val_loss: 125.4194\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 7s 420ms/step - loss: 86.9609 - val_loss: 128.9946\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 7s 414ms/step - loss: 86.0640 - val_loss: 132.0940\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 8s 461ms/step - loss: 84.7157 - val_loss: 132.6775\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 8s 438ms/step - loss: 83.3668 - val_loss: 136.1450\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 8s 433ms/step - loss: 82.9913 - val_loss: 132.7413\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 8s 435ms/step - loss: 82.3165 - val_loss: 139.5983\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 7s 405ms/step - loss: 82.2243 - val_loss: 137.6282\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 8s 437ms/step - loss: 81.0812 - val_loss: 140.9158\n",
      "Model Number: 179 with model WindowRegression in generation 1 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 179: WindowRegression\n",
      "Model Number: 180 with model AverageValueNaive in generation 1 of 10\n",
      "Model Number: 181 with model AverageValueNaive in generation 1 of 10\n",
      "Model Number: 182 with model AverageValueNaive in generation 1 of 10\n",
      "Model Number: 183 with model ARDL in generation 1 of 10\n",
      "Model Number: 184 with model ARDL in generation 1 of 10\n",
      "Model Number: 185 with model ARDL in generation 1 of 10\n",
      "Model Number: 186 with model ARDL in generation 1 of 10\n",
      "Model Number: 187 with model SectionalMotif in generation 1 of 10\n",
      "Model Number: 188 with model SectionalMotif in generation 1 of 10\n",
      "Model Number: 189 with model SectionalMotif in generation 1 of 10\n",
      "Model Number: 190 with model SectionalMotif in generation 1 of 10\n",
      "Model Number: 191 with model ETS in generation 1 of 10\n",
      "Model Number: 192 with model ETS in generation 1 of 10\n",
      "Model Number: 193 with model ETS in generation 1 of 10\n",
      "ETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
      "ETS failed on Close with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
      "Model Number: 194 with model ETS in generation 1 of 10\n",
      "Model Number: 195 with model UnivariateMotif in generation 1 of 10\n",
      "Model Number: 196 with model UnivariateMotif in generation 1 of 10\n",
      "Model Number: 197 with model UnivariateMotif in generation 1 of 10\n",
      "Model Number: 198 with model UnivariateMotif in generation 1 of 10\n",
      "Model Number: 199 with model MultivariateMotif in generation 1 of 10\n",
      "Model Number: 200 with model MultivariateMotif in generation 1 of 10\n",
      "Model Number: 201 with model MultivariateMotif in generation 1 of 10\n",
      "Model Number: 202 with model MultivariateRegression in generation 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.142e+01, tolerance: 5.856e-03\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 203 with model MultivariateRegression in generation 1 of 10\n",
      "Template Eval Error: ValueError('Some value(s) of y are out of the valid range for family PoissonDistribution') in model 203: MultivariateRegression\n",
      "Model Number: 204 with model MultivariateRegression in generation 1 of 10\n",
      "Template Eval Error: ValueError('Some value(s) of y are out of the valid range for family PoissonDistribution') in model 204: MultivariateRegression\n",
      "Model Number: 205 with model MultivariateRegression in generation 1 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 205: MultivariateRegression\n",
      "Model Number: 206 with model DatepartRegression in generation 1 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 206: DatepartRegression\n",
      "Model Number: 207 with model DatepartRegression in generation 1 of 10\n",
      "Template Eval Error: ValueError('Failed to convert a NumPy array to a Tensor (Unsupported object type int).') in model 207: DatepartRegression\n",
      "Model Number: 208 with model DatepartRegression in generation 1 of 10\n",
      "Model Number: 209 with model GLM in generation 1 of 10\n",
      "Model Number: 210 with model GLM in generation 1 of 10\n",
      "Template Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 210: GLM\n",
      "Model Number: 211 with model GLM in generation 1 of 10\n",
      "Template Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 211: GLM\n",
      "Model Number: 212 with model GLM in generation 1 of 10\n",
      "Model Number: 213 with model VAR in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 213: VAR\n",
      "Model Number: 214 with model VAR in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 214: VAR\n",
      "Model Number: 215 with model VAR in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 215: VAR\n",
      "Model Number: 216 with model VAR in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 216: VAR\n",
      "Model Number: 217 with model VECM in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 217: VECM\n",
      "Model Number: 218 with model VECM in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 218: VECM\n",
      "Model Number: 219 with model VECM in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 219: VECM\n",
      "Model Number: 220 with model VECM in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 220: VECM\n",
      "Model Number: 221 with model FBProphet in generation 1 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 221: FBProphet\n",
      "Model Number: 222 with model FBProphet in generation 1 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 222: FBProphet\n",
      "Model Number: 223 with model FBProphet in generation 1 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 223: FBProphet\n",
      "Model Number: 224 with model FBProphet in generation 1 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 224: FBProphet\n",
      "New Generation: 2 of 10\n",
      "Model Number: 225 with model LastValueNaive in generation 2 of 10\n",
      "Model Number: 226 with model LastValueNaive in generation 2 of 10\n",
      "Model Number: 227 with model LastValueNaive in generation 2 of 10\n",
      "Model Number: 228 with model NVAR in generation 2 of 10\n",
      "Model Number: 229 with model NVAR in generation 2 of 10\n",
      "Model Number: 230 with model NVAR in generation 2 of 10\n",
      "Model Number: 231 with model UnivariateMotif in generation 2 of 10\n",
      "Model Number: 232 with model UnivariateMotif in generation 2 of 10\n",
      "Model Number: 233 with model UnivariateMotif in generation 2 of 10\n",
      "Model Number: 234 with model UnivariateMotif in generation 2 of 10\n",
      "Model Number: 235 with model Theta in generation 2 of 10\n",
      "Model Number: 236 with model Theta in generation 2 of 10\n",
      "Model Number: 237 with model Theta in generation 2 of 10\n",
      "Model Number: 238 with model Theta in generation 2 of 10\n",
      "Model Number: 239 with model ConstantNaive in generation 2 of 10\n",
      "Model Number: 240 with model ConstantNaive in generation 2 of 10\n",
      "Model Number: 241 with model ConstantNaive in generation 2 of 10\n",
      "Model Number: 242 with model UnobservedComponents in generation 2 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 242: UnobservedComponents\n",
      "Model Number: 243 with model UnobservedComponents in generation 2 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 243: UnobservedComponents\n",
      "Model Number: 244 with model UnobservedComponents in generation 2 of 10\n",
      "Template Eval Error: LinAlgError('Schur decomposition solver error.') in model 244: UnobservedComponents\n",
      "Model Number: 245 with model GLS in generation 2 of 10\n",
      "Model Number: 246 with model GLS in generation 2 of 10\n",
      "Model Number: 247 with model GLS in generation 2 of 10\n",
      "Model Number: 248 with model SeasonalNaive in generation 2 of 10\n",
      "Model Number: 249 with model SeasonalNaive in generation 2 of 10\n",
      "Model Number: 250 with model SeasonalNaive in generation 2 of 10\n",
      "Model Number: 251 with model SeasonalNaive in generation 2 of 10\n",
      "Model Number: 252 with model WindowRegression in generation 2 of 10\n",
      "Model Number: 253 with model WindowRegression in generation 2 of 10\n",
      "Model Number: 254 with model WindowRegression in generation 2 of 10\n",
      "Model Number: 255 with model AverageValueNaive in generation 2 of 10\n",
      "Model Number: 256 with model AverageValueNaive in generation 2 of 10\n",
      "Model Number: 257 with model AverageValueNaive in generation 2 of 10\n",
      "Model Number: 258 with model ARDL in generation 2 of 10\n",
      "Model Number: 259 with model ARDL in generation 2 of 10\n",
      "Model Number: 260 with model ARDL in generation 2 of 10\n",
      "Model Number: 261 with model ARDL in generation 2 of 10\n",
      "Model Number: 262 with model SectionalMotif in generation 2 of 10\n",
      "Model Number: 263 with model SectionalMotif in generation 2 of 10\n",
      "Model Number: 264 with model SectionalMotif in generation 2 of 10\n",
      "Model Number: 265 with model SectionalMotif in generation 2 of 10\n",
      "Template Eval Error: ValueError(\"regression_type=='User' but no future_regressor supplied\") in model 265: SectionalMotif\n",
      "Model Number: 266 with model ETS in generation 2 of 10\n",
      "Model Number: 267 with model ETS in generation 2 of 10\n",
      "Model Number: 268 with model ETS in generation 2 of 10\n",
      "Model Number: 269 with model ETS in generation 2 of 10\n",
      "Model Number: 270 with model MultivariateMotif in generation 2 of 10\n",
      "Model Number: 271 with model MultivariateMotif in generation 2 of 10\n",
      "Model Number: 272 with model MultivariateMotif in generation 2 of 10\n",
      "Model Number: 273 with model MultivariateMotif in generation 2 of 10\n",
      "Model Number: 274 with model MultivariateRegression in generation 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.359e-02, tolerance: 1.304e-05\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 275 with model MultivariateRegression in generation 2 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 275: MultivariateRegression\n",
      "Model Number: 276 with model MultivariateRegression in generation 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-2)]: Done 200 out of 200 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 277 with model MultivariateRegression in generation 2 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 277: MultivariateRegression\n",
      "Model Number: 278 with model DatepartRegression in generation 2 of 10\n",
      "Model Number: 279 with model DatepartRegression in generation 2 of 10\n",
      "Model Number: 280 with model DatepartRegression in generation 2 of 10\n",
      "Template Eval Error: ValueError('Failed to convert a NumPy array to a Tensor (Unsupported object type int).') in model 280: DatepartRegression\n",
      "Model Number: 281 with model GLM in generation 2 of 10\n",
      "Model Number: 282 with model GLM in generation 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 283 with model GLM in generation 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 284 with model GLM in generation 2 of 10\n",
      "Model Number: 285 with model VAR in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 285: VAR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:187: RuntimeWarning:\n",
      "\n",
      "overflow encountered in exp\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 286 with model VAR in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 286: VAR\n",
      "Model Number: 287 with model VAR in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 287: VAR\n",
      "Model Number: 288 with model VAR in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 288: VAR\n",
      "Model Number: 289 with model VECM in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 289: VECM\n",
      "Model Number: 290 with model VECM in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 290: VECM\n",
      "Model Number: 291 with model VECM in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 291: VECM\n",
      "Model Number: 292 with model VECM in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 292: VECM\n",
      "Model Number: 293 with model FBProphet in generation 2 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 293: FBProphet\n",
      "Model Number: 294 with model FBProphet in generation 2 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 294: FBProphet\n",
      "Model Number: 295 with model FBProphet in generation 2 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 295: FBProphet\n",
      "Model Number: 296 with model FBProphet in generation 2 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 296: FBProphet\n",
      "New Generation: 3 of 10\n",
      "Model Number: 297 with model LastValueNaive in generation 3 of 10\n",
      "Model Number: 298 with model LastValueNaive in generation 3 of 10\n",
      "Model Number: 299 with model LastValueNaive in generation 3 of 10\n",
      "Model Number: 300 with model NVAR in generation 3 of 10\n",
      "Model Number: 301 with model NVAR in generation 3 of 10\n",
      "Model Number: 302 with model NVAR in generation 3 of 10\n",
      "Model Number: 303 with model NVAR in generation 3 of 10\n",
      "Model Number: 304 with model ARDL in generation 3 of 10\n",
      "Model Number: 305 with model ARDL in generation 3 of 10\n",
      "Model Number: 306 with model ARDL in generation 3 of 10\n",
      "Model Number: 307 with model ARDL in generation 3 of 10\n",
      "Model Number: 308 with model UnivariateMotif in generation 3 of 10\n",
      "Model Number: 309 with model UnivariateMotif in generation 3 of 10\n",
      "Model Number: 310 with model UnivariateMotif in generation 3 of 10\n",
      "Model Number: 311 with model UnivariateMotif in generation 3 of 10\n",
      "Model Number: 312 with model Theta in generation 3 of 10\n",
      "Model Number: 313 with model Theta in generation 3 of 10\n",
      "Model Number: 314 with model Theta in generation 3 of 10\n",
      "Model Number: 315 with model Theta in generation 3 of 10\n",
      "Model Number: 316 with model AverageValueNaive in generation 3 of 10\n",
      "Model Number: 317 with model AverageValueNaive in generation 3 of 10\n",
      "Model Number: 318 with model AverageValueNaive in generation 3 of 10\n",
      "Model Number: 319 with model ConstantNaive in generation 3 of 10\n",
      "Model Number: 320 with model ConstantNaive in generation 3 of 10\n",
      "Model Number: 321 with model ConstantNaive in generation 3 of 10\n",
      "Model Number: 322 with model UnobservedComponents in generation 3 of 10\n",
      "Model Number: 323 with model UnobservedComponents in generation 3 of 10\n",
      "Model Number: 324 with model UnobservedComponents in generation 3 of 10\n",
      "Model Number: 325 with model GLM in generation 3 of 10\n",
      "Template Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 325: GLM\n",
      "Model Number: 326 with model GLM in generation 3 of 10\n",
      "Model Number: 327 with model GLM in generation 3 of 10\n",
      "Model Number: 328 with model GLM in generation 3 of 10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:187: RuntimeWarning:\n",
      "\n",
      "overflow encountered in exp\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Template Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 328: GLM\n",
      "Model Number: 329 with model GLS in generation 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:67: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:68: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:68: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 330 with model GLS in generation 3 of 10\n",
      "Model Number: 331 with model GLS in generation 3 of 10\n",
      "Model Number: 332 with model SeasonalNaive in generation 3 of 10\n",
      "Model Number: 333 with model SeasonalNaive in generation 3 of 10\n",
      "Model Number: 334 with model SeasonalNaive in generation 3 of 10\n",
      "Model Number: 335 with model SeasonalNaive in generation 3 of 10\n",
      "Model Number: 336 with model WindowRegression in generation 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.833e+06, tolerance: 6.087e+05\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 337 with model WindowRegression in generation 3 of 10\n",
      "Model Number: 338 with model WindowRegression in generation 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 339 with model SectionalMotif in generation 3 of 10\n",
      "Model Number: 340 with model SectionalMotif in generation 3 of 10\n",
      "Model Number: 341 with model SectionalMotif in generation 3 of 10\n",
      "Model Number: 342 with model SectionalMotif in generation 3 of 10\n",
      "Model Number: 343 with model ETS in generation 3 of 10\n",
      "Model Number: 344 with model ETS in generation 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:997: RuntimeWarning:\n",
      "\n",
      "All-NaN slice encountered\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1376: RuntimeWarning:\n",
      "\n",
      "All-NaN slice encountered\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: Exception('Transformer RobustScaler failed on fit') in model 344: ETS\n",
      "Model Number: 345 with model ETS in generation 3 of 10\n",
      "Model Number: 346 with model ETS in generation 3 of 10\n",
      "Model Number: 347 with model MultivariateMotif in generation 3 of 10\n",
      "Model Number: 348 with model MultivariateMotif in generation 3 of 10\n",
      "Model Number: 349 with model MultivariateMotif in generation 3 of 10\n",
      "Model Number: 350 with model MultivariateMotif in generation 3 of 10\n",
      "Model Number: 351 with model MultivariateRegression in generation 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:17: UserWarning:\n",
      "\n",
      "Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:67: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:68: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:68: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 352 with model MultivariateRegression in generation 3 of 10\n",
      "Model Number: 353 with model MultivariateRegression in generation 3 of 10\n",
      "Model Number: 354 with model MultivariateRegression in generation 3 of 10\n",
      "Model Number: 355 with model DatepartRegression in generation 3 of 10\n",
      "Model Number: 356 with model DatepartRegression in generation 3 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 356: DatepartRegression\n",
      "Model Number: 357 with model DatepartRegression in generation 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\base.py:451: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 358 with model VAR in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 358: VAR\n",
      "Model Number: 359 with model VAR in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 359: VAR\n",
      "Model Number: 360 with model VAR in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 360: VAR\n",
      "Model Number: 361 with model VAR in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 361: VAR\n",
      "Model Number: 362 with model VECM in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 362: VECM\n",
      "Model Number: 363 with model VECM in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 363: VECM\n",
      "Model Number: 364 with model VECM in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 364: VECM\n",
      "Model Number: 365 with model VECM in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 365: VECM\n",
      "Model Number: 366 with model FBProphet in generation 3 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 366: FBProphet\n",
      "Model Number: 367 with model FBProphet in generation 3 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 367: FBProphet\n",
      "Model Number: 368 with model FBProphet in generation 3 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 368: FBProphet\n",
      "Model Number: 369 with model FBProphet in generation 3 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 369: FBProphet\n",
      "New Generation: 4 of 10\n",
      "Model Number: 370 with model LastValueNaive in generation 4 of 10\n",
      "Model Number: 371 with model LastValueNaive in generation 4 of 10\n",
      "Model Number: 372 with model NVAR in generation 4 of 10\n",
      "Model Number: 373 with model NVAR in generation 4 of 10\n",
      "Model Number: 374 with model NVAR in generation 4 of 10\n",
      "Model Number: 375 with model NVAR in generation 4 of 10\n",
      "Model Number: 376 with model ARDL in generation 4 of 10\n",
      "Model Number: 377 with model ARDL in generation 4 of 10\n",
      "Model Number: 378 with model ARDL in generation 4 of 10\n",
      "Model Number: 379 with model ARDL in generation 4 of 10\n",
      "Model Number: 380 with model Theta in generation 4 of 10\n",
      "Model Number: 381 with model Theta in generation 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.295e+01, tolerance: 7.530e-02\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 382 with model Theta in generation 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.194e+00, tolerance: 1.905e-02\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 383 with model UnivariateMotif in generation 4 of 10\n",
      "Model Number: 384 with model UnivariateMotif in generation 4 of 10\n",
      "Model Number: 385 with model UnivariateMotif in generation 4 of 10\n",
      "Model Number: 386 with model UnivariateMotif in generation 4 of 10\n",
      "Model Number: 387 with model AverageValueNaive in generation 4 of 10\n",
      "Model Number: 388 with model AverageValueNaive in generation 4 of 10\n",
      "Model Number: 389 with model AverageValueNaive in generation 4 of 10\n",
      "Model Number: 390 with model ConstantNaive in generation 4 of 10\n",
      "Model Number: 391 with model ConstantNaive in generation 4 of 10\n",
      "Model Number: 392 with model ConstantNaive in generation 4 of 10\n",
      "Model Number: 393 with model GLM in generation 4 of 10\n",
      "Model Number: 394 with model GLM in generation 4 of 10\n",
      "Model Number: 395 with model GLM in generation 4 of 10\n",
      "Model Number: 396 with model GLM in generation 4 of 10\n",
      "Model Number: 397 with model UnobservedComponents in generation 4 of 10\n",
      "Model Number: 398 with model UnobservedComponents in generation 4 of 10\n",
      "Model Number: 399 with model UnobservedComponents in generation 4 of 10\n",
      "Model Number: 400 with model MultivariateRegression in generation 4 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 400: MultivariateRegression\n",
      "Model Number: 401 with model MultivariateRegression in generation 4 of 10\n",
      "Model Number: 402 with model MultivariateRegression in generation 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:67: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:68: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:68: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 403 with model MultivariateRegression in generation 4 of 10\n",
      "Model Number: 404 with model GLS in generation 4 of 10\n",
      "Model Number: 405 with model GLS in generation 4 of 10\n",
      "Model Number: 406 with model GLS in generation 4 of 10\n",
      "Template Eval Error: ValueError('zero-size array to reduction operation maximum which has no identity') in model 406: GLS\n",
      "Model Number: 407 with model SeasonalNaive in generation 4 of 10\n",
      "Model Number: 408 with model SeasonalNaive in generation 4 of 10\n",
      "Model Number: 409 with model SeasonalNaive in generation 4 of 10\n",
      "Model Number: 410 with model SeasonalNaive in generation 4 of 10\n",
      "Model Number: 411 with model WindowRegression in generation 4 of 10\n",
      "Model Number: 412 with model WindowRegression in generation 4 of 10\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 413 with model WindowRegression in generation 4 of 10\n",
      "Model Number: 414 with model SectionalMotif in generation 4 of 10\n",
      "Model Number: 415 with model SectionalMotif in generation 4 of 10\n",
      "Model Number: 416 with model SectionalMotif in generation 4 of 10\n",
      "Model Number: 417 with model SectionalMotif in generation 4 of 10\n",
      "Model Number: 418 with model MultivariateMotif in generation 4 of 10\n",
      "Model Number: 419 with model MultivariateMotif in generation 4 of 10\n",
      "Model Number: 420 with model MultivariateMotif in generation 4 of 10\n",
      "Model Number: 421 with model MultivariateMotif in generation 4 of 10\n",
      "Model Number: 422 with model ETS in generation 4 of 10\n",
      "Model Number: 423 with model ETS in generation 4 of 10\n",
      "Model Number: 424 with model ETS in generation 4 of 10\n",
      "Model Number: 425 with model ETS in generation 4 of 10\n",
      "Model Number: 426 with model DatepartRegression in generation 4 of 10\n",
      "Template Eval Error: ValueError('Failed to convert a NumPy array to a Tensor (Unsupported object type int).') in model 426: DatepartRegression\n",
      "Model Number: 427 with model DatepartRegression in generation 4 of 10\n",
      "Model Number: 428 with model DatepartRegression in generation 4 of 10\n",
      "Epoch 1/100\n",
      "Template Eval Error: TypeError('in user code:\\n\\n    File \"C:\\\\Users\\\\impep\\\\anaconda3\\\\envs\\\\OpenCV\\\\lib\\\\site-packages\\\\keras\\\\engine\\\\training.py\", line 1021, in train_function  *\\n        return step_function(self, iterator)\\n    File \"C:\\\\Users\\\\impep\\\\anaconda3\\\\envs\\\\OpenCV\\\\lib\\\\site-packages\\\\keras\\\\engine\\\\training.py\", line 1010, in step_function  **\\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\\n    File \"C:\\\\Users\\\\impep\\\\anaconda3\\\\envs\\\\OpenCV\\\\lib\\\\site-packages\\\\keras\\\\engine\\\\training.py\", line 1000, in run_step  **\\n        outputs = model.train_step(data)\\n    File \"C:\\\\Users\\\\impep\\\\anaconda3\\\\envs\\\\OpenCV\\\\lib\\\\site-packages\\\\keras\\\\engine\\\\training.py\", line 859, in train_step\\n        y_pred = self(x, training=True)\\n    File \"C:\\\\Users\\\\impep\\\\anaconda3\\\\envs\\\\OpenCV\\\\lib\\\\site-packages\\\\keras\\\\utils\\\\traceback_utils.py\", line 67, in error_handler\\n        raise e.with_traceback(filtered_tb) from None\\n    File \"C:\\\\Users\\\\impep\\\\anaconda3\\\\envs\\\\OpenCV\\\\lib\\\\site-packages\\\\keras\\\\backend.py\", line 2223, in dot\\n        out = tf.matmul(x, y)\\n\\n    TypeError: Exception encountered when calling layer \"forward_lstm\" (type LSTM).\\n    \\n    Input \\'b\\' of \\'MatMul\\' Op has type float32 that does not match type int64 of argument \\'a\\'.\\n    \\n    Call arguments received:\\n      • inputs=tf.Tensor(shape=(None, 1, 7), dtype=int64)\\n      • mask=None\\n      • training=True\\n      • initial_state=None\\n') in model 428: DatepartRegression\n",
      "Model Number: 429 with model VAR in generation 4 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 429: VAR\n",
      "Model Number: 430 with model VAR in generation 4 of 10\n",
      "Template Eval Error: IndexError('tuple index out of range') in model 430: VAR\n",
      "Model Number: 431 with model VAR in generation 4 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 431: VAR\n",
      "Model Number: 432 with model VAR in generation 4 of 10\n",
      "Template Eval Error: IndexError('tuple index out of range') in model 432: VAR\n",
      "Model Number: 433 with model VECM in generation 4 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 433: VECM\n",
      "Model Number: 434 with model VECM in generation 4 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 434: VECM\n",
      "Model Number: 435 with model VECM in generation 4 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 435: VECM\n",
      "Model Number: 436 with model VECM in generation 4 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 436: VECM\n",
      "Model Number: 437 with model FBProphet in generation 4 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 437: FBProphet\n",
      "Model Number: 438 with model FBProphet in generation 4 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 438: FBProphet\n",
      "Model Number: 439 with model FBProphet in generation 4 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 439: FBProphet\n",
      "Model Number: 440 with model FBProphet in generation 4 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 440: FBProphet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.924e+08, tolerance: 6.133e+05\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.669e+13, tolerance: 7.233e+11\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Generation: 5 of 10\n",
      "Model Number: 441 with model LastValueNaive in generation 5 of 10\n",
      "Model Number: 442 with model LastValueNaive in generation 5 of 10\n",
      "Model Number: 443 with model NVAR in generation 5 of 10\n",
      "Model Number: 444 with model NVAR in generation 5 of 10\n",
      "Model Number: 445 with model NVAR in generation 5 of 10\n",
      "Model Number: 446 with model NVAR in generation 5 of 10\n",
      "Model Number: 447 with model ARDL in generation 5 of 10\n",
      "Model Number: 448 with model ARDL in generation 5 of 10\n",
      "Model Number: 449 with model ARDL in generation 5 of 10\n",
      "Model Number: 450 with model ARDL in generation 5 of 10\n",
      "Model Number: 451 with model Theta in generation 5 of 10\n",
      "Model Number: 452 with model Theta in generation 5 of 10\n",
      "Model Number: 453 with model Theta in generation 5 of 10\n",
      "Model Number: 454 with model Theta in generation 5 of 10\n",
      "Model Number: 455 with model UnivariateMotif in generation 5 of 10\n",
      "Model Number: 456 with model UnivariateMotif in generation 5 of 10\n",
      "Template Eval Error: ValueError('kth(=20) out of bounds (11)') in model 456: UnivariateMotif\n",
      "Model Number: 457 with model UnivariateMotif in generation 5 of 10\n",
      "Model Number: 458 with model UnivariateMotif in generation 5 of 10\n",
      "Model Number: 459 with model AverageValueNaive in generation 5 of 10\n",
      "Model Number: 460 with model AverageValueNaive in generation 5 of 10\n",
      "Model Number: 461 with model AverageValueNaive in generation 5 of 10\n",
      "Model Number: 462 with model ConstantNaive in generation 5 of 10\n",
      "Model Number: 463 with model ConstantNaive in generation 5 of 10\n",
      "Model Number: 464 with model ConstantNaive in generation 5 of 10\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-06-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-06-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-06-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-06-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-06-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-06-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-06-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-06-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-06-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-06-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-07-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-08-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-09-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-16 00:00:00 with 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-10-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-11-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2019-12-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-01-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-02-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-03-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-07 00:00:00 with 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-04-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-05-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-06-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-07-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-08-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-28 00:00:00 with 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-09-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-10-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-11-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2020-12-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-01-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-05 00:00:00 with 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-02-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-03-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-04-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-05-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-07 00:00:00 with 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-06-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-07-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-08-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-09-30 00:00:00 with 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-10-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-11-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2021-12-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-01-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-02 00:00:00 with 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-02-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-09 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-10 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-16 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-17 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-23 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-24 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-30 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-03-31 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-01 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-05 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-06 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-07 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-08 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-11 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-12 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-13 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-14 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-15 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-18 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-19 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-20 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-21 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-22 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-25 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-26 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-27 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-28 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-04-29 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-05-02 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-05-03 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for 2022-05-04 00:00:00 with 0\n",
      "SinTrend failed with ValueError('array must not contain infs or NaNs') for Close with 0\n",
      "Template Eval Error: Exception('Transformer SinTrend failed on fit') in model 464: ConstantNaive\n",
      "Model Number: 465 with model GLM in generation 5 of 10\n",
      "Template Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 465: GLM\n",
      "Model Number: 466 with model GLM in generation 5 of 10\n",
      "Template Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 466: GLM\n",
      "Model Number: 467 with model GLM in generation 5 of 10\n",
      "Template Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 467: GLM\n",
      "Model Number: 468 with model GLM in generation 5 of 10\n",
      "Model Number: 469 with model UnobservedComponents in generation 5 of 10\n",
      "Model Number: 470 with model UnobservedComponents in generation 5 of 10\n",
      "Model Number: 471 with model UnobservedComponents in generation 5 of 10\n",
      "Model Number: 472 with model MultivariateRegression in generation 5 of 10\n",
      "Model Number: 473 with model MultivariateRegression in generation 5 of 10\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 473: MultivariateRegression\n",
      "Model Number: 474 with model MultivariateRegression in generation 5 of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 475 with model MultivariateRegression in generation 5 of 10\n",
      "Model Number: 476 with model GLS in generation 5 of 10\n",
      "Model Number: 477 with model GLS in generation 5 of 10\n",
      "Model Number: 478 with model GLS in generation 5 of 10\n",
      "Model Number: 479 with model SeasonalNaive in generation 5 of 10\n",
      "Model Number: 480 with model SeasonalNaive in generation 5 of 10\n",
      "Model Number: 481 with model SeasonalNaive in generation 5 of 10\n",
      "Model Number: 482 with model SeasonalNaive in generation 5 of 10\n",
      "Model Number: 483 with model WindowRegression in generation 5 of 10\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 483: WindowRegression\n",
      "Model Number: 484 with model WindowRegression in generation 5 of 10\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 484: WindowRegression\n",
      "Model Number: 485 with model WindowRegression in generation 5 of 10\n",
      "Model Number: 486 with model SectionalMotif in generation 5 of 10\n",
      "Model Number: 487 with model SectionalMotif in generation 5 of 10\n",
      "Model Number: 488 with model SectionalMotif in generation 5 of 10\n",
      "Model Number: 489 with model SectionalMotif in generation 5 of 10\n",
      "Model Number: 490 with model MultivariateMotif in generation 5 of 10\n",
      "Model Number: 491 with model MultivariateMotif in generation 5 of 10\n",
      "Model Number: 492 with model MultivariateMotif in generation 5 of 10\n",
      "Model Number: 493 with model MultivariateMotif in generation 5 of 10\n",
      "Model Number: 494 with model ETS in generation 5 of 10\n",
      "Model Number: 495 with model ETS in generation 5 of 10\n",
      "ETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
      "ETS failed on Close with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
      "Model Number: 496 with model ETS in generation 5 of 10\n",
      "Model Number: 497 with model ETS in generation 5 of 10\n",
      "ETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
      "ETS failed on Close with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
      "Model Number: 498 with model DatepartRegression in generation 5 of 10\n",
      "Template Eval Error: ValueError('Failed to convert a NumPy array to a Tensor (Unsupported object type int).') in model 498: DatepartRegression\n",
      "Model Number: 499 with model DatepartRegression in generation 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-2)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 500 with model DatepartRegression in generation 5 of 10\n",
      "Model Number: 501 with model VAR in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 501: VAR\n",
      "Model Number: 502 with model VAR in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 502: VAR\n",
      "Model Number: 503 with model VAR in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 503: VAR\n",
      "Model Number: 504 with model VAR in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 504: VAR\n",
      "Model Number: 505 with model VECM in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 505: VECM\n",
      "Model Number: 506 with model VECM in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 506: VECM\n",
      "Model Number: 507 with model VECM in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 507: VECM\n",
      "Model Number: 508 with model VECM in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 508: VECM\n",
      "Model Number: 509 with model FBProphet in generation 5 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 509: FBProphet\n",
      "Model Number: 510 with model FBProphet in generation 5 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 510: FBProphet\n",
      "Model Number: 511 with model FBProphet in generation 5 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 511: FBProphet\n",
      "Model Number: 512 with model FBProphet in generation 5 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 512: FBProphet\n",
      "New Generation: 6 of 10\n",
      "Model Number: 513 with model LastValueNaive in generation 6 of 10\n",
      "Model Number: 514 with model LastValueNaive in generation 6 of 10\n",
      "Model Number: 515 with model LastValueNaive in generation 6 of 10\n",
      "Model Number: 516 with model NVAR in generation 6 of 10\n",
      "Model Number: 517 with model NVAR in generation 6 of 10\n",
      "Model Number: 518 with model NVAR in generation 6 of 10\n",
      "Template Eval Error: ValueError(\"Model returned NaN due to a preprocessing transformer {'fillna': 'quadratic', 'transformations': {'0': 'cffilter', '1': 'IntermittentOccurrence', '2': 'PositiveShift', '3': 'Round'}, 'transformation_params': {'0': {}, '1': {'center': 'mean'}, '2': {}, '3': {'decimals': 2, 'on_transform': True, 'on_inverse': False}}}. fail_on_forecast_nan=True\") in model 518: NVAR\n",
      "Model Number: 519 with model NVAR in generation 6 of 10\n",
      "Model Number: 520 with model ARDL in generation 6 of 10\n",
      "Model Number: 521 with model ARDL in generation 6 of 10\n",
      "Model Number: 522 with model ARDL in generation 6 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 522: ARDL\n",
      "Model Number: 523 with model ARDL in generation 6 of 10\n",
      "Model Number: 524 with model Theta in generation 6 of 10\n",
      "Model Number: 525 with model Theta in generation 6 of 10\n",
      "Model Number: 526 with model Theta in generation 6 of 10\n",
      "Model Number: 527 with model Theta in generation 6 of 10\n",
      "Model Number: 528 with model UnivariateMotif in generation 6 of 10\n",
      "Model Number: 529 with model UnivariateMotif in generation 6 of 10\n",
      "Model Number: 530 with model UnivariateMotif in generation 6 of 10\n",
      "Model Number: 531 with model UnivariateMotif in generation 6 of 10\n",
      "Model Number: 532 with model AverageValueNaive in generation 6 of 10\n",
      "Model Number: 533 with model AverageValueNaive in generation 6 of 10\n",
      "Model Number: 534 with model AverageValueNaive in generation 6 of 10\n",
      "Model Number: 535 with model ConstantNaive in generation 6 of 10\n",
      "Model Number: 536 with model ConstantNaive in generation 6 of 10\n",
      "Model Number: 537 with model ConstantNaive in generation 6 of 10\n",
      "Model Number: 538 with model GLM in generation 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 539 with model GLM in generation 6 of 10\n",
      "Model Number: 540 with model GLM in generation 6 of 10\n",
      "Model Number: 541 with model GLM in generation 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 542 with model UnobservedComponents in generation 6 of 10\n",
      "Model Number: 543 with model UnobservedComponents in generation 6 of 10\n",
      "Model Number: 544 with model UnobservedComponents in generation 6 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 544: UnobservedComponents\n",
      "Model Number: 545 with model MultivariateRegression in generation 6 of 10\n",
      "Model Number: 546 with model MultivariateRegression in generation 6 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 546: MultivariateRegression\n",
      "Model Number: 547 with model MultivariateRegression in generation 6 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 547: MultivariateRegression\n",
      "Model Number: 548 with model MultivariateRegression in generation 6 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 548: MultivariateRegression\n",
      "Model Number: 549 with model GLS in generation 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:461: RuntimeWarning:\n",
      "\n",
      "All-NaN slice encountered\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:462: RuntimeWarning:\n",
      "\n",
      "All-NaN slice encountered\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: Exception('Transformer MinMaxScaler failed on fit') in model 549: GLS\n",
      "Model Number: 550 with model GLS in generation 6 of 10\n",
      "Model Number: 551 with model GLS in generation 6 of 10\n",
      "Model Number: 552 with model SeasonalNaive in generation 6 of 10\n",
      "Model Number: 553 with model SeasonalNaive in generation 6 of 10\n",
      "Model Number: 554 with model SeasonalNaive in generation 6 of 10\n",
      "Model Number: 555 with model SeasonalNaive in generation 6 of 10\n",
      "Model Number: 556 with model WindowRegression in generation 6 of 10\n",
      "Model Number: 557 with model WindowRegression in generation 6 of 10\n",
      "Model Number: 558 with model WindowRegression in generation 6 of 10\n",
      "Epoch 1/100\n",
      "Template Eval Error: ValueError('in user code:\\n\\n    File \"C:\\\\Users\\\\impep\\\\anaconda3\\\\envs\\\\OpenCV\\\\lib\\\\site-packages\\\\keras\\\\engine\\\\training.py\", line 1021, in train_function  *\\n        return step_function(self, iterator)\\n    File \"C:\\\\Users\\\\impep\\\\anaconda3\\\\envs\\\\OpenCV\\\\lib\\\\site-packages\\\\keras\\\\engine\\\\training.py\", line 1010, in step_function  **\\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\\n    File \"C:\\\\Users\\\\impep\\\\anaconda3\\\\envs\\\\OpenCV\\\\lib\\\\site-packages\\\\keras\\\\engine\\\\training.py\", line 1000, in run_step  **\\n        outputs = model.train_step(data)\\n    File \"C:\\\\Users\\\\impep\\\\anaconda3\\\\envs\\\\OpenCV\\\\lib\\\\site-packages\\\\keras\\\\engine\\\\training.py\", line 859, in train_step\\n        y_pred = self(x, training=True)\\n    File \"C:\\\\Users\\\\impep\\\\anaconda3\\\\envs\\\\OpenCV\\\\lib\\\\site-packages\\\\keras\\\\utils\\\\traceback_utils.py\", line 67, in error_handler\\n        raise e.with_traceback(filtered_tb) from None\\n\\n    ValueError: Exception encountered when calling layer \"residual_wrapper\" (type ResidualWrapper).\\n    \\n    in user code:\\n    \\n        File \"C:\\\\Users\\\\impep\\\\anaconda3\\\\envs\\\\OpenCV\\\\lib\\\\site-packages\\\\autots\\\\models\\\\dnn.py\", line 31, in call  *\\n            return inputs + delta\\n    \\n        ValueError: Dimensions must be equal, but are 24 and 30 for \\'{{node residual_wrapper/add}} = AddV2[T=DT_FLOAT](IteratorGetNext, residual_wrapper/sequential/dense_1/BiasAdd)\\' with input shapes: [?,1,24], [?,30].\\n    \\n    \\n    Call arguments received:\\n      • inputs=tf.Tensor(shape=(None, 1, 24), dtype=float32)\\n      • args=<class \\'inspect._empty\\'>\\n      • kwargs={\\'training\\': \\'True\\'}\\n') in model 558: WindowRegression\n",
      "Model Number: 559 with model MultivariateMotif in generation 6 of 10\n",
      "Model Number: 560 with model MultivariateMotif in generation 6 of 10\n",
      "Model Number: 561 with model MultivariateMotif in generation 6 of 10\n",
      "Model Number: 562 with model MultivariateMotif in generation 6 of 10\n",
      "Model Number: 563 with model SectionalMotif in generation 6 of 10\n",
      "Model Number: 564 with model SectionalMotif in generation 6 of 10\n",
      "Model Number: 565 with model SectionalMotif in generation 6 of 10\n",
      "Model Number: 566 with model SectionalMotif in generation 6 of 10\n",
      "Model Number: 567 with model ETS in generation 6 of 10\n",
      "Model Number: 568 with model ETS in generation 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.018e+00, tolerance: 1.951e-02\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 569 with model ETS in generation 6 of 10\n",
      "Model Number: 570 with model ETS in generation 6 of 10\n",
      "Model Number: 571 with model DatepartRegression in generation 6 of 10\n",
      "Template Eval Error: ValueError('Failed to convert a NumPy array to a Tensor (Unsupported object type int).') in model 571: DatepartRegression\n",
      "Model Number: 572 with model DatepartRegression in generation 6 of 10\n",
      "Model Number: 573 with model DatepartRegression in generation 6 of 10\n",
      "Model Number: 574 with model VAR in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 574: VAR\n",
      "Model Number: 575 with model VAR in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 575: VAR\n",
      "Model Number: 576 with model VAR in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 576: VAR\n",
      "Model Number: 577 with model VAR in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 577: VAR\n",
      "Model Number: 578 with model VECM in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 578: VECM\n",
      "Model Number: 579 with model VECM in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 579: VECM\n",
      "Model Number: 580 with model VECM in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 580: VECM\n",
      "Model Number: 581 with model VECM in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 581: VECM\n",
      "Model Number: 582 with model FBProphet in generation 6 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 582: FBProphet\n",
      "Model Number: 583 with model FBProphet in generation 6 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 583: FBProphet\n",
      "Model Number: 584 with model FBProphet in generation 6 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 584: FBProphet\n",
      "Model Number: 585 with model FBProphet in generation 6 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 585: FBProphet\n",
      "New Generation: 7 of 10\n",
      "Model Number: 586 with model LastValueNaive in generation 7 of 10\n",
      "Model Number: 587 with model LastValueNaive in generation 7 of 10\n",
      "Model Number: 588 with model LastValueNaive in generation 7 of 10\n",
      "Model Number: 589 with model ARDL in generation 7 of 10\n",
      "Model Number: 590 with model ARDL in generation 7 of 10\n",
      "Model Number: 591 with model ARDL in generation 7 of 10\n",
      "Model Number: 592 with model ARDL in generation 7 of 10\n",
      "Model Number: 593 with model NVAR in generation 7 of 10\n",
      "Model Number: 594 with model NVAR in generation 7 of 10\n",
      "Model Number: 595 with model NVAR in generation 7 of 10\n",
      "Model Number: 596 with model NVAR in generation 7 of 10\n",
      "Model Number: 597 with model Theta in generation 7 of 10\n",
      "Model Number: 598 with model Theta in generation 7 of 10\n",
      "Model Number: 599 with model Theta in generation 7 of 10\n",
      "Model Number: 600 with model Theta in generation 7 of 10\n",
      "Model Number: 601 with model UnivariateMotif in generation 7 of 10\n",
      "Model Number: 602 with model UnivariateMotif in generation 7 of 10\n",
      "Model Number: 603 with model UnivariateMotif in generation 7 of 10\n",
      "Model Number: 604 with model UnivariateMotif in generation 7 of 10\n",
      "Model Number: 605 with model AverageValueNaive in generation 7 of 10\n",
      "Model Number: 606 with model AverageValueNaive in generation 7 of 10\n",
      "Model Number: 607 with model AverageValueNaive in generation 7 of 10\n",
      "Model Number: 608 with model ConstantNaive in generation 7 of 10\n",
      "Model Number: 609 with model ConstantNaive in generation 7 of 10\n",
      "Model Number: 610 with model ConstantNaive in generation 7 of 10\n",
      "Model Number: 611 with model GLM in generation 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.654e+08, tolerance: 6.081e+05\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:187: RuntimeWarning:\n",
      "\n",
      "overflow encountered in exp\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 612 with model GLM in generation 7 of 10\n",
      "Model Number: 613 with model GLM in generation 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.748e+08, tolerance: 5.158e+05\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 614 with model GLM in generation 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:187: RuntimeWarning:\n",
      "\n",
      "overflow encountered in exp\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 615 with model UnobservedComponents in generation 7 of 10\n",
      "Model Number: 616 with model UnobservedComponents in generation 7 of 10\n",
      "Model Number: 617 with model UnobservedComponents in generation 7 of 10\n",
      "Model Number: 618 with model MultivariateRegression in generation 7 of 10\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 618: MultivariateRegression\n",
      "Model Number: 619 with model MultivariateRegression in generation 7 of 10\n",
      "Model Number: 620 with model MultivariateRegression in generation 7 of 10\n",
      "Model Number: 621 with model MultivariateRegression in generation 7 of 10\n",
      "Model Number: 622 with model GLS in generation 7 of 10\n",
      "Model Number: 623 with model GLS in generation 7 of 10\n",
      "Model Number: 624 with model GLS in generation 7 of 10\n",
      "Model Number: 625 with model ETS in generation 7 of 10\n",
      "Model Number: 626 with model ETS in generation 7 of 10\n",
      "Model Number: 627 with model ETS in generation 7 of 10\n",
      "Model Number: 628 with model ETS in generation 7 of 10\n",
      "Model Number: 629 with model SeasonalNaive in generation 7 of 10\n",
      "Model Number: 630 with model SeasonalNaive in generation 7 of 10\n",
      "Model Number: 631 with model SeasonalNaive in generation 7 of 10\n",
      "Model Number: 632 with model SeasonalNaive in generation 7 of 10\n",
      "Model Number: 633 with model WindowRegression in generation 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 634 with model WindowRegression in generation 7 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 634: WindowRegression\n",
      "Model Number: 635 with model WindowRegression in generation 7 of 10\n",
      "Model Number: 636 with model MultivariateMotif in generation 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:696: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 637 with model MultivariateMotif in generation 7 of 10\n",
      "Model Number: 638 with model MultivariateMotif in generation 7 of 10\n",
      "Model Number: 639 with model MultivariateMotif in generation 7 of 10\n",
      "Model Number: 640 with model SectionalMotif in generation 7 of 10\n",
      "Model Number: 641 with model SectionalMotif in generation 7 of 10\n",
      "Model Number: 642 with model SectionalMotif in generation 7 of 10\n",
      "Model Number: 643 with model SectionalMotif in generation 7 of 10\n",
      "Model Number: 644 with model DatepartRegression in generation 7 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 644: DatepartRegression\n",
      "Model Number: 645 with model DatepartRegression in generation 7 of 10\n",
      "Model Number: 646 with model DatepartRegression in generation 7 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 646: DatepartRegression\n",
      "Model Number: 647 with model VAR in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 647: VAR\n",
      "Model Number: 648 with model VAR in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 648: VAR\n",
      "Model Number: 649 with model VAR in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 649: VAR\n",
      "Model Number: 650 with model VAR in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 650: VAR\n",
      "Model Number: 651 with model VECM in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 651: VECM\n",
      "Model Number: 652 with model VECM in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 652: VECM\n",
      "Model Number: 653 with model VECM in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 653: VECM\n",
      "Model Number: 654 with model VECM in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 654: VECM\n",
      "Model Number: 655 with model FBProphet in generation 7 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 655: FBProphet\n",
      "Model Number: 656 with model FBProphet in generation 7 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 656: FBProphet\n",
      "Model Number: 657 with model FBProphet in generation 7 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 657: FBProphet\n",
      "Model Number: 658 with model FBProphet in generation 7 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 658: FBProphet\n",
      "New Generation: 8 of 10\n",
      "Model Number: 659 with model LastValueNaive in generation 8 of 10\n",
      "Model Number: 660 with model LastValueNaive in generation 8 of 10\n",
      "Model Number: 661 with model LastValueNaive in generation 8 of 10\n",
      "Model Number: 662 with model ARDL in generation 8 of 10\n",
      "Model Number: 663 with model ARDL in generation 8 of 10\n",
      "Model Number: 664 with model ARDL in generation 8 of 10\n",
      "Model Number: 665 with model ARDL in generation 8 of 10\n",
      "Model Number: 666 with model NVAR in generation 8 of 10\n",
      "Model Number: 667 with model NVAR in generation 8 of 10\n",
      "Model Number: 668 with model NVAR in generation 8 of 10\n",
      "Model Number: 669 with model NVAR in generation 8 of 10\n",
      "Model Number: 670 with model Theta in generation 8 of 10\n",
      "Model Number: 671 with model Theta in generation 8 of 10\n",
      "Model Number: 672 with model Theta in generation 8 of 10\n",
      "Model Number: 673 with model Theta in generation 8 of 10\n",
      "Model Number: 674 with model UnivariateMotif in generation 8 of 10\n",
      "Model Number: 675 with model UnivariateMotif in generation 8 of 10\n",
      "Model Number: 676 with model UnivariateMotif in generation 8 of 10\n",
      "Model Number: 677 with model AverageValueNaive in generation 8 of 10\n",
      "Model Number: 678 with model AverageValueNaive in generation 8 of 10\n",
      "Model Number: 679 with model AverageValueNaive in generation 8 of 10\n",
      "Model Number: 680 with model ConstantNaive in generation 8 of 10\n",
      "Model Number: 681 with model ConstantNaive in generation 8 of 10\n",
      "Model Number: 682 with model ConstantNaive in generation 8 of 10\n",
      "Model Number: 683 with model GLM in generation 8 of 10\n",
      "Model Number: 684 with model GLM in generation 8 of 10\n",
      "Template Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.') in model 684: GLM\n",
      "Model Number: 685 with model GLM in generation 8 of 10\n",
      "Template Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 685: GLM\n",
      "Model Number: 686 with model GLM in generation 8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1346: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in log\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:516: RuntimeWarning:\n",
      "\n",
      "overflow encountered in exp\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1346: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in reduce\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:132: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in multiply\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1212: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in multiply\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 687 with model UnobservedComponents in generation 8 of 10\n",
      "Model Number: 688 with model UnobservedComponents in generation 8 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 688: UnobservedComponents\n",
      "Model Number: 689 with model UnobservedComponents in generation 8 of 10\n",
      "Model Number: 690 with model MultivariateRegression in generation 8 of 10\n",
      "Model Number: 691 with model MultivariateRegression in generation 8 of 10\n",
      "Model Number: 692 with model MultivariateRegression in generation 8 of 10\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 692: MultivariateRegression\n",
      "Model Number: 693 with model MultivariateRegression in generation 8 of 10\n",
      "Model Number: 694 with model SeasonalNaive in generation 8 of 10\n",
      "Model Number: 695 with model SeasonalNaive in generation 8 of 10\n",
      "Model Number: 696 with model SeasonalNaive in generation 8 of 10\n",
      "Model Number: 697 with model SeasonalNaive in generation 8 of 10\n",
      "Model Number: 698 with model GLS in generation 8 of 10\n",
      "Model Number: 699 with model GLS in generation 8 of 10\n",
      "Model Number: 700 with model GLS in generation 8 of 10\n",
      "Model Number: 701 with model ETS in generation 8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.961e+08, tolerance: 6.167e+05\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 702 with model ETS in generation 8 of 10\n",
      "Model Number: 703 with model ETS in generation 8 of 10\n",
      "Model Number: 704 with model ETS in generation 8 of 10\n",
      "Model Number: 705 with model WindowRegression in generation 8 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 705: WindowRegression\n",
      "Model Number: 706 with model WindowRegression in generation 8 of 10\n",
      "Model Number: 707 with model WindowRegression in generation 8 of 10\n",
      "Model Number: 708 with model MultivariateMotif in generation 8 of 10\n",
      "Model Number: 709 with model MultivariateMotif in generation 8 of 10\n",
      "Model Number: 710 with model MultivariateMotif in generation 8 of 10\n",
      "Model Number: 711 with model MultivariateMotif in generation 8 of 10\n",
      "Model Number: 712 with model SectionalMotif in generation 8 of 10\n",
      "Template Eval Error: ValueError(\"regression_type=='User' but no future_regressor supplied\") in model 712: SectionalMotif\n",
      "Model Number: 713 with model SectionalMotif in generation 8 of 10\n",
      "Model Number: 714 with model SectionalMotif in generation 8 of 10\n",
      "Model Number: 715 with model SectionalMotif in generation 8 of 10\n",
      "Template Eval Error: ValueError(\"regression_type=='User' but no future_regressor supplied\") in model 715: SectionalMotif\n",
      "Model Number: 716 with model DatepartRegression in generation 8 of 10\n",
      "Template Eval Error: ValueError('Failed to convert a NumPy array to a Tensor (Unsupported object type int).') in model 716: DatepartRegression\n",
      "Model Number: 717 with model DatepartRegression in generation 8 of 10\n",
      "Model Number: 718 with model DatepartRegression in generation 8 of 10\n",
      "Model Number: 719 with model VAR in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 719: VAR\n",
      "Model Number: 720 with model VAR in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 720: VAR\n",
      "Model Number: 721 with model VAR in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 721: VAR\n",
      "Model Number: 722 with model VAR in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 722: VAR\n",
      "Model Number: 723 with model VECM in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 723: VECM\n",
      "Model Number: 724 with model VECM in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 724: VECM\n",
      "Model Number: 725 with model VECM in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 725: VECM\n",
      "Model Number: 726 with model VECM in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 726: VECM\n",
      "Model Number: 727 with model FBProphet in generation 8 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 727: FBProphet\n",
      "Model Number: 728 with model FBProphet in generation 8 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 728: FBProphet\n",
      "Model Number: 729 with model FBProphet in generation 8 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 729: FBProphet\n",
      "Model Number: 730 with model FBProphet in generation 8 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 730: FBProphet\n",
      "New Generation: 9 of 10\n",
      "Model Number: 731 with model LastValueNaive in generation 9 of 10\n",
      "Model Number: 732 with model LastValueNaive in generation 9 of 10\n",
      "Model Number: 733 with model LastValueNaive in generation 9 of 10\n",
      "Model Number: 734 with model MultivariateMotif in generation 9 of 10\n",
      "Model Number: 735 with model MultivariateMotif in generation 9 of 10\n",
      "Model Number: 736 with model MultivariateMotif in generation 9 of 10\n",
      "Model Number: 737 with model MultivariateMotif in generation 9 of 10\n",
      "Model Number: 738 with model ARDL in generation 9 of 10\n",
      "Model Number: 739 with model ARDL in generation 9 of 10\n",
      "Model Number: 740 with model ARDL in generation 9 of 10\n",
      "Model Number: 741 with model ARDL in generation 9 of 10\n",
      "Model Number: 742 with model NVAR in generation 9 of 10\n",
      "Model Number: 743 with model NVAR in generation 9 of 10\n",
      "Model Number: 744 with model NVAR in generation 9 of 10\n",
      "Model Number: 745 with model NVAR in generation 9 of 10\n",
      "Model Number: 746 with model Theta in generation 9 of 10\n",
      "Model Number: 747 with model Theta in generation 9 of 10\n",
      "Model Number: 748 with model Theta in generation 9 of 10\n",
      "Model Number: 749 with model Theta in generation 9 of 10\n",
      "Model Number: 750 with model UnivariateMotif in generation 9 of 10\n",
      "Model Number: 751 with model UnivariateMotif in generation 9 of 10\n",
      "Model Number: 752 with model UnivariateMotif in generation 9 of 10\n",
      "Model Number: 753 with model UnivariateMotif in generation 9 of 10\n",
      "Model Number: 754 with model ConstantNaive in generation 9 of 10\n",
      "Model Number: 755 with model ConstantNaive in generation 9 of 10\n",
      "Model Number: 756 with model AverageValueNaive in generation 9 of 10\n",
      "Model Number: 757 with model AverageValueNaive in generation 9 of 10\n",
      "Model Number: 758 with model AverageValueNaive in generation 9 of 10\n",
      "Model Number: 759 with model GLM in generation 9 of 10\n",
      "Model Number: 760 with model GLM in generation 9 of 10\n",
      "Template Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 760: GLM\n",
      "Model Number: 761 with model GLM in generation 9 of 10\n",
      "Model Number: 762 with model UnobservedComponents in generation 9 of 10\n",
      "Model Number: 763 with model UnobservedComponents in generation 9 of 10\n",
      "Model Number: 764 with model UnobservedComponents in generation 9 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 764: UnobservedComponents\n",
      "Model Number: 765 with model MultivariateRegression in generation 9 of 10\n",
      "Model Number: 766 with model MultivariateRegression in generation 9 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 766: MultivariateRegression\n",
      "Model Number: 767 with model MultivariateRegression in generation 9 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 767: MultivariateRegression\n",
      "Model Number: 768 with model MultivariateRegression in generation 9 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 768: MultivariateRegression\n",
      "Model Number: 769 with model SeasonalNaive in generation 9 of 10\n",
      "Model Number: 770 with model SeasonalNaive in generation 9 of 10\n",
      "Model Number: 771 with model SeasonalNaive in generation 9 of 10\n",
      "Model Number: 772 with model SeasonalNaive in generation 9 of 10\n",
      "Model Number: 773 with model ETS in generation 9 of 10\n",
      "Model Number: 774 with model ETS in generation 9 of 10\n",
      "Template Eval Error: KeyError(Timestamp('2019-06-17 00:00:00', freq='B')) in model 774: ETS\n",
      "Model Number: 775 with model ETS in generation 9 of 10\n",
      "Model Number: 776 with model ETS in generation 9 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\utils\\extmath.py:985: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\utils\\extmath.py:990: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\utils\\extmath.py:1020: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: Exception('Transformer StandardScaler failed on fit') in model 776: ETS\n",
      "Model Number: 777 with model GLS in generation 9 of 10\n",
      "Model Number: 778 with model GLS in generation 9 of 10\n",
      "Model Number: 779 with model GLS in generation 9 of 10\n",
      "Model Number: 780 with model WindowRegression in generation 9 of 10\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 17s 162ms/step - loss: 101.1094 - val_loss: 113.5622\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 22ms/step - loss: 100.5323 - val_loss: 113.5624\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 24ms/step - loss: 100.6309 - val_loss: 113.5632\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 22ms/step - loss: 100.4794 - val_loss: 113.5643\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 26ms/step - loss: 100.5529 - val_loss: 113.5644\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 100.2369 - val_loss: 113.5649\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 1s 31ms/step - loss: 100.0577 - val_loss: 113.5652\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 1s 27ms/step - loss: 100.1277 - val_loss: 113.5663\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 100.4871 - val_loss: 113.5661\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 22ms/step - loss: 100.5493 - val_loss: 113.5679\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 23ms/step - loss: 100.5940 - val_loss: 113.5677\n",
      "Model Number: 781 with model WindowRegression in generation 9 of 10\n",
      "Model Number: 782 with model WindowRegression in generation 9 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 782: WindowRegression\n",
      "Model Number: 783 with model SectionalMotif in generation 9 of 10\n",
      "Model Number: 784 with model SectionalMotif in generation 9 of 10\n",
      "Model Number: 785 with model SectionalMotif in generation 9 of 10\n",
      "Model Number: 786 with model SectionalMotif in generation 9 of 10\n",
      "Model Number: 787 with model DatepartRegression in generation 9 of 10\n",
      "Model Number: 788 with model DatepartRegression in generation 9 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 788: DatepartRegression\n",
      "Model Number: 789 with model DatepartRegression in generation 9 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.609e+08, tolerance: 5.206e+05\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 790 with model VAR in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 790: VAR\n",
      "Model Number: 791 with model VAR in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 791: VAR\n",
      "Model Number: 792 with model VAR in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 792: VAR\n",
      "Model Number: 793 with model VAR in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 793: VAR\n",
      "Model Number: 794 with model VECM in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 794: VECM\n",
      "Model Number: 795 with model VECM in generation 9 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 795: VECM\n",
      "Model Number: 796 with model VECM in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 796: VECM\n",
      "Model Number: 797 with model VECM in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 797: VECM\n",
      "Model Number: 798 with model FBProphet in generation 9 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 798: FBProphet\n",
      "Model Number: 799 with model FBProphet in generation 9 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 799: FBProphet\n",
      "Model Number: 800 with model FBProphet in generation 9 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 800: FBProphet\n",
      "Model Number: 801 with model FBProphet in generation 9 of 10\n",
      "Template Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 801: FBProphet\n",
      "New Generation: 10 of 10\n",
      "Model Number: 802 with model LastValueNaive in generation 10 of 10\n",
      "Model Number: 803 with model LastValueNaive in generation 10 of 10\n",
      "Model Number: 804 with model LastValueNaive in generation 10 of 10\n",
      "Model Number: 805 with model MultivariateMotif in generation 10 of 10\n",
      "Model Number: 806 with model MultivariateMotif in generation 10 of 10\n",
      "Model Number: 807 with model MultivariateMotif in generation 10 of 10\n",
      "Model Number: 808 with model MultivariateMotif in generation 10 of 10\n",
      "Model Number: 809 with model ARDL in generation 10 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 809: ARDL\n",
      "Model Number: 810 with model ARDL in generation 10 of 10\n",
      "Model Number: 811 with model ARDL in generation 10 of 10\n",
      "Template Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 811: ARDL\n",
      "Model Number: 812 with model ARDL in generation 10 of 10\n",
      "Model Number: 813 with model NVAR in generation 10 of 10\n",
      "Model Number: 814 with model NVAR in generation 10 of 10\n",
      "Model Number: 815 with model NVAR in generation 10 of 10\n",
      "Model Number: 816 with model NVAR in generation 10 of 10\n",
      "Model Number: 817 with model Theta in generation 10 of 10\n",
      "Model Number: 818 with model Theta in generation 10 of 10\n",
      "Model Number: 819 with model Theta in generation 10 of 10\n",
      "Model Number: 820 with model Theta in generation 10 of 10\n",
      "Model Number: 821 with model UnivariateMotif in generation 10 of 10\n",
      "Model Number: 822 with model UnivariateMotif in generation 10 of 10\n",
      "Model Number: 823 with model UnivariateMotif in generation 10 of 10\n",
      "Model Number: 824 with model UnivariateMotif in generation 10 of 10\n",
      "Model Number: 825 with model ConstantNaive in generation 10 of 10\n",
      "Model Number: 826 with model ConstantNaive in generation 10 of 10\n",
      "Model Number: 827 with model ConstantNaive in generation 10 of 10\n",
      "Model Number: 828 with model AverageValueNaive in generation 10 of 10\n",
      "Model Number: 829 with model AverageValueNaive in generation 10 of 10\n",
      "Model Number: 830 with model AverageValueNaive in generation 10 of 10\n",
      "Model Number: 831 with model UnobservedComponents in generation 10 of 10\n",
      "Template Eval Error: Exception('Transformer DatepartRegression failed on fit') in model 831: UnobservedComponents\n",
      "Model Number: 832 with model UnobservedComponents in generation 10 of 10\n",
      "Template Eval Error: ValueError(\"'shape' elements cannot be negative\") in model 832: UnobservedComponents\n",
      "Model Number: 833 with model UnobservedComponents in generation 10 of 10\n",
      "Model Number: 834 with model GLM in generation 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\scipy\\interpolate\\polyint.py:545: RuntimeWarning:\n",
      "\n",
      "overflow encountered in multiply\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\scipy\\interpolate\\polyint.py:546: RuntimeWarning:\n",
      "\n",
      "overflow encountered in reduce\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\scipy\\interpolate\\polyint.py:643: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 835 with model GLM in generation 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 836 with model GLM in generation 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\scipy\\interpolate\\polyint.py:545: RuntimeWarning:\n",
      "\n",
      "overflow encountered in multiply\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\scipy\\interpolate\\polyint.py:546: RuntimeWarning:\n",
      "\n",
      "overflow encountered in reduce\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\scipy\\interpolate\\polyint.py:643: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 836: GLM\n",
      "Model Number: 837 with model GLM in generation 10 of 10\n",
      "Model Number: 838 with model MultivariateRegression in generation 10 of 10\n",
      "Model Number: 839 with model MultivariateRegression in generation 10 of 10\n",
      "Model Number: 840 with model MultivariateRegression in generation 10 of 10\n",
      "Model Number: 841 with model MultivariateRegression in generation 10 of 10\n",
      "Model Number: 842 with model SeasonalNaive in generation 10 of 10\n",
      "Model Number: 843 with model SeasonalNaive in generation 10 of 10\n",
      "Model Number: 844 with model SeasonalNaive in generation 10 of 10\n",
      "Model Number: 845 with model SeasonalNaive in generation 10 of 10\n",
      "Model Number: 846 with model ETS in generation 10 of 10\n",
      "Model Number: 847 with model ETS in generation 10 of 10\n",
      "Model Number: 848 with model ETS in generation 10 of 10\n",
      "Model Number: 849 with model ETS in generation 10 of 10\n",
      "Model Number: 850 with model GLS in generation 10 of 10\n",
      "Model Number: 851 with model GLS in generation 10 of 10\n",
      "Model Number: 852 with model GLS in generation 10 of 10\n",
      "Model Number: 853 with model WindowRegression in generation 10 of 10\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 853: WindowRegression\n",
      "Model Number: 854 with model WindowRegression in generation 10 of 10\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Model Number: 855 with model WindowRegression in generation 10 of 10\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 855: WindowRegression\n",
      "Model Number: 856 with model Ensemble in generation 11 of Ensembles\n",
      "Model Number: 857 with model Ensemble in generation 11 of Ensembles\n",
      "Model Number: 858 with model Ensemble in generation 11 of Ensembles\n",
      "Model Number: 859 with model Ensemble in generation 11 of Ensembles\n",
      "Model Number: 860 with model Ensemble in generation 11 of Ensembles\n",
      "Model Number: 861 with model Ensemble in generation 11 of Ensembles\n",
      "Model Number: 862 with model Ensemble in generation 11 of Ensembles\n",
      "Model Number: 863 with model Ensemble in generation 11 of Ensembles\n",
      "Validation Round: 1\n",
      "Model Number: 1 of 130 with model Ensemble for Validation 1\n",
      "📈 1 - Ensemble with avg smape 2.42: \n",
      "Model Number: 2 of 130 with model Ensemble for Validation 1\n",
      "2 - Ensemble with avg smape 2.42: \n",
      "Model Number: 3 of 130 with model Ensemble for Validation 1\n",
      "📈 3 - Ensemble with avg smape 2.31: \n",
      "Model Number: 4 of 130 with model Ensemble for Validation 1\n",
      "📈 4 - Ensemble with avg smape 2.18: \n",
      "Model Number: 5 of 130 with model Ensemble for Validation 1\n",
      "📈 5 - Ensemble with avg smape 2.14: \n",
      "Model Number: 6 of 130 with model LastValueNaive for Validation 1\n",
      "6 - LastValueNaive with avg smape 2.27: \n",
      "Model Number: 7 of 130 with model MultivariateRegression for Validation 1\n",
      "7 - MultivariateRegression with avg smape 2.2: \n",
      "Model Number: 8 of 130 with model LastValueNaive for Validation 1\n",
      "8 - LastValueNaive with avg smape 2.57: \n",
      "Model Number: 9 of 130 with model LastValueNaive for Validation 1\n",
      "📈 9 - LastValueNaive with avg smape 2.05: \n",
      "Model Number: 10 of 130 with model LastValueNaive for Validation 1\n",
      "10 - LastValueNaive with avg smape 2.05: \n",
      "Model Number: 11 of 130 with model Ensemble for Validation 1\n",
      "11 - Ensemble with avg smape 2.05: \n",
      "Model Number: 12 of 130 with model LastValueNaive for Validation 1\n",
      "12 - LastValueNaive with avg smape 2.05: \n",
      "Model Number: 13 of 130 with model LastValueNaive for Validation 1\n",
      "13 - LastValueNaive with avg smape 2.06: \n",
      "Model Number: 14 of 130 with model LastValueNaive for Validation 1\n",
      "14 - LastValueNaive with avg smape 2.1: \n",
      "Model Number: 15 of 130 with model LastValueNaive for Validation 1\n",
      "15 - LastValueNaive with avg smape 2.29: \n",
      "Model Number: 16 of 130 with model MultivariateMotif for Validation 1\n",
      "16 - MultivariateMotif with avg smape 8.22: \n",
      "Model Number: 17 of 130 with model ARDL for Validation 1\n",
      "17 - ARDL with avg smape 2.7: \n",
      "Model Number: 18 of 130 with model NVAR for Validation 1\n",
      "📈 18 - NVAR with avg smape 1.93: \n",
      "Model Number: 19 of 130 with model NVAR for Validation 1\n",
      "19 - NVAR with avg smape 1.93: \n",
      "Model Number: 20 of 130 with model NVAR for Validation 1\n",
      "20 - NVAR with avg smape 1.93: \n",
      "Model Number: 21 of 130 with model NVAR for Validation 1\n",
      "21 - NVAR with avg smape 1.93: \n",
      "Model Number: 22 of 130 with model Ensemble for Validation 1\n",
      "22 - Ensemble with avg smape 2.09: \n",
      "Model Number: 23 of 130 with model NVAR for Validation 1\n",
      "📈 23 - NVAR with avg smape 1.85: \n",
      "Model Number: 24 of 130 with model NVAR for Validation 1\n",
      "24 - NVAR with avg smape 2.29: \n",
      "Model Number: 25 of 130 with model Theta for Validation 1\n",
      "25 - Theta with avg smape 3.06: \n",
      "Model Number: 26 of 130 with model Theta for Validation 1\n",
      "26 - Theta with avg smape 3.06: \n",
      "Model Number: 27 of 130 with model NVAR for Validation 1\n",
      "27 - NVAR with avg smape 2.04: \n",
      "Model Number: 28 of 130 with model NVAR for Validation 1\n",
      "28 - NVAR with avg smape 2.04: \n",
      "Model Number: 29 of 130 with model Theta for Validation 1\n",
      "📈 29 - Theta with avg smape 1.65: \n",
      "Model Number: 30 of 130 with model ARDL for Validation 1\n",
      "30 - ARDL with avg smape 3.73: \n",
      "Model Number: 31 of 130 with model Theta for Validation 1\n",
      "31 - Theta with avg smape 2.69: \n",
      "Model Number: 32 of 130 with model Theta for Validation 1\n",
      "32 - Theta with avg smape 2.66: \n",
      "Model Number: 33 of 130 with model UnivariateMotif for Validation 1\n",
      "33 - UnivariateMotif with avg smape 2.8: \n",
      "Model Number: 34 of 130 with model UnivariateMotif for Validation 1\n",
      "34 - UnivariateMotif with avg smape 2.54: \n",
      "Model Number: 35 of 130 with model Theta for Validation 1\n",
      "35 - Theta with avg smape 2.63: \n",
      "Model Number: 36 of 130 with model Theta for Validation 1\n",
      "36 - Theta with avg smape 2.07: \n",
      "Model Number: 37 of 130 with model Theta for Validation 1\n",
      "37 - Theta with avg smape 2.7: \n",
      "Model Number: 38 of 130 with model ARDL for Validation 1\n",
      "38 - ARDL with avg smape 4.06: \n",
      "Model Number: 39 of 130 with model ARDL for Validation 1\n",
      "39 - ARDL with avg smape 3.61: \n",
      "Model Number: 40 of 130 with model ConstantNaive for Validation 1\n",
      "40 - ConstantNaive with avg smape 2.11: \n",
      "Model Number: 41 of 130 with model ConstantNaive for Validation 1\n",
      "41 - ConstantNaive with avg smape 2.11: \n",
      "Model Number: 42 of 130 with model ARDL for Validation 1\n",
      "42 - ARDL with avg smape 4.06: \n",
      "Model Number: 43 of 130 with model ARDL for Validation 1\n",
      "43 - ARDL with avg smape 3.69: \n",
      "Model Number: 44 of 130 with model ARDL for Validation 1\n",
      "44 - ARDL with avg smape 3.7: \n",
      "Model Number: 45 of 130 with model AverageValueNaive for Validation 1\n",
      "45 - AverageValueNaive with avg smape 1.73: \n",
      "Model Number: 46 of 130 with model ARDL for Validation 1\n",
      "46 - ARDL with avg smape 2.76: \n",
      "Model Number: 47 of 130 with model Ensemble for Validation 1\n",
      "47 - Ensemble with avg smape 2.29: \n",
      "Model Number: 48 of 130 with model ConstantNaive for Validation 1\n",
      "48 - ConstantNaive with avg smape 2.7: \n",
      "Model Number: 49 of 130 with model ConstantNaive for Validation 1\n",
      "49 - ConstantNaive with avg smape 2.7: \n",
      "Model Number: 50 of 130 with model ConstantNaive for Validation 1\n",
      "50 - ConstantNaive with avg smape 2.7: \n",
      "Model Number: 51 of 130 with model ConstantNaive for Validation 1\n",
      "51 - ConstantNaive with avg smape 2.7: \n",
      "Model Number: 52 of 130 with model ConstantNaive for Validation 1\n",
      "52 - ConstantNaive with avg smape 2.7: \n",
      "Model Number: 53 of 130 with model ConstantNaive for Validation 1\n",
      "53 - ConstantNaive with avg smape 2.7: \n",
      "Model Number: 54 of 130 with model AverageValueNaive for Validation 1\n",
      "54 - AverageValueNaive with avg smape 3.89: \n",
      "Model Number: 55 of 130 with model UnobservedComponents for Validation 1\n",
      "55 - UnobservedComponents with avg smape 2.76: \n",
      "Model Number: 56 of 130 with model MultivariateRegression for Validation 1\n",
      "56 - MultivariateRegression with avg smape 1.7: \n",
      "Model Number: 57 of 130 with model UnobservedComponents for Validation 1\n",
      "57 - UnobservedComponents with avg smape 3.54: \n",
      "Model Number: 58 of 130 with model GLM for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 - GLM with avg smape 1.79: \n",
      "Model Number: 59 of 130 with model GLM for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:187: RuntimeWarning:\n",
      "\n",
      "overflow encountered in exp\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:426: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 - GLM with avg smape 1.79: \n",
      "Model Number: 60 of 130 with model GLM for Validation 1\n",
      "Template Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.') in model 60: GLM\n",
      "Model Number: 61 of 130 with model GLM for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:132: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 - GLM with avg smape 1.79: \n",
      "Model Number: 62 of 130 with model GLM for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 - GLM with avg smape 1.79: \n",
      "Model Number: 63 of 130 with model GLM for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 - GLM with avg smape 1.79: \n",
      "Model Number: 64 of 130 with model GLM for Validation 1\n",
      "64 - GLM with avg smape 1.79: \n",
      "Model Number: 65 of 130 with model GLM for Validation 1\n",
      "65 - GLM with avg smape 1.78: \n",
      "Model Number: 66 of 130 with model MultivariateRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:67: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:68: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:68: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 - MultivariateRegression with avg smape 8.45: \n",
      "Model Number: 67 of 130 with model AverageValueNaive for Validation 1\n",
      "67 - AverageValueNaive with avg smape 6.95: \n",
      "Model Number: 68 of 130 with model UnivariateMotif for Validation 1\n",
      "68 - UnivariateMotif with avg smape 3.42: \n",
      "Model Number: 69 of 130 with model SeasonalNaive for Validation 1\n",
      "69 - SeasonalNaive with avg smape 2.5: \n",
      "Model Number: 70 of 130 with model WindowRegression for Validation 1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "70 - WindowRegression with avg smape 2.98: \n",
      "Model Number: 71 of 130 with model UnobservedComponents for Validation 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 71 - UnobservedComponents with avg smape 1.57: \n",
      "Model Number: 72 of 130 with model GLS for Validation 1\n",
      "72 - GLS with avg smape 3.55: \n",
      "Model Number: 73 of 130 with model UnobservedComponents for Validation 1\n",
      "73 - UnobservedComponents with avg smape 3.71: \n",
      "Model Number: 74 of 130 with model UnivariateMotif for Validation 1\n",
      "74 - UnivariateMotif with avg smape 4.18: \n",
      "Model Number: 75 of 130 with model ETS for Validation 1\n",
      "75 - ETS with avg smape 3.49: \n",
      "Model Number: 76 of 130 with model ETS for Validation 1\n",
      "76 - ETS with avg smape 3.5: \n",
      "Model Number: 77 of 130 with model UnobservedComponents for Validation 1\n",
      "77 - UnobservedComponents with avg smape 4.27: \n",
      "Model Number: 78 of 130 with model MultivariateRegression for Validation 1\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 78: MultivariateRegression\n",
      "Model Number: 79 of 130 with model ETS for Validation 1\n",
      "79 - ETS with avg smape 3.49: \n",
      "Model Number: 80 of 130 with model GLS for Validation 1\n",
      "80 - GLS with avg smape 3.54: \n",
      "Model Number: 81 of 130 with model SeasonalNaive for Validation 1\n",
      "📈 81 - SeasonalNaive with avg smape 1.49: \n",
      "Model Number: 82 of 130 with model ETS for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.766e+08, tolerance: 5.935e+05\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 - ETS with avg smape 5.09: \n",
      "Model Number: 83 of 130 with model MultivariateRegression for Validation 1\n",
      "83 - MultivariateRegression with avg smape 3.25: \n",
      "Model Number: 84 of 130 with model ETS for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.058e-01, tolerance: 1.893e-02\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 - ETS with avg smape 4.7: \n",
      "Model Number: 85 of 130 with model SeasonalNaive for Validation 1\n",
      "85 - SeasonalNaive with avg smape 2.32: \n",
      "Model Number: 86 of 130 with model UnobservedComponents for Validation 1\n",
      "86 - UnobservedComponents with avg smape 3.94: \n",
      "Model Number: 87 of 130 with model SeasonalNaive for Validation 1\n",
      "87 - SeasonalNaive with avg smape 2.34: \n",
      "Model Number: 88 of 130 with model SeasonalNaive for Validation 1\n",
      "88 - SeasonalNaive with avg smape 2.1: \n",
      "Model Number: 89 of 130 with model UnivariateMotif for Validation 1\n",
      "89 - UnivariateMotif with avg smape 5.06: \n",
      "Model Number: 90 of 130 with model UnobservedComponents for Validation 1\n",
      "90 - UnobservedComponents with avg smape 3.79: \n",
      "Model Number: 91 of 130 with model AverageValueNaive for Validation 1\n",
      "91 - AverageValueNaive with avg smape 2.01: \n",
      "Model Number: 92 of 130 with model AverageValueNaive for Validation 1\n",
      "92 - AverageValueNaive with avg smape 7.63: \n",
      "Model Number: 93 of 130 with model WindowRegression for Validation 1\n",
      "93 - WindowRegression with avg smape 5.92: \n",
      "Model Number: 94 of 130 with model WindowRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-2)]: Done 444 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-2)]: Done 794 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-2)]: Done 1000 out of 1000 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 - WindowRegression with avg smape 4.03: \n",
      "Model Number: 95 of 130 with model WindowRegression for Validation 1\n",
      "95 - WindowRegression with avg smape 2.2: \n",
      "Model Number: 96 of 130 with model MultivariateRegression for Validation 1\n",
      "96 - MultivariateRegression with avg smape 1.99: \n",
      "Model Number: 97 of 130 with model GLS for Validation 1\n",
      "97 - GLS with avg smape 4.49: \n",
      "Model Number: 98 of 130 with model WindowRegression for Validation 1\n",
      "98 - WindowRegression with avg smape 1.71: \n",
      "Model Number: 99 of 130 with model ETS for Validation 1\n",
      "99 - ETS with avg smape 4.31: \n",
      "Model Number: 100 of 130 with model SectionalMotif for Validation 1\n",
      "100 - SectionalMotif with avg smape 3.32: \n",
      "Model Number: 101 of 130 with model UnobservedComponents for Validation 1\n",
      "101 - UnobservedComponents with avg smape 4.48: \n",
      "Model Number: 102 of 130 with model WindowRegression for Validation 1\n",
      "102 - WindowRegression with avg smape 2.61: \n",
      "Model Number: 103 of 130 with model SeasonalNaive for Validation 1\n",
      "103 - SeasonalNaive with avg smape 2.28: \n",
      "Model Number: 104 of 130 with model ETS for Validation 1\n",
      "104 - ETS with avg smape 4.86: \n",
      "Model Number: 105 of 130 with model ETS for Validation 1\n",
      "105 - ETS with avg smape 3.48: \n",
      "Model Number: 106 of 130 with model MultivariateMotif for Validation 1\n",
      "106 - MultivariateMotif with avg smape 16.13: \n",
      "Model Number: 107 of 130 with model SectionalMotif for Validation 1\n",
      "107 - SectionalMotif with avg smape 3.41: \n",
      "Model Number: 108 of 130 with model MultivariateMotif for Validation 1\n",
      "108 - MultivariateMotif with avg smape 3.07: \n",
      "Model Number: 109 of 130 with model SeasonalNaive for Validation 1\n",
      "109 - SeasonalNaive with avg smape 4.81: \n",
      "Model Number: 110 of 130 with model WindowRegression for Validation 1\n",
      "110 - WindowRegression with avg smape 3.24: \n",
      "Model Number: 111 of 130 with model MultivariateMotif for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.379e+06, tolerance: 5.816e+05\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 - MultivariateMotif with avg smape 2.24: \n",
      "Model Number: 112 of 130 with model UnivariateMotif for Validation 1\n",
      "112 - UnivariateMotif with avg smape 3.75: \n",
      "Model Number: 113 of 130 with model MultivariateRegression for Validation 1\n",
      "113 - MultivariateRegression with avg smape 3.49: \n",
      "Model Number: 114 of 130 with model MultivariateMotif for Validation 1\n",
      "114 - MultivariateMotif with avg smape 3.74: \n",
      "Model Number: 115 of 130 with model UnivariateMotif for Validation 1\n",
      "115 - UnivariateMotif with avg smape 2.21: \n",
      "Model Number: 116 of 130 with model AverageValueNaive for Validation 1\n",
      "116 - AverageValueNaive with avg smape 1.78: \n",
      "Model Number: 117 of 130 with model GLS for Validation 1\n",
      "117 - GLS with avg smape 3.93: \n",
      "Model Number: 118 of 130 with model AverageValueNaive for Validation 1\n",
      "118 - AverageValueNaive with avg smape 5.27: \n",
      "Model Number: 119 of 130 with model MultivariateRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 - MultivariateRegression with avg smape 3.97: \n",
      "Model Number: 120 of 130 with model WindowRegression for Validation 1\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 10s 84ms/step - loss: 99.2440 - val_loss: 111.3699\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 99.2834 - val_loss: 111.3751\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 99.2982 - val_loss: 111.3863\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 99.2836 - val_loss: 111.3936\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 99.2469 - val_loss: 111.4016\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 99.1917 - val_loss: 111.4130\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 99.2039 - val_loss: 111.4200\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 99.2091 - val_loss: 111.4346\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 99.2968 - val_loss: 111.4445\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 99.2864 - val_loss: 111.4534\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 99.2964 - val_loss: 111.4684\n",
      "120 - WindowRegression with avg smape 4.71: \n",
      "Model Number: 121 of 130 with model MultivariateMotif for Validation 1\n",
      "121 - MultivariateMotif with avg smape 7.41: \n",
      "Model Number: 122 of 130 with model SectionalMotif for Validation 1\n",
      "122 - SectionalMotif with avg smape 5.63: \n",
      "Model Number: 123 of 130 with model DatepartRegression for Validation 1\n",
      "123 - DatepartRegression with avg smape 7.22: \n",
      "Model Number: 124 of 130 with model SeasonalNaive for Validation 1\n",
      "124 - SeasonalNaive with avg smape 3.03: \n",
      "Model Number: 125 of 130 with model MultivariateMotif for Validation 1\n",
      "125 - MultivariateMotif with avg smape 4.55: \n",
      "Model Number: 126 of 130 with model UnivariateMotif for Validation 1\n",
      "126 - UnivariateMotif with avg smape 2.78: \n",
      "Model Number: 127 of 130 with model SectionalMotif for Validation 1\n",
      "127 - SectionalMotif with avg smape 4.09: \n",
      "Model Number: 128 of 130 with model SectionalMotif for Validation 1\n",
      "128 - SectionalMotif with avg smape 3.4: \n",
      "Model Number: 129 of 130 with model GLS for Validation 1\n",
      "129 - GLS with avg smape 2.73: \n",
      "Model Number: 130 of 130 with model MultivariateMotif for Validation 1\n",
      "130 - MultivariateMotif with avg smape 3.41: \n",
      "Validation Round: 2\n",
      "Model Number: 1 of 130 with model Ensemble for Validation 2\n",
      "📈 1 - Ensemble with avg smape 13.2: \n",
      "Model Number: 2 of 130 with model Ensemble for Validation 2\n",
      "2 - Ensemble with avg smape 13.2: \n",
      "Model Number: 3 of 130 with model Ensemble for Validation 2\n",
      "📈 3 - Ensemble with avg smape 12.29: \n",
      "Model Number: 4 of 130 with model Ensemble for Validation 2\n",
      "📈 4 - Ensemble with avg smape 2.0: \n",
      "Model Number: 5 of 130 with model Ensemble for Validation 2\n",
      "5 - Ensemble with avg smape 2.11: \n",
      "Model Number: 6 of 130 with model LastValueNaive for Validation 2\n",
      "6 - LastValueNaive with avg smape 2.16: \n",
      "Model Number: 7 of 130 with model MultivariateRegression for Validation 2\n",
      "7 - MultivariateRegression with avg smape 2.33: \n",
      "Model Number: 8 of 130 with model LastValueNaive for Validation 2\n",
      "8 - LastValueNaive with avg smape 6.16: \n",
      "Model Number: 9 of 130 with model LastValueNaive for Validation 2\n",
      "📈 9 - LastValueNaive with avg smape 1.86: \n",
      "Model Number: 10 of 130 with model LastValueNaive for Validation 2\n",
      "10 - LastValueNaive with avg smape 1.86: \n",
      "Model Number: 11 of 130 with model Ensemble for Validation 2\n",
      "11 - Ensemble with avg smape 1.86: \n",
      "Model Number: 12 of 130 with model LastValueNaive for Validation 2\n",
      "12 - LastValueNaive with avg smape 1.86: \n",
      "Model Number: 13 of 130 with model LastValueNaive for Validation 2\n",
      "13 - LastValueNaive with avg smape 1.87: \n",
      "Model Number: 14 of 130 with model LastValueNaive for Validation 2\n",
      "14 - LastValueNaive with avg smape 1.9: \n",
      "Model Number: 15 of 130 with model LastValueNaive for Validation 2\n",
      "15 - LastValueNaive with avg smape 2.17: \n",
      "Model Number: 16 of 130 with model MultivariateMotif for Validation 2\n",
      "16 - MultivariateMotif with avg smape 47.41: \n",
      "Model Number: 17 of 130 with model ARDL for Validation 2\n",
      "17 - ARDL with avg smape 2.91: \n",
      "Model Number: 18 of 130 with model NVAR for Validation 2\n",
      "📈 18 - NVAR with avg smape 1.65: \n",
      "Model Number: 19 of 130 with model NVAR for Validation 2\n",
      "19 - NVAR with avg smape 1.65: \n",
      "Model Number: 20 of 130 with model NVAR for Validation 2\n",
      "20 - NVAR with avg smape 1.65: \n",
      "Model Number: 21 of 130 with model NVAR for Validation 2\n",
      "21 - NVAR with avg smape 1.65: \n",
      "Model Number: 22 of 130 with model Ensemble for Validation 2\n",
      "22 - Ensemble with avg smape 2.03: \n",
      "Model Number: 23 of 130 with model NVAR for Validation 2\n",
      "23 - NVAR with avg smape 1.65: \n",
      "Model Number: 24 of 130 with model NVAR for Validation 2\n",
      "24 - NVAR with avg smape 1.67: \n",
      "Model Number: 25 of 130 with model Theta for Validation 2\n",
      "25 - Theta with avg smape 5.05: \n",
      "Model Number: 26 of 130 with model Theta for Validation 2\n",
      "26 - Theta with avg smape 5.05: \n",
      "Model Number: 27 of 130 with model NVAR for Validation 2\n",
      "27 - NVAR with avg smape 1.76: \n",
      "Model Number: 28 of 130 with model NVAR for Validation 2\n",
      "28 - NVAR with avg smape 1.76: \n",
      "Model Number: 29 of 130 with model Theta for Validation 2\n",
      "29 - Theta with avg smape 1.86: \n",
      "Model Number: 30 of 130 with model ARDL for Validation 2\n",
      "30 - ARDL with avg smape 3.54: \n",
      "Model Number: 31 of 130 with model Theta for Validation 2\n",
      "31 - Theta with avg smape 2.29: \n",
      "Model Number: 32 of 130 with model Theta for Validation 2\n",
      "32 - Theta with avg smape 2.26: \n",
      "Model Number: 33 of 130 with model UnivariateMotif for Validation 2\n",
      "33 - UnivariateMotif with avg smape 2.42: \n",
      "Model Number: 34 of 130 with model UnivariateMotif for Validation 2\n",
      "34 - UnivariateMotif with avg smape 3.1: \n",
      "Model Number: 35 of 130 with model Theta for Validation 2\n",
      "35 - Theta with avg smape 2.27: \n",
      "Model Number: 36 of 130 with model Theta for Validation 2\n",
      "36 - Theta with avg smape 2.04: \n",
      "Model Number: 37 of 130 with model Theta for Validation 2\n",
      "37 - Theta with avg smape 2.31: \n",
      "Model Number: 38 of 130 with model ARDL for Validation 2\n",
      "38 - ARDL with avg smape 3.49: \n",
      "Model Number: 39 of 130 with model ARDL for Validation 2\n",
      "39 - ARDL with avg smape 3.25: \n",
      "Model Number: 40 of 130 with model ConstantNaive for Validation 2\n",
      "40 - ConstantNaive with avg smape 2.18: \n",
      "Model Number: 41 of 130 with model ConstantNaive for Validation 2\n",
      "41 - ConstantNaive with avg smape 2.18: \n",
      "Model Number: 42 of 130 with model ARDL for Validation 2\n",
      "42 - ARDL with avg smape 3.35: \n",
      "Model Number: 43 of 130 with model ARDL for Validation 2\n",
      "43 - ARDL with avg smape 3.25: \n",
      "Model Number: 44 of 130 with model ARDL for Validation 2\n",
      "44 - ARDL with avg smape 3.24: \n",
      "Model Number: 45 of 130 with model AverageValueNaive for Validation 2\n",
      "45 - AverageValueNaive with avg smape 2.15: \n",
      "Model Number: 46 of 130 with model ARDL for Validation 2\n",
      "46 - ARDL with avg smape 2.71: \n",
      "Model Number: 47 of 130 with model Ensemble for Validation 2\n",
      "47 - Ensemble with avg smape 4.24: \n",
      "Model Number: 48 of 130 with model ConstantNaive for Validation 2\n",
      "48 - ConstantNaive with avg smape 2.38: \n",
      "Model Number: 49 of 130 with model ConstantNaive for Validation 2\n",
      "49 - ConstantNaive with avg smape 2.38: \n",
      "Model Number: 50 of 130 with model ConstantNaive for Validation 2\n",
      "50 - ConstantNaive with avg smape 2.38: \n",
      "Model Number: 51 of 130 with model ConstantNaive for Validation 2\n",
      "51 - ConstantNaive with avg smape 2.38: \n",
      "Model Number: 52 of 130 with model ConstantNaive for Validation 2\n",
      "52 - ConstantNaive with avg smape 2.38: \n",
      "Model Number: 53 of 130 with model ConstantNaive for Validation 2\n",
      "53 - ConstantNaive with avg smape 2.38: \n",
      "Model Number: 54 of 130 with model AverageValueNaive for Validation 2\n",
      "54 - AverageValueNaive with avg smape 3.32: \n",
      "Model Number: 55 of 130 with model UnobservedComponents for Validation 2\n",
      "55 - UnobservedComponents with avg smape 2.7: \n",
      "Model Number: 56 of 130 with model MultivariateRegression for Validation 2\n",
      "56 - MultivariateRegression with avg smape 9.03: \n",
      "Model Number: 57 of 130 with model UnobservedComponents for Validation 2\n",
      "57 - UnobservedComponents with avg smape 3.85: \n",
      "Model Number: 58 of 130 with model GLM for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 - GLM with avg smape 3.77: \n",
      "Model Number: 59 of 130 with model GLM for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:187: RuntimeWarning:\n",
      "\n",
      "overflow encountered in exp\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 - GLM with avg smape 3.77: \n",
      "Model Number: 60 of 130 with model GLM for Validation 2\n",
      "Template Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.') in model 60: GLM\n",
      "Model Number: 61 of 130 with model GLM for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:426: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:132: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 - GLM with avg smape 3.77: \n",
      "Model Number: 62 of 130 with model GLM for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 - GLM with avg smape 3.77: \n",
      "Model Number: 63 of 130 with model GLM for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 - GLM with avg smape 3.77: \n",
      "Model Number: 64 of 130 with model GLM for Validation 2\n",
      "64 - GLM with avg smape 3.77: \n",
      "Model Number: 65 of 130 with model GLM for Validation 2\n",
      "65 - GLM with avg smape 3.77: \n",
      "Model Number: 66 of 130 with model MultivariateRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:67: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:68: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:68: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 - MultivariateRegression with avg smape 12.15: \n",
      "Model Number: 67 of 130 with model AverageValueNaive for Validation 2\n",
      "67 - AverageValueNaive with avg smape 4.34: \n",
      "Model Number: 68 of 130 with model UnivariateMotif for Validation 2\n",
      "68 - UnivariateMotif with avg smape 3.28: \n",
      "Model Number: 69 of 130 with model SeasonalNaive for Validation 2\n",
      "69 - SeasonalNaive with avg smape 7.93: \n",
      "Model Number: 70 of 130 with model WindowRegression for Validation 2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "70 - WindowRegression with avg smape 2.85: \n",
      "Model Number: 71 of 130 with model UnobservedComponents for Validation 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 - UnobservedComponents with avg smape 2.32: \n",
      "Model Number: 72 of 130 with model GLS for Validation 2\n",
      "72 - GLS with avg smape 3.22: \n",
      "Model Number: 73 of 130 with model UnobservedComponents for Validation 2\n",
      "73 - UnobservedComponents with avg smape 3.37: \n",
      "Model Number: 74 of 130 with model UnivariateMotif for Validation 2\n",
      "74 - UnivariateMotif with avg smape 5.14: \n",
      "Model Number: 75 of 130 with model ETS for Validation 2\n",
      "75 - ETS with avg smape 3.04: \n",
      "Model Number: 76 of 130 with model ETS for Validation 2\n",
      "76 - ETS with avg smape 3.04: \n",
      "Model Number: 77 of 130 with model UnobservedComponents for Validation 2\n",
      "77 - UnobservedComponents with avg smape 3.93: \n",
      "Model Number: 78 of 130 with model MultivariateRegression for Validation 2\n",
      "78 - MultivariateRegression with avg smape 2.16: \n",
      "Model Number: 79 of 130 with model ETS for Validation 2\n",
      "79 - ETS with avg smape 3.17: \n",
      "Model Number: 80 of 130 with model GLS for Validation 2\n",
      "80 - GLS with avg smape 3.06: \n",
      "Model Number: 81 of 130 with model SeasonalNaive for Validation 2\n",
      "81 - SeasonalNaive with avg smape 1.84: \n",
      "Model Number: 82 of 130 with model ETS for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.312e+08, tolerance: 5.467e+05\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 - ETS with avg smape 3.47: \n",
      "Model Number: 83 of 130 with model MultivariateRegression for Validation 2\n",
      "83 - MultivariateRegression with avg smape 2.21: \n",
      "Model Number: 84 of 130 with model ETS for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.015e+00, tolerance: 1.803e-02\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 - ETS with avg smape 3.84: \n",
      "Model Number: 85 of 130 with model SeasonalNaive for Validation 2\n",
      "85 - SeasonalNaive with avg smape 5.52: \n",
      "Model Number: 86 of 130 with model UnobservedComponents for Validation 2\n",
      "86 - UnobservedComponents with avg smape 3.19: \n",
      "Model Number: 87 of 130 with model SeasonalNaive for Validation 2\n",
      "87 - SeasonalNaive with avg smape 5.5: \n",
      "Model Number: 88 of 130 with model SeasonalNaive for Validation 2\n",
      "88 - SeasonalNaive with avg smape 3.67: \n",
      "Model Number: 89 of 130 with model UnivariateMotif for Validation 2\n",
      "89 - UnivariateMotif with avg smape 3.52: \n",
      "Model Number: 90 of 130 with model UnobservedComponents for Validation 2\n",
      "90 - UnobservedComponents with avg smape 3.21: \n",
      "Model Number: 91 of 130 with model AverageValueNaive for Validation 2\n",
      "91 - AverageValueNaive with avg smape 1.96: \n",
      "Model Number: 92 of 130 with model AverageValueNaive for Validation 2\n",
      "92 - AverageValueNaive with avg smape 12.27: \n",
      "Model Number: 93 of 130 with model WindowRegression for Validation 2\n",
      "93 - WindowRegression with avg smape 3.37: \n",
      "Model Number: 94 of 130 with model WindowRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-2)]: Done 444 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-2)]: Done 794 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-2)]: Done 1000 out of 1000 | elapsed:    4.9s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 - WindowRegression with avg smape 3.2: \n",
      "Model Number: 95 of 130 with model WindowRegression for Validation 2\n",
      "95 - WindowRegression with avg smape 1.77: \n",
      "Model Number: 96 of 130 with model MultivariateRegression for Validation 2\n",
      "96 - MultivariateRegression with avg smape 6.62: \n",
      "Model Number: 97 of 130 with model GLS for Validation 2\n",
      "97 - GLS with avg smape 3.74: \n",
      "Model Number: 98 of 130 with model WindowRegression for Validation 2\n",
      "98 - WindowRegression with avg smape 1.89: \n",
      "Model Number: 99 of 130 with model ETS for Validation 2\n",
      "99 - ETS with avg smape 2.87: \n",
      "Model Number: 100 of 130 with model SectionalMotif for Validation 2\n",
      "100 - SectionalMotif with avg smape 5.26: \n",
      "Model Number: 101 of 130 with model UnobservedComponents for Validation 2\n",
      "101 - UnobservedComponents with avg smape 3.73: \n",
      "Model Number: 102 of 130 with model WindowRegression for Validation 2\n",
      "102 - WindowRegression with avg smape 10.88: \n",
      "Model Number: 103 of 130 with model SeasonalNaive for Validation 2\n",
      "103 - SeasonalNaive with avg smape 1.93: \n",
      "Model Number: 104 of 130 with model ETS for Validation 2\n",
      "104 - ETS with avg smape 4.14: \n",
      "Model Number: 105 of 130 with model ETS for Validation 2\n",
      "105 - ETS with avg smape 3.36: \n",
      "Model Number: 106 of 130 with model MultivariateMotif for Validation 2\n",
      "106 - MultivariateMotif with avg smape 17.36: \n",
      "Model Number: 107 of 130 with model SectionalMotif for Validation 2\n",
      "107 - SectionalMotif with avg smape 8.29: \n",
      "Model Number: 108 of 130 with model MultivariateMotif for Validation 2\n",
      "108 - MultivariateMotif with avg smape 2.49: \n",
      "Model Number: 109 of 130 with model SeasonalNaive for Validation 2\n",
      "109 - SeasonalNaive with avg smape 5.45: \n",
      "Model Number: 110 of 130 with model WindowRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.521e+06, tolerance: 5.406e+05\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 - WindowRegression with avg smape 2.81: \n",
      "Model Number: 111 of 130 with model MultivariateMotif for Validation 2\n",
      "111 - MultivariateMotif with avg smape 7.09: \n",
      "Model Number: 112 of 130 with model UnivariateMotif for Validation 2\n",
      "112 - UnivariateMotif with avg smape 1.8: \n",
      "Model Number: 113 of 130 with model MultivariateRegression for Validation 2\n",
      "113 - MultivariateRegression with avg smape 2.17: \n",
      "Model Number: 114 of 130 with model MultivariateMotif for Validation 2\n",
      "114 - MultivariateMotif with avg smape 8.75: \n",
      "Model Number: 115 of 130 with model UnivariateMotif for Validation 2\n",
      "115 - UnivariateMotif with avg smape 7.46: \n",
      "Model Number: 116 of 130 with model AverageValueNaive for Validation 2\n",
      "116 - AverageValueNaive with avg smape 4.61: \n",
      "Model Number: 117 of 130 with model GLS for Validation 2\n",
      "117 - GLS with avg smape 3.48: \n",
      "Model Number: 118 of 130 with model AverageValueNaive for Validation 2\n",
      "118 - AverageValueNaive with avg smape 3.74: \n",
      "Model Number: 119 of 130 with model MultivariateRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 - MultivariateRegression with avg smape 2.28: \n",
      "Model Number: 120 of 130 with model WindowRegression for Validation 2\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 24s 310ms/step - loss: 99.5590 - val_loss: 103.4906\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 98.5988 - val_loss: 103.4897\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 98.0109 - val_loss: 103.4895\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 97.7775 - val_loss: 103.4966\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 98.3618 - val_loss: 103.5075\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 1s 58ms/step - loss: 97.5772 - val_loss: 103.5201\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 99.9695 - val_loss: 103.5243\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 99.5447 - val_loss: 103.5418\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 99.2083 - val_loss: 103.5426\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 100.1419 - val_loss: 103.5384\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 98.6760 - val_loss: 103.5516\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 99.9153 - val_loss: 103.5497\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 98.8464 - val_loss: 103.5610\n",
      "120 - WindowRegression with avg smape 3.88: \n",
      "Model Number: 121 of 130 with model MultivariateMotif for Validation 2\n",
      "121 - MultivariateMotif with avg smape 3.07: \n",
      "Model Number: 122 of 130 with model SectionalMotif for Validation 2\n",
      "122 - SectionalMotif with avg smape 4.27: \n",
      "Model Number: 123 of 130 with model DatepartRegression for Validation 2\n",
      "123 - DatepartRegression with avg smape 3.08: \n",
      "Model Number: 124 of 130 with model SeasonalNaive for Validation 2\n",
      "124 - SeasonalNaive with avg smape 7.14: \n",
      "Model Number: 125 of 130 with model MultivariateMotif for Validation 2\n",
      "125 - MultivariateMotif with avg smape 4.06: \n",
      "Model Number: 126 of 130 with model UnivariateMotif for Validation 2\n",
      "126 - UnivariateMotif with avg smape 1.84: \n",
      "Model Number: 127 of 130 with model SectionalMotif for Validation 2\n",
      "127 - SectionalMotif with avg smape 3.28: \n",
      "Model Number: 128 of 130 with model SectionalMotif for Validation 2\n",
      "128 - SectionalMotif with avg smape 7.18: \n",
      "Model Number: 129 of 130 with model GLS for Validation 2\n",
      "129 - GLS with avg smape 6.82: \n",
      "Model Number: 130 of 130 with model MultivariateMotif for Validation 2\n",
      "130 - MultivariateMotif with avg smape 8.57: \n",
      "Validation Round: 3\n",
      "Model Number: 1 of 130 with model Ensemble for Validation 3\n",
      "📈 1 - Ensemble with avg smape 1.42: \n",
      "Model Number: 2 of 130 with model Ensemble for Validation 3\n",
      "2 - Ensemble with avg smape 1.42: \n",
      "Model Number: 3 of 130 with model Ensemble for Validation 3\n",
      "3 - Ensemble with avg smape 1.42: \n",
      "Model Number: 4 of 130 with model Ensemble for Validation 3\n",
      "📈 4 - Ensemble with avg smape 1.29: \n",
      "Model Number: 5 of 130 with model Ensemble for Validation 3\n",
      "5 - Ensemble with avg smape 1.31: \n",
      "Model Number: 6 of 130 with model LastValueNaive for Validation 3\n",
      "6 - LastValueNaive with avg smape 1.42: \n",
      "Model Number: 7 of 130 with model MultivariateRegression for Validation 3\n",
      "7 - MultivariateRegression with avg smape 1.89: \n",
      "Model Number: 8 of 130 with model LastValueNaive for Validation 3\n",
      "8 - LastValueNaive with avg smape 10.44: \n",
      "Model Number: 9 of 130 with model LastValueNaive for Validation 3\n",
      "📈 9 - LastValueNaive with avg smape 1.1: \n",
      "Model Number: 10 of 130 with model LastValueNaive for Validation 3\n",
      "10 - LastValueNaive with avg smape 1.1: \n",
      "Model Number: 11 of 130 with model Ensemble for Validation 3\n",
      "11 - Ensemble with avg smape 1.1: \n",
      "Model Number: 12 of 130 with model LastValueNaive for Validation 3\n",
      "12 - LastValueNaive with avg smape 1.1: \n",
      "Model Number: 13 of 130 with model LastValueNaive for Validation 3\n",
      "13 - LastValueNaive with avg smape 1.1: \n",
      "Model Number: 14 of 130 with model LastValueNaive for Validation 3\n",
      "📈 14 - LastValueNaive with avg smape 1.09: \n",
      "Model Number: 15 of 130 with model LastValueNaive for Validation 3\n",
      "15 - LastValueNaive with avg smape 1.49: \n",
      "Model Number: 16 of 130 with model MultivariateMotif for Validation 3\n",
      "16 - MultivariateMotif with avg smape 1.65: \n",
      "Model Number: 17 of 130 with model ARDL for Validation 3\n",
      "17 - ARDL with avg smape 1.61: \n",
      "Model Number: 18 of 130 with model NVAR for Validation 3\n",
      "18 - NVAR with avg smape 1.74: \n",
      "Model Number: 19 of 130 with model NVAR for Validation 3\n",
      "19 - NVAR with avg smape 1.74: \n",
      "Model Number: 20 of 130 with model NVAR for Validation 3\n",
      "20 - NVAR with avg smape 1.74: \n",
      "Model Number: 21 of 130 with model NVAR for Validation 3\n",
      "21 - NVAR with avg smape 1.74: \n",
      "Model Number: 22 of 130 with model Ensemble for Validation 3\n",
      "22 - Ensemble with avg smape 1.09: \n",
      "Model Number: 23 of 130 with model NVAR for Validation 3\n",
      "23 - NVAR with avg smape 1.72: \n",
      "Model Number: 24 of 130 with model NVAR for Validation 3\n",
      "24 - NVAR with avg smape 1.48: \n",
      "Model Number: 25 of 130 with model Theta for Validation 3\n",
      "25 - Theta with avg smape 1.43: \n",
      "Model Number: 26 of 130 with model Theta for Validation 3\n",
      "26 - Theta with avg smape 1.43: \n",
      "Model Number: 27 of 130 with model NVAR for Validation 3\n",
      "27 - NVAR with avg smape 1.57: \n",
      "Model Number: 28 of 130 with model NVAR for Validation 3\n",
      "28 - NVAR with avg smape 1.57: \n",
      "Model Number: 29 of 130 with model Theta for Validation 3\n",
      "29 - Theta with avg smape 1.59: \n",
      "Model Number: 30 of 130 with model ARDL for Validation 3\n",
      "30 - ARDL with avg smape 1.84: \n",
      "Model Number: 31 of 130 with model Theta for Validation 3\n",
      "📈 31 - Theta with avg smape 0.99: \n",
      "Model Number: 32 of 130 with model Theta for Validation 3\n",
      "32 - Theta with avg smape 0.99: \n",
      "Model Number: 33 of 130 with model UnivariateMotif for Validation 3\n",
      "33 - UnivariateMotif with avg smape 1.46: \n",
      "Model Number: 34 of 130 with model UnivariateMotif for Validation 3\n",
      "34 - UnivariateMotif with avg smape 1.14: \n",
      "Model Number: 35 of 130 with model Theta for Validation 3\n",
      "35 - Theta with avg smape 1.0: \n",
      "Model Number: 36 of 130 with model Theta for Validation 3\n",
      "36 - Theta with avg smape 2.08: \n",
      "Model Number: 37 of 130 with model Theta for Validation 3\n",
      "37 - Theta with avg smape 0.99: \n",
      "Model Number: 38 of 130 with model ARDL for Validation 3\n",
      "38 - ARDL with avg smape 1.38: \n",
      "Model Number: 39 of 130 with model ARDL for Validation 3\n",
      "39 - ARDL with avg smape 1.54: \n",
      "Model Number: 40 of 130 with model ConstantNaive for Validation 3\n",
      "40 - ConstantNaive with avg smape 2.12: \n",
      "Model Number: 41 of 130 with model ConstantNaive for Validation 3\n",
      "41 - ConstantNaive with avg smape 2.12: \n",
      "Model Number: 42 of 130 with model ARDL for Validation 3\n",
      "42 - ARDL with avg smape 1.41: \n",
      "Model Number: 43 of 130 with model ARDL for Validation 3\n",
      "43 - ARDL with avg smape 1.51: \n",
      "Model Number: 44 of 130 with model ARDL for Validation 3\n",
      "44 - ARDL with avg smape 1.52: \n",
      "Model Number: 45 of 130 with model AverageValueNaive for Validation 3\n",
      "45 - AverageValueNaive with avg smape 2.62: \n",
      "Model Number: 46 of 130 with model ARDL for Validation 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3196: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in power\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3196: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in power\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3196: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in power\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 46 - ARDL with avg smape 0.0: \n",
      "Model Number: 47 of 130 with model Ensemble for Validation 3\n",
      "47 - Ensemble with avg smape 2.7: \n",
      "Model Number: 48 of 130 with model ConstantNaive for Validation 3\n",
      "48 - ConstantNaive with avg smape 1.83: \n",
      "Model Number: 49 of 130 with model ConstantNaive for Validation 3\n",
      "49 - ConstantNaive with avg smape 1.83: \n",
      "Model Number: 50 of 130 with model ConstantNaive for Validation 3\n",
      "50 - ConstantNaive with avg smape 1.83: \n",
      "Model Number: 51 of 130 with model ConstantNaive for Validation 3\n",
      "51 - ConstantNaive with avg smape 1.83: \n",
      "Model Number: 52 of 130 with model ConstantNaive for Validation 3\n",
      "52 - ConstantNaive with avg smape 0.0: \n",
      "Model Number: 53 of 130 with model ConstantNaive for Validation 3\n",
      "53 - ConstantNaive with avg smape 0.0: \n",
      "Model Number: 54 of 130 with model AverageValueNaive for Validation 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3196: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in power\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3196: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in power\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3196: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in power\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3196: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in power\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3196: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in power\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3196: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in power\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 - AverageValueNaive with avg smape 1.42: \n",
      "Model Number: 55 of 130 with model UnobservedComponents for Validation 3\n",
      "55 - UnobservedComponents with avg smape 2.03: \n",
      "Model Number: 56 of 130 with model MultivariateRegression for Validation 3\n",
      "56 - MultivariateRegression with avg smape 7.8: \n",
      "Model Number: 57 of 130 with model UnobservedComponents for Validation 3\n",
      "57 - UnobservedComponents with avg smape 1.86: \n",
      "Model Number: 58 of 130 with model GLM for Validation 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 - GLM with avg smape 3.84: \n",
      "Model Number: 59 of 130 with model GLM for Validation 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:187: RuntimeWarning:\n",
      "\n",
      "overflow encountered in exp\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 - GLM with avg smape 3.84: \n",
      "Model Number: 60 of 130 with model GLM for Validation 3\n",
      "60 - GLM with avg smape 3.84: \n",
      "Model Number: 61 of 130 with model GLM for Validation 3\n",
      "61 - GLM with avg smape 3.84: \n",
      "Model Number: 62 of 130 with model GLM for Validation 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 - GLM with avg smape 3.84: \n",
      "Model Number: 63 of 130 with model GLM for Validation 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:301: DomainWarning:\n",
      "\n",
      "The inverse_power link function does not respect the domain of the Gamma family.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 - GLM with avg smape 3.84: \n",
      "Model Number: 64 of 130 with model GLM for Validation 3\n",
      "64 - GLM with avg smape 3.84: \n",
      "Model Number: 65 of 130 with model GLM for Validation 3\n",
      "65 - GLM with avg smape 3.84: \n",
      "Model Number: 66 of 130 with model MultivariateRegression for Validation 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:67: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:68: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\autots\\tools\\probabilistic.py:68: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 - MultivariateRegression with avg smape 18.34: \n",
      "Model Number: 67 of 130 with model AverageValueNaive for Validation 3\n",
      "67 - AverageValueNaive with avg smape 0.0: \n",
      "Model Number: 68 of 130 with model UnivariateMotif for Validation 3\n",
      "68 - UnivariateMotif with avg smape 1.33: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3196: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in power\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3196: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in power\n",
      "\n",
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3196: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in power\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Number: 69 of 130 with model SeasonalNaive for Validation 3\n",
      "69 - SeasonalNaive with avg smape 2.09: \n",
      "Model Number: 70 of 130 with model WindowRegression for Validation 3\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "70 - WindowRegression with avg smape 3.88: \n",
      "Model Number: 71 of 130 with model UnobservedComponents for Validation 3\n",
      "71 - UnobservedComponents with avg smape 2.11: \n",
      "Model Number: 72 of 130 with model GLS for Validation 3\n",
      "72 - GLS with avg smape 1.31: \n",
      "Model Number: 73 of 130 with model UnobservedComponents for Validation 3\n",
      "73 - UnobservedComponents with avg smape 1.64: \n",
      "Model Number: 74 of 130 with model UnivariateMotif for Validation 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 - UnivariateMotif with avg smape 1.27: \n",
      "Model Number: 75 of 130 with model ETS for Validation 3\n",
      "75 - ETS with avg smape 1.72: \n",
      "Model Number: 76 of 130 with model ETS for Validation 3\n",
      "76 - ETS with avg smape 1.72: \n",
      "Model Number: 77 of 130 with model UnobservedComponents for Validation 3\n",
      "77 - UnobservedComponents with avg smape 1.78: \n",
      "Model Number: 78 of 130 with model MultivariateRegression for Validation 3\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 78: MultivariateRegression\n",
      "Model Number: 79 of 130 with model ETS for Validation 3\n",
      "79 - ETS with avg smape 1.72: \n",
      "Model Number: 80 of 130 with model GLS for Validation 3\n",
      "80 - GLS with avg smape 1.32: \n",
      "Model Number: 81 of 130 with model SeasonalNaive for Validation 3\n",
      "81 - SeasonalNaive with avg smape 1.69: \n",
      "Model Number: 82 of 130 with model ETS for Validation 3\n",
      "82 - ETS with avg smape 2.75: \n",
      "Model Number: 83 of 130 with model MultivariateRegression for Validation 3\n",
      "83 - MultivariateRegression with avg smape 1.13: \n",
      "Model Number: 84 of 130 with model ETS for Validation 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.184e+00, tolerance: 1.720e-02\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 - ETS with avg smape 2.07: \n",
      "Model Number: 85 of 130 with model SeasonalNaive for Validation 3\n",
      "85 - SeasonalNaive with avg smape 10.06: \n",
      "Model Number: 86 of 130 with model UnobservedComponents for Validation 3\n",
      "86 - UnobservedComponents with avg smape 1.45: \n",
      "Model Number: 87 of 130 with model SeasonalNaive for Validation 3\n",
      "87 - SeasonalNaive with avg smape 10.03: \n",
      "Model Number: 88 of 130 with model SeasonalNaive for Validation 3\n",
      "88 - SeasonalNaive with avg smape 1.96: \n",
      "Model Number: 89 of 130 with model UnivariateMotif for Validation 3\n",
      "89 - UnivariateMotif with avg smape 1.97: \n",
      "Model Number: 90 of 130 with model UnobservedComponents for Validation 3\n",
      "90 - UnobservedComponents with avg smape 1.42: \n",
      "Model Number: 91 of 130 with model AverageValueNaive for Validation 3\n",
      "91 - AverageValueNaive with avg smape 4.35: \n",
      "Model Number: 92 of 130 with model AverageValueNaive for Validation 3\n",
      "92 - AverageValueNaive with avg smape 16.88: \n",
      "Model Number: 93 of 130 with model WindowRegression for Validation 3\n",
      "93 - WindowRegression with avg smape 1.02: \n",
      "Model Number: 94 of 130 with model WindowRegression for Validation 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-2)]: Done 444 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-2)]: Done 794 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-2)]: Done 1000 out of 1000 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 - WindowRegression with avg smape 1.52: \n",
      "Model Number: 95 of 130 with model WindowRegression for Validation 3\n",
      "95 - WindowRegression with avg smape 2.21: \n",
      "Model Number: 96 of 130 with model MultivariateRegression for Validation 3\n",
      "96 - MultivariateRegression with avg smape 10.31: \n",
      "Model Number: 97 of 130 with model GLS for Validation 3\n",
      "97 - GLS with avg smape 1.47: \n",
      "Model Number: 98 of 130 with model WindowRegression for Validation 3\n",
      "98 - WindowRegression with avg smape 1.98: \n",
      "Model Number: 99 of 130 with model ETS for Validation 3\n",
      "99 - ETS with avg smape 1.68: \n",
      "Model Number: 100 of 130 with model SectionalMotif for Validation 3\n",
      "100 - SectionalMotif with avg smape 2.71: \n",
      "Model Number: 101 of 130 with model UnobservedComponents for Validation 3\n",
      "101 - UnobservedComponents with avg smape 1.46: \n",
      "Model Number: 102 of 130 with model WindowRegression for Validation 3\n",
      "102 - WindowRegression with avg smape 1.81: \n",
      "Model Number: 103 of 130 with model SeasonalNaive for Validation 3\n",
      "103 - SeasonalNaive with avg smape 2.08: \n",
      "Model Number: 104 of 130 with model ETS for Validation 3\n",
      "104 - ETS with avg smape 1.74: \n",
      "Model Number: 105 of 130 with model ETS for Validation 3\n",
      "105 - ETS with avg smape 1.86: \n",
      "Model Number: 106 of 130 with model MultivariateMotif for Validation 3\n",
      "106 - MultivariateMotif with avg smape 18.06: \n",
      "Model Number: 107 of 130 with model SectionalMotif for Validation 3\n",
      "107 - SectionalMotif with avg smape 17.13: \n",
      "Model Number: 108 of 130 with model MultivariateMotif for Validation 3\n",
      "108 - MultivariateMotif with avg smape 1.21: \n",
      "Model Number: 109 of 130 with model SeasonalNaive for Validation 3\n",
      "109 - SeasonalNaive with avg smape 3.06: \n",
      "Model Number: 110 of 130 with model WindowRegression for Validation 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\impep\\anaconda3\\envs\\OpenCV\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.126e+06, tolerance: 4.854e+05\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 - WindowRegression with avg smape 1.3: \n",
      "Model Number: 111 of 130 with model MultivariateMotif for Validation 3\n",
      "111 - MultivariateMotif with avg smape 12.3: \n",
      "Model Number: 112 of 130 with model UnivariateMotif for Validation 3\n",
      "112 - UnivariateMotif with avg smape 1.51: \n",
      "Model Number: 113 of 130 with model MultivariateRegression for Validation 3\n",
      "113 - MultivariateRegression with avg smape 1.11: \n",
      "Model Number: 114 of 130 with model MultivariateMotif for Validation 3\n",
      "114 - MultivariateMotif with avg smape 13.83: \n",
      "Model Number: 115 of 130 with model UnivariateMotif for Validation 3\n",
      "115 - UnivariateMotif with avg smape 12.28: \n",
      "Model Number: 116 of 130 with model AverageValueNaive for Validation 3\n",
      "116 - AverageValueNaive with avg smape 8.7: \n",
      "Model Number: 117 of 130 with model GLS for Validation 3\n",
      "117 - GLS with avg smape 1.4: \n",
      "Model Number: 118 of 130 with model AverageValueNaive for Validation 3\n",
      "118 - AverageValueNaive with avg smape 1.57: \n",
      "Model Number: 119 of 130 with model MultivariateRegression for Validation 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 - MultivariateRegression with avg smape 1.76: \n",
      "Model Number: 120 of 130 with model WindowRegression for Validation 3\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 11s 116ms/step - loss: 105.7642 - val_loss: 108.1195\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 101.8077 - val_loss: 108.1208\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 111.3971 - val_loss: 108.1320\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 102.1923 - val_loss: 108.1304\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 105.6344 - val_loss: 108.1349\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 103.9355 - val_loss: 108.1401\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 1s 51ms/step - loss: 98.0228 - val_loss: 108.1465\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 115.9061 - val_loss: 108.1488\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 1s 44ms/step - loss: 101.9901 - val_loss: 108.1591\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 114.2572 - val_loss: 108.1666\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 99.9602 - val_loss: 108.1745\n",
      "120 - WindowRegression with avg smape 1.53: \n",
      "Model Number: 121 of 130 with model MultivariateMotif for Validation 3\n",
      "121 - MultivariateMotif with avg smape 2.17: \n",
      "Model Number: 122 of 130 with model SectionalMotif for Validation 3\n",
      "122 - SectionalMotif with avg smape 2.09: \n",
      "Model Number: 123 of 130 with model DatepartRegression for Validation 3\n",
      "123 - DatepartRegression with avg smape 4.41: \n",
      "Model Number: 124 of 130 with model SeasonalNaive for Validation 3\n",
      "124 - SeasonalNaive with avg smape 12.08: \n",
      "Model Number: 125 of 130 with model MultivariateMotif for Validation 3\n",
      "125 - MultivariateMotif with avg smape 1.79: \n",
      "Model Number: 126 of 130 with model UnivariateMotif for Validation 3\n",
      "126 - UnivariateMotif with avg smape 1.23: \n",
      "Model Number: 127 of 130 with model SectionalMotif for Validation 3\n",
      "127 - SectionalMotif with avg smape 1.24: \n",
      "Model Number: 128 of 130 with model SectionalMotif for Validation 3\n",
      "128 - SectionalMotif with avg smape 5.13: \n",
      "Model Number: 129 of 130 with model GLS for Validation 3\n",
      "129 - GLS with avg smape 11.68: \n",
      "Model Number: 130 of 130 with model MultivariateMotif for Validation 3\n",
      "130 - MultivariateMotif with avg smape 13.47: \n"
     ]
    }
   ],
   "source": [
    "from autots import AutoTS  # 載入 AutoTS 函式庫\n",
    "model = AutoTS(forecast_length=30, frequency='infer', ensemble='simple')\n",
    "model = model.fit(data, date_col='Date', value_col='Close', id_col=None)\n",
    "prediction = model.predict()\n",
    "forecast = prediction.forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fab692e",
   "metadata": {},
   "source": [
    "在經過漫長（但自動化）的複雜運算之後，AutoTS會選擇最佳的模型預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47a75ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Close\n",
      "2022-06-16  16021.916427\n",
      "2022-06-17  16009.360169\n",
      "2022-06-20  15971.442562\n",
      "2022-06-21  15958.721237\n",
      "2022-06-22  15945.959265\n",
      "2022-06-23  15933.156894\n",
      "2022-06-24  15920.314374\n",
      "2022-06-27  15881.548438\n",
      "2022-06-28  15868.547841\n",
      "2022-06-29  15855.508357\n",
      "2022-06-30  15842.430242\n",
      "2022-07-01  15829.313751\n",
      "2022-07-04  15789.736584\n",
      "2022-07-05  15776.469157\n",
      "2022-07-06  15763.164642\n",
      "2022-07-07  15749.823298\n",
      "2022-07-08  15736.445387\n",
      "2022-07-11  15696.094866\n",
      "2022-07-12  15682.573305\n",
      "2022-07-13  15669.016490\n",
      "2022-07-14  15655.424686\n",
      "2022-07-15  15641.798158\n",
      "2022-07-18  15600.712897\n",
      "2022-07-19  15586.950142\n",
      "2022-07-20  15573.154002\n",
      "2022-07-21  15559.324744\n",
      "2022-07-22  15545.462640\n",
      "2022-07-25  15503.681958\n",
      "2022-07-26  15489.691181\n",
      "2022-07-27  15475.668918\n"
     ]
    }
   ],
   "source": [
    "print(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122bace",
   "metadata": {},
   "source": [
    "從預測數據來看，在未來30天，台灣加權股價指數為向下趨勢。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f01275",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "投資買賣一定會帶來風險。 使用機器學習進行加價格預測，是根據歷史價格進行推算所得到的結果，希望您在投資之前先做好所有的風險評估。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14767202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
